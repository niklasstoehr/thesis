{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Citeseer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "## libs \n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "## Keras\n",
    "from keras.layers import Lambda, Input, Dense, Conv2D, Conv2DTranspose, Flatten, Reshape\n",
    "from keras.models import Model\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.losses import mse, binary_crossentropy\n",
    "from keras.utils import plot_model\n",
    "from keras import backend as K\n",
    "\n",
    "## Basic\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import argparse\n",
    "import os\n",
    "import random\n",
    "import itertools\n",
    "\n",
    "# Computation\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mutual_info_score\n",
    "\n",
    "import scipy\n",
    "from scipy.stats.stats import pearsonr \n",
    "\n",
    "## Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "## Network Processing\n",
    "import networkx as nx\n",
    "from networkx.generators import random_graphs\n",
    "\n",
    "## node colour\n",
    "color_map = [\"steelblue\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Supporting Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "## supporting functions\n",
    "from preprocessing import sort_adjacency, reshape_A, calculate_A_shape, reconstruct_adjacency, pad_matrix, unpad_matrix, prepare_in_out\n",
    "from metrics import compute_mig, compute_mi\n",
    "from generating import generate_single, generate_manifold, generate_topol_manifold, generate_topol_manifold\n",
    "from latent_space import vis2D, visDistr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and Preprocessing Network Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Network Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of nodes: 384413\n",
      "number of edges: 1743018\n"
     ]
    }
   ],
   "source": [
    "e_file = open('data/citeseer/out.citeseer','rb') \n",
    "g_complete = nx.read_edgelist(e_file)\n",
    "g_complete = nx.convert_node_labels_to_integers(g_complete, first_label=0, ordering='default', label_attribute=None)\n",
    "e_file.close()\n",
    "\n",
    "n_complete = len(g_complete)\n",
    "e_complete = len(g_complete.edges())\n",
    "#pagerank_complete = nx.pagerank(g_complete, alpha=0.85)\n",
    "print(\"number of nodes:\", n_complete)\n",
    "print(\"number of edges:\", e_complete)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample Subgraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAeEAAAFCCAYAAADGwmVOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAAIABJREFUeJzt3X14VOWdN/DvOfOSaHgZKhAZQBIIIZRiqATEEGpVoM/uFYvXurHpI0Z3a9M+dautLd29ti2rfZ7WttLdZeu2lj7d1qh9WPpiuw0XLm8FKoRiUFHBJAQICoHw5ggEmUxmzvNHHMjLZGbOue/zMud8P9fVfyoMR+LMb+77/t3fn6JpmgYiIiKynGr3AxAREXkVizAREZFNWISJiIhswiJMRERkExZhIiIim7AIExER2YRFmIiIyCYswkRERDZhESYiIrIJizAREZFNWISJiIhswiJMRERkExZhIiIim7AIExER2YRFmIiIyCYswkRERDZhESYiIrIJizAREZFN/HY/ABHZI9IdxcZ9x3Ck6zy6o70oyPOjuHAUlpZPQqggz+7HI/IERdM0ze6HICLrtHZGsPaldrzcfhqKAvT0Jq78s6BfhaYB80rGobaqBDPCIRuflMj9WISJPKSxuQNrNregJxZHuje+AiAY8KF+cRmqK4osejoi7+F2NJFH9BXgtxCNJTL+Wg1ANBbHms1vAQALMZFJWISJcojRc9zWzgjWbG7JqgD3F40lsGZzC0rDIZRya5pIOm5HE+UA0XPcx9c1o6m1K+0W9HAUAJVlhVhZU2H8X4CIUuJKmMjhMp3jJgtyU2sX9h4+M+QcN9Idxcvtpw0VYKBva3rPwdOIdEfZNU0kGe8JEznY1XPc9I1UwMBz3Mbmjiv//8Z9x6AoYs+hKMCmfcfEXoSIhuBKmMihZJ3jHuk6P2D72oie3gSOnLog9Bpm4p1nylUswkQOtfaldvTE4oZ+b08sjrU727GypgLd0V4pz3PxckzK68iU9qy85SQatrXxzjM5GhuziBwo0h3F8tVbEYsbX8H6VeDjBUfwp+MKoqOKhJ/pjtkT8bW75gi/jiy880xuwDNhIgeScY7b0xPD3ncuoXTih+AXfKcH/SqKx48UexGJZJyVEzkBizCRA8k4x1X9Ady0aCm+8dm/gqKIvdU1DVhSPknoNWQRPStv64yY9GRE+rEIEzmQzHPcUEEe5pWMg9GFtQJg/vRxjmlwknFWTuQULMJEDlSQJ6dnckR+AABQW1WCYMBn6DWCAR9qF5ZIeR5RMu88EzkBu6OJTGbk+kxx4SgEW04KbUn3P8edEQ6hfnFZ1tnRSXkBFfWLyxwTWSnzznNN5TQ5D0UkgEWYyCQi12eWlk9Cw7ZWoT9/8DlusjM4lzuKvXDnmbyFRZjIBCJRk/F4HOuefwan204iNPWjgIGmquHOcasrilAaDmHtznbsOThMDjWA+SXjULuwxDEr4CQ333kmb2IRJpJMZGRgoXYaX/jCF5CXl4dv/J9/xk93n0PUQBNSunPc0nAIK2sqEOmOYtO+Yzhy6gIuXo5hRH4AxeNHYomDU6Zkn5UT2Y1FmEgikeszT63fh7bfPIHHvvpF3HfffVAUBcqI7At6UrbnuKGCvJw7F5V9Vk5kNxZhIolErs9oiop7/+FfUPc/b7ny/7nhHFeGZHNb6/F3hVLEAGfdeSZiESaSRPT6DBQVr3ZEhowMzPVzXBHpmtuMcNqdZyIWYSJJzLw+k8vnuEZlmw2th5PuPBMBLMJE0lhxfSYXz3GN0NPcli2n3XkmAliEiaTh9Rk5jDa3DcfNZ+WU+1iEiSTh9Rk5RJrb+nPzWTm5B4swkSS8PiNOuLkNfSvfqpnXY0Y45MqzcnIXDnAgkmRp+SRogh1EXr8+I6O5LeBXMSMcQk3lNBZgcjwWYSJJ3DYy0A7MhiavYREmkqi2qgR+n7EyzOszbG4j72ERJpLoUtcRHNn6LHzQt5rj9Zk+bG4jr2FjFlEG2c4D3r59O2pqavD0008jeMNsz0dNGsHmNvIaRdNEW0mI3CntPGC/Ck3DlXnArc078OCDD2Lt2rW4/fbbAQBtnRFPRk2KiHRHsXz1VqF86IBPxXOP3O7ps3XKHSzCRClkG5moAFAVDUf/+Bye+d5XUVFRMeTXeClqUobH1zWjqbXL0DUlBUBlWSFW1gz9ORA5EbejiQbROw84rikovqMOJzE25a/xStSkLLVVJdh7+Iz0OcpETsTGLKJ+jEYmxuIa1mxuQVtnxKQn844Z4RDqF5chL6Dv44nNbZSLuBIm6kckMrEnFsfane3cCtUhXdPb8c4T+FXzCfgDeWxuI9diER5Gth2x5B6ikYkagD0HTw+ZB0xDpW16azmJhm1t6D62Hwumj8c1k25gcxu5FhuzBtHTETuDb3pXWbfrEJ7d3iZ8Pabu1lKeAaeRbdOblkggP+hH/ZKZqJo5gc1t5EpcCfeT6cMh+eHc1NqFvYfPcPvLZRiZaD49TW+KqiLam8CazW8BAL/YkCt5qgin22J+6a0Tujpio7H4lQ8HFmJ3YGRi9owc1xhteovGElizuQWl4RC3nMl1PFGEM50/PbOtFYmEhoTOjflsPhx4tpw7GJmYWTZnucMd17DpjWgo1xfhbLeYjRruw0HkwyoVFnPzMTIxPZHjGja9EaXm6iKs5/zJqFQfDjLPlmUXcxre0vJJaNjWJvQabp0HrDfAZPBxjYw5wYoCbNp3jGfD5CquDeswev5kRPLDAej/YZW+8xMY+GHV2Nwx5J83NndgRcNuNLV2IRZPDFmh9fQmEIsn0NTahRUNu1O+BmWP84BTEz3LbeuMsOmNaBiuLcIi5096JT8cZHxYJcks5pS92qoSBP3G3hZujUyUcZbLpjei1FxZhEXPn4y4eDkm5cMKkLPyIGNKJ4yG7+0mIKGvaLg1MlHWWW7AJ7gX/QE3N72RN7myCMs4f9Ir6FelNZ7IKuak33e+8x0c3/siPrf0w8gL+DJuTSsA8gI+1C+e6cqrahs/OGYRoShAtDdheIchyc1Nb+RdrmzMknH+pEfQr+JyLI5EQuzPVBTgv17uYBepTV544QU8/fTT+POf/4xwOIyPTBnn6XnArZ0R/H7PEaHZvkDf35uqAL2Cr+PWpjfyNlcWYVnnT9nSNODs+cuIC+5/9/QmsLP1JLtIJcvmete+fftQX1+PDRs2IBwOAwBKwyGsrKnw5DzgZIe/kXGCqexpPy30+93a9EbkyiIsK3QhK5qGi2c6cSReCBjuq72q49RF4ddgF2mfbK93feLDH0LtsmX44Q9/iIqKoWEQXpsHbMbVPtGEerc2vRG5sgjLCF3ImqIgf8z1SGia5efQ6Xi9i1TPXe2dB45h6f1fQW1trbUP6UBWXu3LVv+mN4bWkNu4sgjLCF3QQwOgOKkCw9tdpHqDJRRfACcDJWhs7nBlc5UeVl7ty6T/nODp4RAeX9fM0BpyHVd2R4uGLuQ6L3eR8nqXcXZc7UtFUYCAX0VlWSFW1S0AAIbWkGu5sggDH4QuBHx2P4YtvNxFyutdxtlxtS8VVVHwowersLKmAm2dEYbWkKu5cjsaAGaEQ6hfXGZ6drTTeLmLlEMCxFh9tW84PlXBnw+ewvuxuPTRhzxTJqdxbREGrs75Tdeg4zZe7iLlkAAxVl/tG06yu//AsXeljT7kIBRyKlcXYaCvEJeGQxlDF0bk+fFud499DyqBW6MTs8UhAWIsvdqXwen3uvHW8fNSdjVeeuuEtKlmRLI5511nomxCF36y8QC2vtlp96Ma0r+L1MsfHhwSIMbSq30Z7PzzXhSMmwzVHzT8GooCrF7/BvYePm14BCOR2TxRhJPShS446QMoFVUB/D7Vc9GJeshayXn1epfVV/vSub6oFN1RsatSPb0J7G47hYTOpJB0Z8pEsnmqCKfjpA+gwYJ+FTW3TMU1Qb+nohP1kvFFysvXu0IFeZh8XQEOO2A7XrQAJ+ktwEmDz5TZ0EVmYRH+QPJucVNrl+MauDQN+OS8Ir7ZM5DxRcrL17samzvwzlnx2FQ3SJ4pNx86jfV7j7Khi0zDItxPbVUJ9h4+Iy20XgYvXznSS/SLlJf/rpMhJzHRKSQOommaUJJdQtOwcu3LSCQ0Qw1dXD1TNliE+3Hi3WIvXzkyQuSLlJf/rp0UVymLaJRsPJHdF5LBDV3TwyFeh6KsKZomOt/EfWSPcTOq78qRO4fFm8nIFCAv/11HuqNYvnqr8Nxg6gsaURUFvfFE2t0Y3migJNfGVoqorijCF/9iFlSbIvwUAHkBn2eLgqjqiiLUVExAPBbNmB/Ov2vnxFXKZNd7N57QEMtQgAFGbNJV3I4exq7WLuEZqHrxypEcmqZh3epv4qMVH8PIstvShrTw79o5cZWy5AVUzJ06Fs2Hzjj+34vXoYhFOAUrp8lcNzIfJdeP4pUjiX75y1/i6NGj+M1vHkEwGEwb0sK/a+fEVYrqv8VbNXMClq/eavcjZWXwdSjyFhbhFKzcnptTdB2+dtcca/4wDzh16hQeffRRNDY2IhjsS1tKF9JCzoqrTGfcqHxEunuy3tVw6pXDwbw+OMTrcuPdZzGrtue8HAxhVKZrH1/84hfxwAMPYN68eXY/as5weloc0PdeWTavCEvKJ2W9q+HEK4fD8fLgEK9jEU7Bqu05LwdD6JXNFJyJ18aw/+0z+MUvfmHfg+YgJ6fFJSXfK3p2NZx45XA4Xh4c4nXsjk7Biu05LwdD6NXY3IEVDbvR1NqFWDwxZMXW05tALJ7AkfcUFFU/ii37u2x60tyUDDlxaoO0yHuluqII9YtnIi/gy6pT3qcq8Nn0F+HVwSFexyKcQnHhKAT95v7VeDkYQo+rd34zz4NWVBWxBHjtw4DaqhIEAz67HyMl0fdKdUURVtUtQGVZIQI+dch7O+hXEfCrqCwrxP+unQdVtedj0auDQ7yO29EpmL095/W5v9lKRinq3UrktQ/9jG7d5gVU3DK9EE0HTw07r1eErPdKNuNMkyttOxq62B/iXSzCKZg1zIEpOfqIRCny2od+yf8m12xuyVhQB/+33NYZwdqd7RnvZE/8UAF+//JR3a8vSzZnynY0dLE/xLsYWzmM1s4IVjTslvJGZDCEfjKiFAM+Fc89cjvP3XXKtqCm+m+5/0pz20tNKJ48AR+fN3vASlPk9a1iJPrUKAVAZVkhvzB6FItwGkbeiEG/iopp43BN0M9gCAHrdh3Cs9vbhGcD191aymsfBomGnDz88MOYOnUqvvSlL5ny+mZLZshns2r3+1RomobeLIc+9JcX8GFV3QJ+OfcobkenIbI9R2Jk3NXmtQ8xoiEnEyZMwIkTJ0x7fbNVVxShNBzKetXe1hkxODiE/SFexiKcgd43It9Mcsi6q81rH/YJh8M4cOCA3Y8hRE9DV/K9zy/tpAeLcBb0vBFJDll3tXntwx6R7ije0cbj5KhyrFz7cs4PtM921c4v7aQXz4TJkXgmnJv6J5tBS6D/zmzQr0LT4JmB9vzSTtlgESZHYnd07tHTyMStWKI+TMwiRxKNUmQsqLX0JJtxoD3RVSzC5FgiUYqMBbWOaLJZW2fEpCcjcj4WYXKsZJRiXkDff6a89mEtGclmRF7FIkyO1n8KjqalX2kp6As+qF88k2eNFol0R/Fy+2nD8a79B9oTeRGLMDledUUR/nbeaFzo2JdxCs6qugUswBbauO8YFMHRf8mB9kRexHvClBOe+/EPULtoEe5/8HZe+3AQJpsRiWERJsdrbW3Fjh078Mwzz6DA4VGHXsNkMyIx3I4mx3vyySfx0EMPoaCgwO5HoUGYbEYkhithcrTjx4/jt7/9LQ4ePGj3o1AKxYWjEGw5KZxsxoH25FUswuQIke4oNu47hiNd59Ed7b2SNbzztz/FAw88gOuuu87uR6QUlpZPQsO2NqHX4EB78jIWYbJV/6zhIWH3b53A5YKbMXfqWLR2RlyfNZyLkslmTa1dhq4pMdmMvI7Z0WQbZg27Q2tnBCsadiNqILCDA+3J69iYRbZg1rB7MNmMyDgWYbIcs4bdp3+yWabsDiabEV3FIkyWY9awO1VXFGFV3QJUlhWmTDZDopfJZkSD8EyYLMU5wd4weKA9eqN44fmfYsevfoIxI/Ltfjwix2B3NFlKZtYwk7OcKzQo2UzTNDzz9Xtx6vhRjJkxw8YnI3IWbkeTpZg17E2KouDjH/84/vjHP9r9KESOwpUwWYpZw961YNFt2PDq23jvhVcHBLIs5fAN8jAWYbIUs4a9JxnIsqdrPHrGjsbWNzuv/LNgy0k0bGvDvJJxqK0qYSALeQ6LMFmKWcPeMjiQRfUHB/zz5H8HTa1d2Hv4DANZyHN4JkyWWlo+CaL9+Mwazg0MZCHKjCthshSzhr1BNJClNBxikhZJN9ygGDv7EliEyXK1VSXYe/iMoazhYMCH2oUlJjwVySQjkGVlTYXkpyKvSjsoxua+BG5Hk+WYNexuke4oXm4/bWinA+jbmt5z8DQi3VGZj0Ue1djcgRUNu9HU2oVYPDGkH6WnN4FYPIGm1i6saNht+XEIV8Jki2TzDacouQ8DWcgsereTr/YlZD4W6d+XAMCyzxoWYbJNdUURSsMhrN3Zjp0HOuFTFcT7bc4E/So0APNLxqF2YQlXwDmCgSwkm5Ht5FzpS2ARJluVhkP45l/PxeSpn8Y3/u15nI8HcfFyDCPyAygePxJLGOSQcxjIQjJlmjs+3DW3XOlLYBEm2x04cAAB9OLzd863+1FIAgaykCxGt5MvRXul9SWYvQhgYxbZbsuWLbjjjjvsfgySpLhw1NAxhjoxkIVEtpN/sa0NmuES3CfZl2A2roTJdlu2bEFtba3dj0GSLC2fhIZtbUKvwUAWdzFyP1dkOzmeEJ/Qa1VfAoswWWrwm/GaoA+t3SNQccsiux+NJGEgCyUZvZ8res1NFiv6EliEyRLp3oyTF/41vvzLNzCv5CRD/F2CgSxktKEKkHPNTQYr+hJ4Jkymy3RZXvUHbb0sT/IxkMXbRHPDZVxzE2VVXwJXwmSqXLgsT+ZgIIs3ybifK+uamwir+hJYhMk0uXJZnszTP5Blz8EU54I+BZejUcwvGY/775jFn7cLiN7P/cG67Th98gRQMFnyk2XPyr4EFmEyTa5clidzlYZDWFlTgUh3FJv2HcORUxcGBLL84Wffx5iR01EaXmj3o5IgGbnhHe8B00LX4mgcENmR9vsUaJqxTmkr+xJYhMkUMkP82SXrDqGCvJRZ0NfHl6O+vh6f+fzfYdPrxx01Zo70kdFQFQwGsWDBzXh712EAxquwAgX33zYdz+04qGs3zuq+BBZhMgVD/Clb46Z+BNdV1WH56i1QVdVRY+ZIH1m54Scj70u55nZPZQmuDfod3ZfAIkymYIg/ZSN5jaXghtl9W4+JoWPmgNTXWMh5ZOaG3/ux6VKuuWXsS7B5UAyLMJmCIf6UycDO+fTbJuyczw0yc8OT19yyvV2RlGo7OVNfgp2DYliEyRQM8ad02DnvTsWFoxBsOSm0C9b/fq7sa27D9SXYiWEdZAqG+FM6MjrnyXmWlk+CJpg1Ofh+bnVFEVbVLUBlWSECPnXI50rQryLgV1FZVohVdQtybpeEK2EyBUP8aTjsnHcvs3LDnbydLIpFmEzBEH8aDjvn3e2WGYXY3dZlaEWc6X6uE7eTRXE7mkxTW1WCYMBn6PcyxN+92DnvXo3NHXhqw34YmSTo1dxwFmEyDUP8KRV2zrtT/6ENeuUFfKhfPDPnznNl4HY0mYoh/jQYO+fdx2i3OwCoCvDwX8zC4nL7sqLtxJUwmc7t3Y2kDzvn3Uek213TgF1tXZKfKHdwJUyWSNXdePT4Sbz1xmt4sK4mp7sbSR92zrsLu93FcCVMlkp2N37trjl4ZMlUvLPlZ6ipnObJN59XJTvnjTZIs3PeWWR2u3sRizDZprCwEF1d3t2G8jJ2zrsHu93FsAiTLSLdUbz45mlM/R+fx9ef343vvfAq1u06hEh31O5HIwuwc9492O0uhmfCZKnWzgjWvtSOl9v7ppkUzr4VzYfPAuDYOq9h57w7sNtdjKJpokmfRNlJjq3jBy7119YZSTtmrqenB+MD7+NLn7oDh7rO40jXeXRHe1GQ50dx4SgsZVOfrdbtOoRnt7cJD22ou7XUdWlY2WARJksMHFuXnb6tR29e4Pei4XKB0X0GP/jlRoydPheKogydBauBuyc2inRHsXz1VsTixotwwKfiuUdu9+SXKRZhMl1rZwQrGnYbTtJZVbeAZ4Aeldw9ifbEAGX482Punpgv0h3Fxn3HUu5ErF7/hlBOfGVZIVbWVMh+5JzAM2EynYyxdV59g3rZgN2TNAUY6LtrGo3FsWbzWwDAQizR4D6OATsRH/RxlE0Mwe9TDa2Gvd7tziJMpuJFfjLCaAxiNJbAms0tKA2HuHsiQaY+jmRBfvPtc9ASfb9GUbO/esZud15RIpPxIj8ZIWP3hMT0H8iQ6Uu0BgCqD36fD35VyRjEosDbQxv640qYTMWL/KQXd0/sZ3QnIq4BAVXBjVPG4MCxSMpudw3A/JJxqF1Y4ukVcBKLMJmKF/lJL5m7J1688iKDyE5EbzyBEdcE8Nwjt6fsdmdO/EAswmQqXuQnvbh7Yi9ZOxEA+CUoCzwTJlNxbB3pxd0Te7GPw1oswmSqpeWTIHoTnWPrvIW7J/biToS1WITJVBxbR3px98Re3ImwFs+EyXS1VSXYe/iMocQsr1/k96Kl5ZPQsK1N6DW4ezJQurSrwV9wuRNhLRZhMl1ybJ2x7GhvX+T3ouTuiUgMIndP+mSTdjU4d7u4cBSCLSeFBzJwJyI7zI4my2Q7RQnQkBfwMwfYw0Tyxn2qgo8Wj4VPVTw9acno1LJIdxT3/usW9CaMlwYvD2TQi0WYLJVpbF0ikcC5Q6/g//7T5zFryjgbn5TsZmTyFtBXhOP9CogXJy0ZnVr2wK0leGX9M9j4ThBjSm4CDHRzeH0gg14swmSL4cbWLSmfhE/91Sdxzz334DOf+Yzdj0k2y3Y1p2naB9dqhi8aXpm0JLKLkOiNYlTHZnz50S/j+xsOcfKZBViEyXG2b9+OBx98EC0tLfD5sg+DJ3dKt3viU/qiEvVw+5zqx9c1Gz5PB4CFH6xiOQPcGizC5DiapqGqqgoPP/wwPlF915WuzsilHrzX3QNAw+hr8xAqCHr2vM+LBu+eJBIaXjlyZsDWc7bculqLdEexfPVWQyMFk/qf5xo9V6bssQiTI/3HuvV4Zut+jLxhNgANsWGWO36fAgVK1ud9eq5qkLOJrPjcem65btchPLu9Tbizue7W0iuRk5n6ODiQQQyvKJHjNDZ34HeHVORNmJnxG31vXAOgoam1C3sPnxn2m7iRqxrkXJy0lJoZaVel4RBW1lSk7eNw09+h1ViEyVGunEP1JqCo2acmaQCisTjWbH4LAAYU4mwHk2cq5OQcnLSUmplpV6GCPFf9XTkFizA5htEZpv1FYwms2dyC0nAIpeGQruaSdIWcnMXN+cYiRyZMu8o9LMLkGCIzTPvricWxdmc7PrWwxFBRH1zIyXncmG8s48iEaVe5h0WYHEH0jK+/5HlfT2/CcFFPFnK3Ne64hdtWfDKOTKLRKM7s34FodAwUn/F/L+ZuW4tTlMgRZJzxDaThlcNnpDTukPPImLSkKEDHqQtYt+uQrT/nq0cmmeJcBx6ZNDZ3AADeffddPPHEEyguLsZ//eY/MX1sHqeW5RAWYXIEGWd8/cXiGkRv33EwuXPJmlN9qOs8nt3ehuWrt+Lxdc1o7YzIecAsGe2DiMYS+MnGA6j/yjcxbdo0tLS04MUXX8SLL76Ih//qFgQDxkJuOLXMetyOJksN13QSudQj/c8SyJ8H4NzGHRKftNSfnd3xIn0Q0VgckRHT8Prrr2PSpKvbx5xalltYhMkSmZpOYhJXwTL1b9xh0IeziMypTsXq7njRPghFVXEpUIgRY4YOOkk+O9OunI+JWWS67EcYyqMoEN6uBIA7Zk/EsvlFw3+B8OCEHicxOmkpEytiLc1ItxqMaVfOxyJMpjLrQzITVQF8qiqUoRv0q6iYNg57D5/hasLBzPiSZ0Ws5fdeeBVb3+wUfp07Zk/E1+6ak/bXMO3KubgdTaaREb5hhAJg7rRxeO3IWaHXiSc0NB86hZ7ezB/tDPqwT3VFEUrDoWFXfEaki7WUdSxh5V1npl05F4swmUZW+IZewYAPdbeWIuBrN9y4oyUSiGtAPKHvAgGDPuwxON942/5OHOo6L3QkMTjWUnb+uNvuOpMxvKJEppAZvqGHFo/h7pvGozQcQm1VieGrGqrP+FsjGfRB1kuu+G4YO0K4J6B/d3xjcwdWNOxGU2sXYvHEkJV2T28CsXgCTa1dWNGw+8od3uG0tLSg5ZWdSPSK3QpgulXuYxEmU8gP30hPAZDnVzEjeApfvfcT+PnPf47SCaNRv7gMeQF9/5kH/Wpf2IFi7O3BoA/7ydzqFQ3TSDp37hx+9KMf4eabb8Ztt92GULQTwWBQ6PmYbpX7WITJFLLCN1QFCPiGr+Z+n4KAX0VlWSFW3X8LfviP9di6dStWr16NZcuWoWJSPuoXz0RewJd1itCIPD9UVewbBIM+7CVrqzeR0ITyxw+8fQaNjY2oqanB1KlTsWPHDjz22GN455138C/f/w7mTy9kupXH8UyYTCFrJTKnaCxumjoWR05dwLvdUZz/INRj1DVBjBmRl7LDc/bs2dizZw8ef/xxzJkzB0899RRW1S3Gvza+jkNdmcM3znWLB4cw6MNesgYZnDr/vvEwjZ4Y/mblj+BrexEPPPAAfvrTnyIUGnhWLHLXmelW7sAiTKaQtRIZM8JYV2cwGMS3v/1t3Hnnnbj//vsx6443EJu8QMozZctJE3q8Zmn5JDRsaxN6jURCQ+e5S8b7GhQVY6dX4Pl///thV6tMtyJuR5MpZATsy2g6WbBgAdZt2I5L18+Vmk2dDXat2icZaymy1TvxugLhvgZVVTIeS1RXFGV9ZKKgL0ikfvFMXoNzCRZhMoWsgH0ZTSe/az4GKMa6pI1i16r9RLrjgwEfxo0CDAUMAAASNElEQVS6RviLW7bHEtUVRVhVtwCVZYUI+NQhX2CDfvVq70PdAhZgF+F2NJlCNGBfVtOJbVel2LVqmXThGSJbvXvaT0t5vmyPJQbfdWa6lTewCJNpnNB0YvVVKYBdq1bJNjxjWUURft98VHf0aHObeKQkoP9YgulW3sIiTKaZEQ5h+aIS/GJbK/REOMtsOpE9pzgb7Fo1X6a86P7jCYMBH5bNm4Lj57rTDzKYNg7zJih45b+fx7e+8HtcHF2KyVU1gGr8Y5LHEpQJizCZov8qpW+NkXlD2IwhCLKuSmWLXavm0zMUJBme8fuXO1C/eCYe/svZA7Z6r83zQbl0Fp2vbkHDz36Nn8ViWLZsGb773e9i9k3z8Tc/+pPQEBAeS1AmLMIknd6pNj6lLybSjJFqsq5KZaYh0RvDZ5beiKqZE7Bu1yHOHTaB0aEg/TO9/7L8emza9AZ+9+LvsH79ekyePBnLli3Dr3/9a5SXl0Ppd37hhL4GcjeOMiSpjIwu9KsK7r+tFPdUyt/ClTGz1acAk8eOwPFzl9JuZe78w7MYV7YAESU05NepSt+q7LoR+agsK8S9i6bzw9mAx9c1Gy6KgAb13BHs/NnXMX/+fCxbtgyf/OQnMWXKlGF/R2tnBCsadhvqa7BiJjHlPhZhksaJH1iR7iiWr94qtKWY6I3h0vZ/x9/93UNQJ3wER09fHNK1+tJbJ/CTjQcQjcWhqJlv/qkKsKC0MOuJOyTnZ6kqGp7+2/mYEh6f9e8x8sWy71iCd3kpM25HkzQiowuTk4dkD1GXcVWq6sMTMXPWZ/Hk976N8+fP4ytf+QoerqtDfn4+gKsf0j1xLasCDAAJDdjV2oW9h89IPQO3i6wZu+nI6HT3+3zY03FBVxFO/myyOWIxo6+B3I1FmKQQvY+bboi6KNGrUp9eNB2l4XmoqanBjh078OSTT2LlypV46KGH8Im76wydUSYlJ+4AyMkPbdkzdtOR0eluNNO7uqIIpeEQ1u5sT99hbUJfA7kbt6NJChlnr0G/irpbS025Iyl7S3H//v34wQ9+gDd7p2BMyU2Gxx5e/bNy7/ww2wY8WavDf2jYiVePRgz//qSbp4/Ht2rnGf79DNMgmbgSJinsXKVkQ/aW4qxZs/DPP/wxlq/eglhc/HusWdvxZjFyTUjPil/TNLS1tWHnzp1X/nftnLswbtYiwScXz/RmmAbJxCJMUsgcom4W2VuKfWeU2d2BzsTM7XjZZFwTGvx3e/nyZTQ3N18puLt27cLIkSNRWVmJhQsX4uGHH8ZbF67F839qF95tYXgGOQmLMEkh6z6u2ZOHZObzyk7jUhRg075jjl9lyWjA+9zHJmPXrl1Xiu7rr7+OWbNmobKyEvfddx+efvpphMPhAb93SncUz+1oF3p2hmeQ07AIkxSyhqhbtUqRsaUoO41LZDveiu7k5J8j2oD3pzeP4akv3Y15cz6CyspKPPHEE5g3bx4KCgrS/l6nDAUhkolFmKSQMUQ911YpZqRx6d2Ot7I7GZBzTSgYDOKH67bgUwbytZ0wFIRIJs4TJilkDFHPtVVKceGoIXNfRenZjm9s7sCKht1oau1CLJ4YsgvR05tALJ5AU2sXVjTsRmNzh/DzydiC701oOHr6oqHfOyMcQv3iMuQF9P29M9ObnIpFmKQRHaKea6uUpeWTIPWCX6IXF0524Pz58xl/6dXu5Mz53P27k0UKcTQaxfFTZw3//v5EGvCqK4pQv3gm8gK+jF/6FPRd/2J6FTkVt6NJmuQqRe99XJ8K3Lco9wIORM8oB1NVFS+v/w9M+lod5s+fjzvvvBPV1dWYNm3g2bWs7uR058jXBhTs378fzc3N2Lt3L5qbm3HgwAHM/fQ/4tqijwr/u4o24DE8g9yCYR0kne4pSqoCVVGknl1aRSQvuz8FQGVZIVbWVODixYvYvHkzGhsbsX79eoRCoSsFubKyEt/+7WtCzUmzp3wII/IDKc+RlUQvEpqGs+2vIPH2HtxYXIiKigrMnTsXc+bMQeNrJxwXysLwDMplLMJkirbOyJVVSiKRQDZ5Frmau2skjWuw4RKzEokEXnnlFfzhD39AY2MjjnWdxZzP/hugGtv2v0oD0mzmDvezkDFEIeBT8dwjt7NAEgHwPfbYY4/Z/RDkPteNzMets8Lw+xS81nEu67PTeELDvqNnMTI/kDNbiKXhEEbmB7Dv6DnEE/q/0ybjMW+Zcf2Qf6YoCsLhMG677TZ87nOfw5iZH0PLiQvQDLfAXXnljL8i1c8iP+jHwRPv4djZbsN/6oIZ4/GJOTcY+v1EbsPGLDJNa2cEz+1o112YkmeXbZ3iOcFWqa4owqq6BVhYVgifml2BNNI0dPaygoSFb9tUPwuvNeARmYlFmEwjI1kplyTTuH75pTuwbN4UjBuVn/LXqQrg9ymoLCvEqroFurbeZQeEZGPwz4LXhIjkYREmU8gcbZhrQgV5uOPGSZg+YTQCPnXIyljTNChQoGn6U6fNCAjJJNXPgteEiOTgmTCZ4r+aj+L1o2cNnZEm+X0KRuYHMGvyhyQ+mfkamzvw3Rdew9FTFxDXtBTn4QoSmoZjZ7ux9c1OjMz3Z1wdvv/++33NWVteQjTvOqg+a4txqp9FaTiEuVPH4r33e3Dy3ffh9ykDft5BvwpVVbCgdDwevfPGlGfeRF7He8JkCqePNjSLkRF///7ifvz3a8ew6MMTBmQ9v//++9iwYQN+9atfYcOGDaioqMCdd38Km97NlzI+UY/hfhYyB2IQeRGLMJnCrNGGVg0qMMJoiEZCA9pOvIfDp86jYVsbJl4bw7uvb8TG3z6HuXPn4p577sHq1asxfvx4AEBkXbO0gBA90qVcccYukTEswmQK2aMNzRxUIKuwizSiAUBvXAOg4ch7CvzFn8CPX/g8Pn3bR4b8OpEhBiLMHjNJ5EUswmQKmaMNMyVwJf+MptYu7D18JuuwD5mFXbQRrT9FVREH8P/+/A5Gjhwx5N/FaDyoCCvHTBJ5CbujyRQyhhv0dQ9rpgwqkD2BSMaIv8HS3ZfW050sQ66NmSTKFSzCZArR0YZaIoHY2bfx7LY2w4MKhgv7MGMCkYxGtFTS3ZdOBoRUlhXCn2VAiBG5OGaSKFewCJNpRJKV8oN+hMaEDJ97Dle8RCcQDVfYzQrRyHRfOtmd/KmF07JO6tKLKVdE5mERJtOIJCst/9h0dKujoKjG/hMdrnjJTPG6ePEiNm3ahG984xvYuf2Phl4zG4oCbNp3LO2vOfHuJaE72cNhyhWRuViEyVRGk5WgKMJnrIOLl4wUr92tJ/Ho338dN998MwoLC/Gtb30LAHDbzbMR9JmzEs3mvrTslThTroiswe5oMp2RAezfe+FV6WEfMpqnent7cbFgMr7//e9j/vz5uOaaawD0Ffjdq7dCfxBldtLd0QXkXQlTFcDnUwf8LIjIPCzCZAm9yUqyVnYbNm3Fxqe+itGjRyM+7Q70jJwi9HqKL4CS2Qtw661zBvz/yUY0s0I0Mt3RlXElTFWAudPG4aufLGcTFpFFWITJUtkmK8la2c376I1YctdsnD9/Hn84DLx/Wfw1h1uVmhWikc0d3aXlk9CwrU3oz/GpKgswkcV4JkyOVFw4CkG/2H+eQb+KeR8uxpIlS3D33XdjZslUKc823KrUaCNaJtnc0RW9EsZrSET2YBEmR5IV9tG/eMkq7OlWpbJDNPQUR5ErYbyGRGQPFmFyJDNWdmYU9lT6h2gEfCoCAl3TeoqjyJUwXkMisgeLMDmW7JWdlVu2yUa05x65Hfd/fAZmhEdDb5aGkeJo9EoYryER2UPRNNG1AZF59MznTeorXqkLS2tnBCsadhtqnsoL+LCqboHhFWOmQRRJCvq+RGQ7iCKVts6IrithRGQPFmFyPNnFS3Zh18Pq4pjtlTAisgeLMOUE2cXLylVpKv2L49vHu3DgjVfxv+pqWByJPIZFmHKKzJWdU7ZsOzo6sGjRIrzzzjum/RlE5EwswuR5dm/ZxmIxFBQU4NKlS/D7mZ9D5CUswkQOMHHiRDQ1NeGGG26w+1GIyEL82k1ks0h3FNNuvQf/9mILrh3VhYI8P4oLR2Epz4eJXI8rYSKbtHZGsPaldrzcfhqxWA+gXv1OHPSr0DRgXsk41FaVYAavERG5EoswkQ3s7s5OJdIdxcZ9x3Ck6zy6o71ckRNZgEWYyGJ23lNOpf+KPGWXOFfkRKZhESaykJ2JXak4cUVO5CXMjiay0NqX2tFjcN5wTyyOtTvbpT3L1RV5+gIMABqAaCyONZvfQmNzh7RnIPI6FmEii0S6o3i5/XTGgjccDcCeg6cR6Y4KP0trZwRrNrfo2hIHgGgsgTWbW9DWGRF+BiJiESayzMZ9x6AIDhlWFGDTvmPCz+KkFTmRl7EIE1nkSNf5AU1PRvT0JnDk1AWh13DSipzI61iEiSzSHe2V8joXL8eEfr+TVuREXsciTGSRgjw5AXUj8gNCv98pK3IiYhEmskxx4SgE/WJvuaBfRfH4kUKv4ZQVORGxCBNZZmn5JIjeytc0YEn5JKHXcMqKnIg4wIHIdP3jIEfk+/Fud4+h11EAzJ8+TjhCsrhwFIItJ4W2pGWsyImIRZjINOniII0IBnyoXVgi/FxLyyehYVub0GvE4gkUF44SfhYir+N2NJEJGps7sKJhN5pauxCLJ4QLcF92dJmUyMpQQR7mlYyDSIO0pgHf+tVepmcRCWIRJpJMTxxkNvICPunDG2qrShAM+IRegzGWROI4wIFIIpEBDf0F/Sri8TguvrMfP/nmZzFhTIH0MYNGpjmlYsZgCSKvYBEmkujxdc1oau0yvAIeUxDETVPHoXj8SCy+cSLurLkXZYuX42Q035Qxg8kpSiJfGhQAlWWFWFlTYfg1iLyK29FEkojGQQLAxcu9qF8yEzWV07Cz5SRGV/0t3r7oT3mu3NObQCyeQFNrF1Y07Da0LVxdUYR/qpkrdj4MxlgSGcUiTCSJzDjI5FZxbwJQ1PRvU9Exg4e6ziMgGCLCGEsiY3hFiUgSWXGQ+zrO4vW3zxkeM1gaDuk6n2WMJZF9uBImkkRWHGT7yfcsHTPIGEsi+7AIE0kiKw7yvUs9lo4ZZIwlkX1YhIkkkTGgQVUARfBgWe/5rFMGSxB5EYswkSSyBjTEE2Ivovd81imDJYi8iEWYSBLROEgFQKggKOVZ9JzPJp/bKFmDJYi8iEWYSCKROMhgwIdp14+W8hx6zmdbOyNCTVWyBksQeRGLMJFEM8Ih1C8uQ15A31srOaChvOg6S89nk4Mm3jh6ztCfJXOwBJEX8Z4wkWTJQQtrNregJ8MQBwV9K8n6xWWorihCpDsqPGZQ04BLPb343guvps2ZFsmOHvzcRGQMs6OJTNLWGcHane3Yc3DoPOGgX4UGYH7JONQuLBmwkhTNn1YABPxq2pxpAEKDJm6c8iF8dvFMroCJBLEIE5ks0h3Fpn3HcOTUBVy8HMOI/ACKx4/EkmEmIMmaxJRKcgU7ccy1OHLqgqFCz4ENRPKwCBM5kKwxg2YJ+FQ898jt7IgmEsTGLCIHqq4oQv3imcgL+IQmHJmFAxuI5GBjFpFDVVcUoTQcSnuuHIsnhIM2jODABiI5WISJHKw0HMLKmoqU58rXh67Bf+48hF6bTpQ4sIFIHIswUQ4IFeShpnLagP9v3a5DUFUFEIy5NIoDG4jE8UyYKEfJmANsFAc2EMnBIkyUo2TNATaCAxuI5GARJspRsuYA68WBDUTysAgT5SgZc4CN4MAGInlYhIlylIw5wHpxYAORXCzCRDlKxvziqYUjswoEUQDkBXyoXzyTAxuIJOIVJaIcVltVgr2HzxjKmQ4GfPhy9Y0AYGjQBBGJY3Y0UY4zkjPdt608cFWrd9AEEYljESZygb5CrH9+MRHZi0WYyCWMzi8mIvuwCBO5DLeViXIHizAREZFNeEWJiIjIJizCRERENmERJiIisgmLMBERkU1YhImIiGzCIkxERGQTFmEiIiKbsAgTERHZhEWYiIjIJizCRERENmERJiIisgmLMBERkU1YhImIiGzCIkxERGQTFmEiIiKbsAgTERHZhEWYiIjIJizCRERENmERJiIisgmLMBERkU1YhImIiGzCIkxERGQTFmEiIiKbsAgTERHZhEWYiIjIJizCRERENmERJiIisgmLMBERkU1YhImIiGzCIkxERGQTFmEiIiKb/H/LgHKqsftQywAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def get_graph(g_complete, sampleArgs): \n",
    "    \n",
    "    g = nx.Graph()\n",
    "    n_complete = len(g_complete)\n",
    "    \n",
    "\n",
    "    if sampleArgs[\"sample\"] == \"bfs\":\n",
    "        \n",
    "        for i in range(0, sampleArgs[\"source_starts\"]):\n",
    "        \n",
    "            print(i)\n",
    "            e = nx.bfs_edges(g_complete, source = random.randint(0, n_complete), depth_limit = sampleArgs[\"depth\"])\n",
    "            g.add_edges_from(list(e))\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    if sampleArgs[\"sample\"] == \"random_walk\":\n",
    "                    \n",
    "        ## create random walks _________\n",
    "        \n",
    "        nodes = []\n",
    "        edges = []\n",
    "        \n",
    "        for i in range(0, sampleArgs[\"source_starts\"]):\n",
    "        \n",
    "            start_node = random.randint(0, n_complete)\n",
    "        \n",
    "            for j in range(0, sampleArgs[\"iters\"]):\n",
    "\n",
    "                nodes.append(start_node)\n",
    "\n",
    "                while sampleArgs[\"random_p\"] <= np.random.rand(1): \n",
    "                    # Extract vertex neighbours vertex neighborhood\n",
    "                    neighbors = [n for n in g_complete.neighbors(nodes[-1])]\n",
    "                    # Set probability of going to a neighbour is uniform\n",
    "                    prob = []\n",
    "                    prob = prob + [1./len(neighbors)] * len(neighbors)\n",
    "                    # Choose a vertex from the vertex neighborhood to start the next random walk\n",
    "                    next_node = np.random.choice(neighbors, p=prob)\n",
    "\n",
    "                    # Append to path\n",
    "                    edges.append((nodes[-1], next_node))\n",
    "                    nodes.append(next_node)\n",
    "\n",
    "        e = set(edges)\n",
    "        g.add_edges_from(list(e))\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    ## random edge selection\n",
    "    if sampleArgs[\"sample\"] == \"random_edge\":\n",
    "                    \n",
    "        ## create random walks _________\n",
    "        \n",
    "        edges = []\n",
    "        \n",
    "        for i in range(0, sampleArgs[\"source_starts\"]):\n",
    "        \n",
    "            start_node = random.randint(0, n_complete)\n",
    "        \n",
    "            for j in range(0, sampleArgs[\"iters\"]):\n",
    "\n",
    "                nodes.append(start_node)\n",
    "\n",
    "                while sampleArgs[\"random_p\"] <= np.random.rand(1): \n",
    "                    # Extract vertex neighbours vertex neighborhood\n",
    "                    neighbors = [n for n in g_complete.neighbors(nodes[-1])]\n",
    "                    # Set probability of going to a neighbour is uniform\n",
    "                    prob = []\n",
    "                    prob = prob + [1./len(neighbors)] * len(neighbors)\n",
    "                    # Choose a vertex from the vertex neighborhood to start the next random walk\n",
    "                    next_node = np.random.choice(neighbors, p=prob)\n",
    "\n",
    "                    # Append to path\n",
    "                    edges.append((nodes[-1], next_node))\n",
    "                    nodes.append(next_node)\n",
    "\n",
    "        e = set(edges)\n",
    "        g.add_edges_from(list(e))    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "        \n",
    "    \n",
    "    \n",
    "    if sampleArgs[\"sample\"] == \"random_jump\":\n",
    "        \n",
    "        ## create random walks _________\n",
    "        \n",
    "        degrees_complete = [d for n, d in g_complete.degree()] \n",
    "        degree_probs = np.asarray(degrees_complete) / sum(degrees_complete)\n",
    "\n",
    "        nodes = []\n",
    "        edges = []\n",
    "\n",
    "        for i in range(0, sampleArgs[\"source_starts\"]):\n",
    "            \n",
    "            if sampleArgs[\"jump_bias\"] == \"uniform\":\n",
    "                next_node = random.randint(0, n_complete)\n",
    "            if sampleArgs[\"jump_bias\"] == \"degree\":  # toDo\n",
    "                next_node = np.random.choice(g_complete.nodes, 1, p=degree_probs)   \n",
    "\n",
    "            nodes.append(int(next_node))\n",
    "\n",
    "            while sampleArgs[\"random_p\"] <= np.random.rand(1): \n",
    "                # Extract vertex neighbours vertex neighborhood\n",
    "                neighbors = [n for n in g_complete.neighbors(nodes[-1])]\n",
    "                # Set probability of going to a neighbour is uniform\n",
    "                prob = [1./len(neighbors)] * len(neighbors)\n",
    "                # Choose a vertex from the vertex neighborhood to start the next random walk\n",
    "                next_node = np.random.choice(neighbors, p=prob)\n",
    "\n",
    "                # Append to path\n",
    "                edges.append((nodes[-1], next_node))\n",
    "                nodes.append(next_node)\n",
    "\n",
    "        e = set(edges)\n",
    "        g.add_edges_from(list(e))\n",
    "\n",
    "\n",
    "    ## Draw____________________________________________________________\n",
    "\n",
    "    if sampleArgs[\"draw\"]:\n",
    "        nx.draw(g, node_color = color_map, with_labels = sampleArgs[\"labels\"])\n",
    "        plt.show()\n",
    "\n",
    "    return g\n",
    "\n",
    "\n",
    "sampleArgs = {\"sample\": \"random_walk\", \"jump_bias\": \"degree\", \"random_p\": 0.15, \"source_starts\": 3, \"iters\": 5, \"depth\": 2, \"draw\": True, \"labels\": False}\n",
    "g = get_graph(g_complete, sampleArgs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def generate_data(dataArgs): \n",
    "    \n",
    "    ## Data ________________________________\n",
    "\n",
    "    G = np.zeros((dataArgs[\"n_graphs\"], *calculate_A_shape(dataArgs[\"n_max\"], diag_offset = dataArgs[\"diag_offset\"])))\n",
    "\n",
    "    ## Ground Truth Labels ______________________________\n",
    "\n",
    "    T = list()\n",
    "\n",
    "    ## Generate Graph Data_______________________________\n",
    "\n",
    "    for i in tqdm(range(0,dataArgs[\"n_graphs\"])):\n",
    "\n",
    "        ## Generate Graph Type ______________________________________________\n",
    "\n",
    "        if dataArgs[\"fix_n\"] == True:\n",
    "            n = dataArgs[\"n_max\"] # generate fixed number of nodes n_max\n",
    "        else:\n",
    "            n = random.randint(1, dataArgs[\"n_max\"]) # generate number of nodes n between 1 and n_max and\n",
    "\n",
    "        p = np.random.rand(1)  # float in range 0 - 1 \n",
    "        g = get_graph(n, p, draw = False)\n",
    "\n",
    "        g, a = sort_adjacency(g)\n",
    "        a = pad_matrix(a, dataArgs[\"n_max\"], dataArgs[\"diag_value\"])  # pad adjacency matrix to allow less nodes than n_max and fill diagonal\n",
    "        a_transformed = reshape_A(a, diag_offset = dataArgs[\"diag_offset\"])\n",
    "\n",
    "\n",
    "        ## Generate Ground Truth features____________________________________\n",
    "\n",
    "        if nx.is_connected(g):\n",
    "            diameter = nx.diameter(g)\n",
    "        else:\n",
    "            diameter = -1\n",
    "\n",
    "        density = nx.density(g)\n",
    "\n",
    "        ## toDO: add more graph topologies and figure out good data structure - dicts?\n",
    "\n",
    "\n",
    "        ## Build Data Arrays___________________________________________________\n",
    "\n",
    "        G[i] = a_transformed\n",
    "\n",
    "        t = dict()\n",
    "        t[\"n\"] = n\n",
    "        t[\"p\"] = p\n",
    "        t[\"diameter\"] = diameter\n",
    "        t[\"density\"] = density\n",
    "\n",
    "        T.append(t)\n",
    "\n",
    "\n",
    "\n",
    "    ## Input and Output Size ___________________________________________________________\n",
    "\n",
    "    T, input_shape, output_shape = prepare_in_out(T, dataArgs[\"diag_offset\"], calculate_A_shape(dataArgs[\"n_max\"], dataArgs[\"diag_offset\"]))\n",
    "    print(\"input_shape:\", input_shape, \", output_shape:\", output_shape)\n",
    "    \n",
    "    return G,T,input_shape,output_shape\n",
    "    \n",
    "dataArgs = {\"n_graphs\": 10000, \"n_max\": 24, \"fix_n\": False, \"diag_offset\": 0, \"diag_value\": 1, \"clip\": True}  #\"diag_offset\" - 1 == full adjacency\n",
    "G, T, input_shape, output_shape = generate_data(dataArgs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# beta-VAE (MLP, Conv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     8,
     29,
     72
    ]
   },
   "outputs": [],
   "source": [
    "# network parameters\n",
    "modelArgs = {\"nn_architecture\": \"mlp\", \"latent_dim\": 2, \"filters\": 16, \"kernel_size\": 3, \"input_shape\": input_shape, \"output_shape\": output_shape}\n",
    "\n",
    "\n",
    "# reparameterization trick\n",
    "# instead of sampling from Q(z|X), sample eps = N(0,I)\n",
    "# then z = z_mean + sqrt(var)*eps\n",
    "\n",
    "def sampling(args):\n",
    "    \"\"\"Reparameterization trick by sampling fr an isotropic unit Gaussian.\n",
    "    # Arguments\n",
    "        args (tensor): mean and log of variance of Q(z|X)\n",
    "    # Returns\n",
    "        z (tensor): sampled latent vector\n",
    "    \"\"\"\n",
    "\n",
    "    z_mean, z_log_var = args\n",
    "    batch = K.shape(z_mean)[0]\n",
    "    dim = K.int_shape(z_mean)[1]\n",
    "    # by default, random_normal has mean=0 and std=1.0\n",
    "    epsilon = K.random_normal(shape=(batch, dim))\n",
    "    return z_mean + K.exp(0.5 * z_log_var) * epsilon\n",
    "\n",
    "\n",
    "\n",
    "## MODEL ______________________________________________________________\n",
    "\n",
    "## Multi-layer Perceptron without convolutions__________________________________\n",
    "\n",
    "if modelArgs[\"nn_architecture\"] == \"mlp\":\n",
    "\n",
    "    ## 1) build encoder model\n",
    "    inputs = Input(shape=modelArgs[\"input_shape\"], name='encoder_input')\n",
    "    x = Dense(128, activation='relu')(inputs)\n",
    "    x = Dense(64, activation='relu')(x)\n",
    "    z_mean = Dense(modelArgs[\"latent_dim\"], name='z_mean')(x)\n",
    "    z_log_var = Dense(modelArgs[\"latent_dim\"], name='z_log_var')(x)\n",
    "\n",
    "    ## 2) build decoder model\n",
    "    latent_inputs = Input(shape=(modelArgs[\"latent_dim\"],), name='z_sampling')\n",
    "    y = Dense(64, activation='relu')(latent_inputs)\n",
    "    y = Dense(128, activation='relu')(y)\n",
    "    outputs = Dense(modelArgs[\"output_shape\"], activation='sigmoid')(y)\n",
    "\n",
    "    # use reparameterization trick to push the sampling out as input\n",
    "    # note that \"output_shape\" isn't necessary with the TensorFlow backend\n",
    "    z = Lambda(sampling, output_shape=(modelArgs[\"latent_dim\"],), name='z')([z_mean, z_log_var])\n",
    "\n",
    "\n",
    "    ## INSTANTIATE ________________________________________________\n",
    "\n",
    "    ## 1) instantiate encoder model\n",
    "    encoder = Model(inputs, [z_mean, z_log_var, z], name='encoder')\n",
    "    encoder.summary()\n",
    "    #plot_model(encoder, to_file='vae_mlp_encoder.png', show_shapes=True)\n",
    "\n",
    "    ## 2) instantiate decoder model\n",
    "    decoder = Model(latent_inputs, outputs, name='decoder')\n",
    "    decoder.summary()\n",
    "    #plot_model(decoder, to_file='vae_mlp_decoder.png', show_shapes=True)\n",
    "\n",
    "    ## 3) instantiate VAE model\n",
    "    outputs = decoder(encoder(inputs)[2])\n",
    "    vae = Model(inputs, outputs, name='vae_graph')\n",
    "    #vae.summary()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Convolutional Neural Network_________________________________\n",
    "\n",
    "if modelArgs[\"nn_architecture\"] == \"conv\":\n",
    "\n",
    "\n",
    "    ## 1) build encoder model____________________________________\n",
    "\n",
    "    inputs = Input(shape=modelArgs[\"input_shape\"], name='encoder_input')\n",
    "    x = inputs\n",
    "\n",
    "    for i in range(2):\n",
    "        modelArgs['filters'] *= 2\n",
    "        x = Conv2D(filters=modelArgs['filters'], kernel_size=modelArgs['kernel_size'], activation='relu', strides=2, padding='same')(x)\n",
    "\n",
    "    # shape info needed to build decoder model\n",
    "    shape = K.int_shape(x)\n",
    "\n",
    "    # generate latent vector Q(z|X)\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(16, activation='relu')(x)\n",
    "    z_mean = Dense(modelArgs[\"latent_dim\"], name='z_mean')(x)\n",
    "    z_log_var = Dense(modelArgs[\"latent_dim\"], name='z_log_var')(x)\n",
    "\n",
    "\n",
    "\n",
    "    ## 2) build decoder model____________________________________\n",
    "\n",
    "    latent_inputs = Input(shape=(modelArgs[\"latent_dim\"],), name='z_sampling')\n",
    "    x = Dense(shape[1] * shape[2] * shape[3], activation='relu')(latent_inputs)\n",
    "    x = Reshape((shape[1], shape[2], shape[3]))(x)\n",
    "\n",
    "    for i in range(2):\n",
    "        x = Conv2DTranspose(filters=modelArgs['filters'], kernel_size=modelArgs['kernel_size'], activation='relu', strides=2, padding='same')(x)\n",
    "        modelArgs['filters'] //= 2\n",
    "\n",
    "    outputs = Conv2DTranspose(filters=1, kernel_size=modelArgs['kernel_size'], activation='sigmoid', padding='same', name='decoder_output')(x)\n",
    "\n",
    "    # use reparameterization trick to push the sampling out as input\n",
    "    # note that \"output_shape\" isn't necessary with the TensorFlow backend\n",
    "    z = Lambda(sampling, output_shape=(modelArgs[\"output_shape\"],), name='z')([z_mean, z_log_var])\n",
    "\n",
    "\n",
    "\n",
    "    ## INSTANTIATE___________________________________\n",
    "\n",
    "    ## 1) instantiate encoder model    \n",
    "    encoder = Model(inputs, [z_mean, z_log_var, z], name='encoder')\n",
    "    encoder.summary()\n",
    "    #plot_model(encoder, to_file='vae_cnn_encoder.png', show_shapes=True)\n",
    "\n",
    "\n",
    "    ## 2) instantiate decoder model\n",
    "    decoder = Model(latent_inputs, outputs, name='decoder')\n",
    "    decoder.summary()\n",
    "    #plot_model(decoder, to_file='vae_cnn_decoder.png', show_shapes=True)\n",
    "\n",
    "\n",
    "    ## 3) instantiate VAE model\n",
    "    outputs = decoder(encoder(inputs)[2])\n",
    "    vae = Model(inputs, outputs, name='conv_vae')\n",
    "    #vae.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     14,
     24,
     53,
     59
    ]
   },
   "outputs": [],
   "source": [
    "## Configs  \n",
    "trainArgs = {\"loss\": \"mse\", \"weights\": \"train\", \"early_stop\": 5, \"batch_size\": 128, \"epochs\": 10, \"beta\": 10, \"data_split\": 0.2}\n",
    "\n",
    "## MLP: beta =\n",
    "## CNN: beta = (latent 2 / beta 25)\n",
    "\n",
    "## Train and Validation Split _______________________________________________\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(G, T, test_size= trainArgs[\"data_split\"], random_state=1, shuffle=True)\n",
    "\n",
    "models = (encoder, decoder)\n",
    "data = (x_test, y_test)\n",
    "\n",
    "\n",
    "## VAE loss = mse_loss or xent_loss + kl_loss_______________________\n",
    "\n",
    "if trainArgs[\"loss\"] == \"mse\":\n",
    "    \n",
    "    if modelArgs[\"nn_architecture\"] == \"mlp\":\n",
    "        reconstruction_loss = mse(inputs, outputs)\n",
    "        reconstruction_loss *= modelArgs[\"input_shape\"]\n",
    "        \n",
    "    if modelArgs[\"nn_architecture\"] == \"conv\":\n",
    "        reconstruction_loss = mse(K.flatten(inputs), K.flatten(outputs))\n",
    "        reconstruction_loss *= modelArgs[\"input_shape\"][0] * modelArgs[\"input_shape\"][1]\n",
    "        \n",
    "if trainArgs[\"loss\"] == \"binary_crossentropy\":\n",
    "    \n",
    "    if modelArgs[\"nn_architecture\"] == \"mlp\":\n",
    "        reconstruction_loss = binary_crossentropy(inputs, outputs)\n",
    "        reconstruction_loss *= modelArgs[\"input_shape\"]\n",
    "        \n",
    "    if modelArgs[\"nn_architecture\"] == \"conv\":\n",
    "        reconstruction_loss = binary_crossentropy(K.flatten(inputs),K.flatten(outputs))\n",
    "        reconstruction_loss *= modelArgs[\"input_shape\"][0] * modelArgs[\"input_shape\"][1]\n",
    "\n",
    "\n",
    "\n",
    "## LOSS _____________________________________________\n",
    "\n",
    "kl_loss = 1 + z_log_var - K.square(z_mean) - K.exp(z_log_var)\n",
    "kl_loss = K.sum(kl_loss, axis=-1)\n",
    "kl_loss *= -0.5\n",
    "vae_loss = K.mean(reconstruction_loss + (trainArgs[\"beta\"] * kl_loss))\n",
    "vae.add_loss(vae_loss)\n",
    "vae.compile(optimizer='adam', metrics=['accuracy'])\n",
    "vae.summary()\n",
    "#plot_model(vae,to_file='vae_mlp.png',show_shapes=True)\n",
    "\n",
    "\n",
    "\n",
    "## TRAIN______________________________________________\n",
    "\n",
    "# load the autoencoder weights\n",
    "\n",
    "if trainArgs[\"weights\"] == \"load\":\n",
    "    \n",
    "    vae.load_weights(\"models/weights/vae_mlp_mnist_latent_dim_\" + str(modelArgs[\"latent_dim\"]) + \".h5\")\n",
    "\n",
    "# train the autoencoder\n",
    "\n",
    "elif trainArgs[\"weights\"] == \"train\":\n",
    "    \n",
    "    # Set callback functions to early stop training and save the best model so far\n",
    "    callbacks = [EarlyStopping(monitor='val_loss', patience=trainArgs[\"early_stop\"]), ModelCheckpoint(filepath=\"models/weights/vae_mlp_mnist_latent_dim_\" + str(modelArgs[\"latent_dim\"]) + \".h5\", save_best_only=True)]\n",
    "    \n",
    "    vae.fit(x_train, epochs=trainArgs[\"epochs\"], batch_size=trainArgs[\"batch_size\"], callbacks=callbacks, validation_data=(x_test, None))\n",
    "    vae.save_weights(\"models/weights/vae_mlp_mnist_latent_dim_\" + str(modelArgs[\"latent_dim\"]) + \".h5\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Latent Space Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iterate through single data dimension and oberseve single latent space dimension  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     2,
     45
    ]
   },
   "outputs": [],
   "source": [
    "def latent_space_feature_correlation(analyzeArgs, modelArgs, models,batch_size=128,model_name=\"vae_graph\"):\n",
    "\n",
    "    encoder, decoder = models  # trained models\n",
    "    \n",
    "    \n",
    "    \n",
    "    if analyzeArgs[\"root_params\"] == 1 or modelArgs[\"latent_dim\"] == 1:\n",
    "        \n",
    "        ## Generate Graph Data_______________________________\n",
    "            \n",
    "        P = np.linspace(0,1,analyzeArgs[\"n_config_graphs\"])  # array 0.1, 0.2 - 1 / n_config_graphs  \n",
    "        n = dataArgs[\"n_max\"]\n",
    "        \n",
    "        ## growth and topol parameters\n",
    "        growth_topol_params = [\"p\",\"density\", \"diameter\", \"cluster_coef\", \"assort\", \"#edges\", \"avg_degree\"]\n",
    "        \n",
    "        ## store graphs and targets\n",
    "        # shape: n_config_graphs, params, upper_A_size\n",
    "        G = np.zeros((analyzeArgs[\"n_config_graphs\"], *calculate_A_shape(dataArgs[\"n_max\"], dataArgs[\"diag_offset\"])))\n",
    "        Growth_Topol = np.zeros((analyzeArgs[\"n_config_graphs\"], len(growth_topol_params)))\n",
    "    \n",
    "        for i, p in enumerate(P):\n",
    "\n",
    "            ## Generate Graph Type ______________________________________________\n",
    "\n",
    "            g = get_graph(int(n), p, draw = False)\n",
    "\n",
    "            g, a = sort_adjacency(g)\n",
    "            a = pad_matrix(a, dataArgs[\"n_max\"], dataArgs[\"diag_value\"])  # pad adjacency matrix to allow less nodes than n_max and fill diagonal\n",
    "            upper_a = reshape_A(a, dataArgs[\"diag_offset\"])\n",
    "\n",
    "\n",
    "            ## Generate Ground Truth features____________________________________\n",
    "\n",
    "            density = nx.density(g)\n",
    "\n",
    "            if nx.is_connected(g):\n",
    "                diameter = nx.diameter(g)\n",
    "            else:\n",
    "                diameter = -1\n",
    "\n",
    "            cluster_coef = nx.average_clustering(g)\n",
    "\n",
    "            if g.number_of_edges() > 0:\n",
    "                assort = nx.degree_assortativity_coefficient(g, x='out', y='in')\n",
    "            else:\n",
    "                assort = 0\n",
    "\n",
    "            edges = g.number_of_edges()\n",
    "\n",
    "            avg_degree = sum(i for i in nx.degree_centrality(g).values()) / len(nx.degree_centrality(g).keys())\n",
    "\n",
    "\n",
    "            ## toDO: add more graph topologies\n",
    "\n",
    "            ## Build Data Arrays___________________________________________________\n",
    "\n",
    "            G[i] = upper_a\n",
    "\n",
    "            Growth_Topol[i,0] = p\n",
    "            Growth_Topol[i,1] = density\n",
    "            Growth_Topol[i,2] = diameter\n",
    "            Growth_Topol[i,3] = cluster_coef\n",
    "            Growth_Topol[i,4] = assort\n",
    "            Growth_Topol[i,5] = edges\n",
    "            Growth_Topol[i,6] = avg_degree\n",
    "\n",
    "  \n",
    "    \n",
    "        ## ENCODER - 2D Digit Classes ______________________________________________\n",
    "\n",
    "        # display a 2D plot of the digit classes in the latent space\n",
    "        z_mean, _, _ = encoder.predict(G, batch_size = batch_size)\n",
    "        \n",
    "        \n",
    "        ## Measure the Mutual Information Gap ____________________________________________\n",
    "        if analyzeArgs[\"metric\"] == \"mig\":\n",
    "            mig = compute_mig(P, np.squeeze(z_mean))\n",
    "            \n",
    "        \n",
    "        ## Visualize Latent Variables x Graph Properties ____________________________\n",
    "        fig, ax = plt.subplots(nrows= z_mean.shape[1], ncols= Growth_Topol.shape[1], figsize=(20, 10))\n",
    "\n",
    "        for latent_z, row in enumerate(ax):  \n",
    "            \n",
    "            if z_mean.shape[1] == 1:   # only one latent variable\n",
    "                \n",
    "                if latent_z == 0:\n",
    "                    y = z_mean[:,0]\n",
    "                    x = Growth_Topol[:,latent_z]\n",
    "                    row.plot(x, y) \n",
    "\n",
    "                else:\n",
    "                    y = z_mean[:,0]\n",
    "                    x = Growth_Topol[:,latent_z]\n",
    "                    #row.scatter(x, y) \n",
    "                    sns.regplot(x, y, color=\"steelblue\", ax=row)\n",
    "\n",
    "                    ## plot trend line\n",
    "                    #x = np.nan_to_num(x)\n",
    "                    #y = np.nan_to_num(y)\n",
    "\n",
    "                    #z = np.polyfit(x, y, 1)\n",
    "                    #p = np.poly1d(z)\n",
    "                    #row.plot(x,p(x),\"steelblue\")\n",
    "                    \n",
    "                ## compute correlation and standardized covariance\n",
    "                corr = round(pearsonr(x,y)[0],3)\n",
    "                cov = round(np.cov(x, y)[0][1]/max(x),3)\n",
    "                row.annotate(\"corr:\"+str(corr)+\", cov:\"+str(cov), xy=(0, 1), xytext=(12, -12), va='top',xycoords='axes fraction', textcoords='offset points')\n",
    "\n",
    "                    \n",
    "            else:                     # multiple latent variables\n",
    "                \n",
    "                for feature, col in enumerate(row):\n",
    "\n",
    "                    if feature == 0:\n",
    "                        y = z_mean[:,latent_z]\n",
    "                        x = Growth_Topol[:,feature]\n",
    "                        col.plot(x, y) \n",
    "\n",
    "                    else:\n",
    "                        y = z_mean[:,latent_z]\n",
    "                        x = Growth_Topol[:,feature]\n",
    "                        #col.scatter(x, y) \n",
    "                        sns.regplot(x, y, color=\"steelblue\", ax=col)\n",
    "\n",
    "                        ## plot trend line\n",
    "                        #x = np.nan_to_num(x)\n",
    "                        #y = np.nan_to_num(y)\n",
    "\n",
    "                        #z = np.polyfit(x, y, 1)\n",
    "                        #p = np.poly1d(z)\n",
    "                        #col.plot(x,p(x),\"steelblue\")\n",
    "            \n",
    "                \n",
    "                    ## compute correlation and standardized covariance\n",
    "                    corr = round(pearsonr(x,y)[0],3)\n",
    "                    cov = round(np.cov(x, y)[0][1]/max(x),3)\n",
    "                    col.annotate(\"corr:\"+str(corr)+\", cov:\"+str(cov), xy=(0, 1), xytext=(12, -12), va='top',xycoords='axes fraction', textcoords='offset points')\n",
    "\n",
    "\n",
    "\n",
    "        ## add row and column titles _____________________\n",
    "        \n",
    "        if z_mean.shape[1] == 1:   # only one latent variable\n",
    "                \n",
    "            cols = [t for t in growth_topol_params]\n",
    "            \n",
    "            for axis, col in zip(ax[:,], cols):\n",
    "                axis.set_title(col, fontweight='bold')\n",
    "                \n",
    "        \n",
    "        if z_mean.shape[1] != 1:   # more than one latent variable\n",
    "            \n",
    "            rows = ['z_{}'.format(row) for row in range(z_mean.shape[-1])]\n",
    "            cols = [t for t in growth_topol_params]\n",
    "\n",
    "            for axis, row in zip(ax[:,0], rows):\n",
    "                axis.set_ylabel(row, rotation=0, size='large', fontweight='bold')\n",
    "            \n",
    "            for axis, col in zip(ax[0], cols):\n",
    "                axis.set_title(col, fontweight='bold')\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "\n",
    "    if analyzeArgs[\"root_params\"] == 2 and modelArgs[\"latent_dim\"] != 1:\n",
    "        \n",
    "        ## Generate Graph Data_______________________________\n",
    "    \n",
    "        N = np.linspace(1,dataArgs[\"n_max\"],analyzeArgs[\"n_config_graphs\"], dtype=int)  # array 1,2,3,4,5 - n_max / n_config_graphs\n",
    "        P = np.linspace(0,1,analyzeArgs[\"n_config_graphs\"])  # array 0.1, 0.2 - 1 / n_config_graphs \n",
    "        \n",
    "        ## growth and topol parameters\n",
    "        growth_params = [\"p\", \"n\"]\n",
    "        topol_params = [\"density\", \"diameter\", \"cluster_coef\", \"assort\", \"#edges\", \"avg_degree\"]\n",
    "\n",
    "        ## store graphs and targets\n",
    "        # shape: n_config_graphs, params, upper_A_size\n",
    "        G = np.zeros((analyzeArgs[\"n_config_graphs\"]**len(growth_params), *calculate_A_shape(dataArgs[\"n_max\"], dataArgs[\"diag_offset\"])))\n",
    "        Growth = np.zeros((analyzeArgs[\"n_config_graphs\"]**len(growth_params), len(growth_params)))\n",
    "        Topol = np.zeros((analyzeArgs[\"n_config_graphs\"]**len(growth_params), len(topol_params)))\n",
    "\n",
    "        ## iterate through topological features\n",
    "        graph_configs = np.asarray(list(itertools.product(P,N)))\n",
    "        \n",
    "\n",
    "        for i, (p,n) in enumerate(graph_configs):\n",
    "\n",
    "            ## Generate Graph Type ______________________________________________\n",
    "\n",
    "            g = get_graph(int(n), p, draw = False)\n",
    "\n",
    "            g, a = sort_adjacency(g)\n",
    "            a = pad_matrix(a, dataArgs[\"n_max\"], dataArgs[\"diag_value\"])  # pad adjacency matrix to allow less nodes than n_max and fill diagonal\n",
    "            upper_a = reshape_A(a, dataArgs[\"diag_offset\"])\n",
    "\n",
    "\n",
    "            ## Generate Ground Truth features____________________________________\n",
    "\n",
    "            density = nx.density(g)\n",
    "\n",
    "            if nx.is_connected(g):\n",
    "                diameter = nx.diameter(g)\n",
    "            else:\n",
    "                diameter = -1\n",
    "\n",
    "            cluster_coef = nx.average_clustering(g)\n",
    "\n",
    "            if g.number_of_edges() > 0:\n",
    "                assort = nx.degree_assortativity_coefficient(g, x='out', y='in')\n",
    "            else:\n",
    "                assort = 0\n",
    "\n",
    "            edges = g.number_of_edges()\n",
    "\n",
    "            avg_degree = sum(i for i in nx.degree_centrality(g).values()) / len(nx.degree_centrality(g).keys())\n",
    "\n",
    "\n",
    "            ## toDO: add more graph topologies\n",
    "\n",
    "            ## Build Data Arrays___________________________________________________\n",
    "\n",
    "            G[i] = upper_a\n",
    "\n",
    "            Growth[i,0] = p\n",
    "            Growth[i,1] = int(n)\n",
    "\n",
    "            Topol[i,0] = density\n",
    "            Topol[i,1] = diameter\n",
    "            Topol[i,2] = cluster_coef\n",
    "            Topol[i,3] = assort\n",
    "            Topol[i,4] = edges\n",
    "            Topol[i,5] = avg_degree\n",
    "  \n",
    "    \n",
    "        ## ENCODER - 2D Digit Classes ______________________________________________\n",
    "\n",
    "        # display a 2D plot of the digit classes in the latent space\n",
    "        z_mean, _, _ = encoder.predict(G, batch_size = batch_size)\n",
    "        \n",
    "                \n",
    "        ## Measure the Mutual Information Gap ____________________________________________\n",
    "        if analyzeArgs[\"metric\"] == \"mig\":\n",
    "            #mi = compute_mi(P, np.squeeze(z_mean))\n",
    "            mig = compute_mig(Growth, z_mean)\n",
    "        \n",
    "        \n",
    "        ##  Reshape Array according to Parameters  \n",
    "        z_mean_growth = np.reshape(z_mean, (analyzeArgs[\"n_config_graphs\"], analyzeArgs[\"n_config_graphs\"], -1))\n",
    "        Growth = np.reshape(Growth,(analyzeArgs[\"n_config_graphs\"], analyzeArgs[\"n_config_graphs\"], -1))\n",
    "            \n",
    "        ## 1.) Growth Parameters________________________________________________________\n",
    "\n",
    "        ## Visualize Latent Variables x Growth Parameters ____________________________\n",
    "\n",
    "        fig, ax = plt.subplots(nrows= z_mean_growth.shape[-1] , ncols= len(growth_params))\n",
    "\n",
    "        for latent_z, row in enumerate(ax):        \n",
    "            for feature, col in enumerate(row):\n",
    "\n",
    "                if feature == 0:\n",
    "                    feature_1 = 1\n",
    "                if feature == 1:\n",
    "                    feature_1 = 0\n",
    "\n",
    "                y = np.mean(z_mean_growth[:,:,latent_z], axis= feature_1)\n",
    "                x = np.mean(Growth[:,:,feature], axis= feature_1)\n",
    "                col.plot(x, y)  \n",
    "\n",
    "                ## compute correlation and standardized covariance\n",
    "                corr = round(pearsonr(x,y)[0],3)\n",
    "                cov = round(np.cov(x, y)[0][1]/max(x),3)\n",
    "                col.annotate(\"corr:\"+str(corr)+\", cov:\"+str(cov), xy=(0, 1), xytext=(12, -12), va='top',xycoords='axes fraction', textcoords='offset points')\n",
    "\n",
    "\n",
    "        ## add row and column titles _____________________\n",
    "\n",
    "        rows = ['z_{}'.format(row) for row in range(z_mean_growth.shape[-1])]\n",
    "        cols = [t for t in growth_params]\n",
    "\n",
    "        for axis, col in zip(ax[0], cols):\n",
    "            axis.set_title(col, fontweight='bold')\n",
    "\n",
    "        for axis, row in zip(ax[:,0], rows):\n",
    "            axis.set_ylabel(row, rotation=0, size='large', fontweight='bold')\n",
    "\n",
    "\n",
    "\n",
    "        ## 2.) Graph Topologies________________________________________________________\n",
    "\n",
    "        ## Visualize Latent Variables x Growth Parameters ____________________________\n",
    "\n",
    "        ##  Reshape Array according to Parameters  \n",
    "        #Topol = np.reshape(Topol,(n_config_graphs, n_config_graphs, -1))\n",
    "\n",
    "        fig, ax = plt.subplots(nrows= z_mean.shape[-1] , ncols= len(topol_params), figsize=(30,10))\n",
    "\n",
    "        for latent_z, row in enumerate(ax):        \n",
    "            for feature, col in enumerate(row):\n",
    "\n",
    "                ## toDO: change sorting\n",
    "                y = z_mean[:,latent_z]\n",
    "                x = Topol[:,feature]\n",
    "                sns.regplot(x, y, color=\"steelblue\", ax=col)\n",
    "                #col.scatter(x, y) \n",
    "\n",
    "                # set axes range\n",
    "                #plt.xlim(-4, 4)\n",
    "                #plt.ylim(-4, 4)\n",
    "\n",
    "               # try:\n",
    "               #     ## plot trend line\n",
    "               #     x = np.nan_to_num(x)\n",
    "               #     y = np.nan_to_num(y)\n",
    "\n",
    "               #     z = np.polyfit(x, y, 1)\n",
    "               #     p = np.poly1d(z)\n",
    "               #     col.plot(x,p(x),\"steelblue\")\n",
    "               # except:\n",
    "               #     pass\n",
    "\n",
    "\n",
    "                ## compute correlation and standardized covariance\n",
    "                corr = round(pearsonr(x,y)[0],3)\n",
    "                cov = round(np.cov(x, y)[0][1]/max(x),3)\n",
    "                col.annotate(\"corr:\"+str(corr)+\", cov:\"+str(cov), xy=(0, 1), xytext=(12, -12), va='top',xycoords='axes fraction', textcoords='offset points')\n",
    "\n",
    "\n",
    "\n",
    "        ## add row and column titles _____________________\n",
    "\n",
    "        rows = ['z_{}'.format(row) for row in range(z_mean.shape[-1])]\n",
    "        cols = [t for t in topol_params]\n",
    "\n",
    "        for axis, col in zip(ax[0], cols):\n",
    "            axis.set_title(col, fontweight='bold')\n",
    "\n",
    "        for axis, row in zip(ax[:,0], rows):\n",
    "            axis.set_ylabel(row, rotation=0, size='large', fontweight='bold')\n",
    "\n",
    "\n",
    "            \n",
    "            \n",
    "\n",
    "\n",
    "## PLOT RESULTS ________________________________________\n",
    "\n",
    "analyzeArgs = {\"root_params\": 2, \"n_config_graphs\": 40, \"metric\": \"mig\"}\n",
    "latent_space_feature_correlation(analyzeArgs, modelArgs, models, batch_size=trainArgs[\"batch_size\"], model_name=\"vae_mlp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize the Latent Space in 2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "analyzeArgs = {\"save_plots\": False}\n",
    "vis2D(analyzeArgs, modelArgs, models, data, batch_size=trainArgs[\"batch_size\"], model_name=\"vae_mlp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Latent Generative Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "analyzeArgs = {\"z\": [0,1]}\n",
    "visDistr(modelArgs, analyzeArgs, models,data,trainArgs[\"batch_size\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate a Single Graph Instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "analyzeArgs = {\"activations\": [0], \"z\": [0]}\n",
    "generate_single(analyzeArgs, modelArgs, dataArgs, models, color_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Interpolated Manifold from Latent Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# range, normal, z\n",
    "analyzeArgs = {\"z\": [0,1], \"sample\": \"z\", \"act_range\": [-4, 4], \"act_scale\": 1, \"size_of_manifold\": 10, \"save_plots\": False}\n",
    "generate_manifold(analyzeArgs, modelArgs, dataArgs, models, data, color_map, batch_size=trainArgs[\"batch_size\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzeArgs = {\"z\": [0,1], \"plot\": \"distr\", \"sample\": \"range\", \"act_range\": [-4, 4], \"act_scale\": 1, \"size_of_manifold\": 10, \"save_plots\": False}\n",
    "generate_topol_manifold(analyzeArgs, modelArgs, dataArgs, models, data, color_map, batch_size=trainArgs[\"batch_size\"])\n",
    "\n",
    "## \"density\", \"cluster_coef\", \"assort\", \"avg_degree\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
