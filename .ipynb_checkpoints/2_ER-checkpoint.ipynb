{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Erd√∂s-Renyi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "## libs \n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "## Keras\n",
    "from keras.layers import Lambda, Input, Dense, Conv2D, Conv2DTranspose, Flatten, Reshape\n",
    "from keras.models import Model\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.losses import mse, binary_crossentropy\n",
    "from keras.utils import plot_model\n",
    "from keras import backend as K\n",
    "\n",
    "## Basic\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import argparse\n",
    "import os\n",
    "import random\n",
    "import itertools\n",
    "\n",
    "# Computation\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mutual_info_score\n",
    "\n",
    "import scipy\n",
    "from scipy.stats.stats import pearsonr \n",
    "\n",
    "## Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "## Network Processing\n",
    "import networkx as nx\n",
    "from networkx.generators import random_graphs\n",
    "\n",
    "## node colour\n",
    "color_map = [\"steelblue\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Supporting Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "## supporting functions\n",
    "from support.preprocessing import sort_adjacency, reshape_A, calculate_A_shape, reconstruct_adjacency, pad_matrix, unpad_matrix, prepare_in_out\n",
    "from support.metrics import compute_mig, compute_mi\n",
    "from support.generating import generate_single, generate_manifold, generate_topol_manifold, generate_topol_manifold\n",
    "from support.latent_space import vis2D, visDistr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and Preprocessing Network Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Network Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def get_graph(n,p,draw): \n",
    "\n",
    "    g = random_graphs.erdos_renyi_graph(n, p, seed=None, directed=False)\n",
    "\n",
    "    if draw:\n",
    "        nx.draw(g, node_color = color_map, with_labels = True)\n",
    "        plt.show()\n",
    "    \n",
    "    return g\n",
    "\n",
    "g = get_graph(n = 10, p = 0.4, draw = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def generate_data(dataArgs): \n",
    "    \n",
    "    ## Data ________________________________\n",
    "\n",
    "    G = np.zeros((dataArgs[\"n_graphs\"], *calculate_A_shape(dataArgs[\"n_max\"], diag_offset = dataArgs[\"diag_offset\"])))\n",
    "\n",
    "    ## Ground Truth Labels ______________________________\n",
    "\n",
    "    T = list()\n",
    "\n",
    "    ## Generate Graph Data_______________________________\n",
    "\n",
    "    for i in tqdm(range(0,dataArgs[\"n_graphs\"])):\n",
    "\n",
    "        ## Generate Graph Type ______________________________________________\n",
    "\n",
    "        if dataArgs[\"fix_n\"] == True:\n",
    "            n = dataArgs[\"n_max\"] # generate fixed number of nodes n_max\n",
    "        else:\n",
    "            n = random.randint(1, dataArgs[\"n_max\"]) # generate number of nodes n between 1 and n_max and\n",
    "\n",
    "        p = np.random.rand(1)  # float in range 0 - 1 \n",
    "        g = get_graph(n, p, draw = False)\n",
    "\n",
    "        g, a = sort_adjacency(g)\n",
    "        a = pad_matrix(a, dataArgs[\"n_max\"], dataArgs[\"diag_value\"])  # pad adjacency matrix to allow less nodes than n_max and fill diagonal\n",
    "        a_transformed = reshape_A(a, diag_offset = dataArgs[\"diag_offset\"])\n",
    "\n",
    "\n",
    "        ## Build Data Arrays___________________________________________________\n",
    "\n",
    "        G[i] = a_transformed\n",
    "\n",
    "        t = dict()\n",
    "        t[\"n\"] = n\n",
    "        t[\"p\"] = p\n",
    "\n",
    "        T.append(t)\n",
    "\n",
    "\n",
    "\n",
    "    ## Input and Output Size ___________________________________________________________\n",
    "\n",
    "    T, input_shape, output_shape = prepare_in_out(T, dataArgs[\"diag_offset\"], calculate_A_shape(dataArgs[\"n_max\"], dataArgs[\"diag_offset\"]))\n",
    "    print(\"input_shape:\", input_shape, \", output_shape:\", output_shape)\n",
    "    \n",
    "    return G,T,input_shape,output_shape\n",
    "    \n",
    "dataArgs = {\"n_graphs\": 1000, \"n_max\": 24, \"fix_n\": False, \"diag_offset\": 0, \"diag_value\": 1, \"clip\": True}  #\"diag_offset\" - 1 == full adjacency\n",
    "G, T, input_shape, output_shape = generate_data(dataArgs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# beta-VAE (MLP, Conv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     8,
     29,
     72
    ]
   },
   "outputs": [],
   "source": [
    "# network parameters\n",
    "modelArgs = {\"nn_architecture\": \"mlp\", \"latent_dim\": 2, \"filters\": 16, \"kernel_size\": 3, \"input_shape\": input_shape, \"output_shape\": output_shape}\n",
    "\n",
    "\n",
    "# reparameterization trick\n",
    "# instead of sampling from Q(z|X), sample eps = N(0,I)\n",
    "# then z = z_mean + sqrt(var)*eps\n",
    "\n",
    "def sampling(args):\n",
    "    \"\"\"Reparameterization trick by sampling fr an isotropic unit Gaussian.\n",
    "    # Arguments\n",
    "        args (tensor): mean and log of variance of Q(z|X)\n",
    "    # Returns\n",
    "        z (tensor): sampled latent vector\n",
    "    \"\"\"\n",
    "\n",
    "    z_mean, z_log_var = args\n",
    "    batch = K.shape(z_mean)[0]\n",
    "    dim = K.int_shape(z_mean)[1]\n",
    "    # by default, random_normal has mean=0 and std=1.0\n",
    "    epsilon = K.random_normal(shape=(batch, dim))\n",
    "    return z_mean + K.exp(0.5 * z_log_var) * epsilon\n",
    "\n",
    "\n",
    "\n",
    "## MODEL ______________________________________________________________\n",
    "\n",
    "## Multi-layer Perceptron without convolutions__________________________________\n",
    "\n",
    "if modelArgs[\"nn_architecture\"] == \"mlp\":\n",
    "\n",
    "    ## 1) build encoder model\n",
    "    inputs = Input(shape=modelArgs[\"input_shape\"], name='encoder_input')\n",
    "    x = Dense(128, activation='relu')(inputs)\n",
    "    x = Dense(64, activation='relu')(x)\n",
    "    z_mean = Dense(modelArgs[\"latent_dim\"], name='z_mean')(x)\n",
    "    z_log_var = Dense(modelArgs[\"latent_dim\"], name='z_log_var')(x)\n",
    "\n",
    "    ## 2) build decoder model\n",
    "    latent_inputs = Input(shape=(modelArgs[\"latent_dim\"],), name='z_sampling')\n",
    "    y = Dense(64, activation='relu')(latent_inputs)\n",
    "    y = Dense(128, activation='relu')(y)\n",
    "    outputs = Dense(modelArgs[\"output_shape\"], activation='sigmoid')(y)\n",
    "\n",
    "    # use reparameterization trick to push the sampling out as input\n",
    "    # note that \"output_shape\" isn't necessary with the TensorFlow backend\n",
    "    z = Lambda(sampling, output_shape=(modelArgs[\"latent_dim\"],), name='z')([z_mean, z_log_var])\n",
    "\n",
    "\n",
    "    ## INSTANTIATE ________________________________________________\n",
    "\n",
    "    ## 1) instantiate encoder model\n",
    "    encoder = Model(inputs, [z_mean, z_log_var, z], name='encoder')\n",
    "    encoder.summary()\n",
    "    #plot_model(encoder, to_file='vae_mlp_encoder.png', show_shapes=True)\n",
    "\n",
    "    ## 2) instantiate decoder model\n",
    "    decoder = Model(latent_inputs, outputs, name='decoder')\n",
    "    decoder.summary()\n",
    "    #plot_model(decoder, to_file='vae_mlp_decoder.png', show_shapes=True)\n",
    "\n",
    "    ## 3) instantiate VAE model\n",
    "    outputs = decoder(encoder(inputs)[2])\n",
    "    vae = Model(inputs, outputs, name='vae_graph')\n",
    "    #vae.summary()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Convolutional Neural Network_________________________________\n",
    "\n",
    "if modelArgs[\"nn_architecture\"] == \"conv\":\n",
    "\n",
    "\n",
    "    ## 1) build encoder model____________________________________\n",
    "\n",
    "    inputs = Input(shape=modelArgs[\"input_shape\"], name='encoder_input')\n",
    "    x = inputs\n",
    "\n",
    "    for i in range(2):\n",
    "        modelArgs['filters'] *= 2\n",
    "        x = Conv2D(filters=modelArgs['filters'], kernel_size=modelArgs['kernel_size'], activation='relu', strides=2, padding='same')(x)\n",
    "\n",
    "    # shape info needed to build decoder model\n",
    "    shape = K.int_shape(x)\n",
    "\n",
    "    # generate latent vector Q(z|X)\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(16, activation='relu')(x)\n",
    "    z_mean = Dense(modelArgs[\"latent_dim\"], name='z_mean')(x)\n",
    "    z_log_var = Dense(modelArgs[\"latent_dim\"], name='z_log_var')(x)\n",
    "\n",
    "\n",
    "\n",
    "    ## 2) build decoder model____________________________________\n",
    "\n",
    "    latent_inputs = Input(shape=(modelArgs[\"latent_dim\"],), name='z_sampling')\n",
    "    x = Dense(shape[1] * shape[2] * shape[3], activation='relu')(latent_inputs)\n",
    "    x = Reshape((shape[1], shape[2], shape[3]))(x)\n",
    "\n",
    "    for i in range(2):\n",
    "        x = Conv2DTranspose(filters=modelArgs['filters'], kernel_size=modelArgs['kernel_size'], activation='relu', strides=2, padding='same')(x)\n",
    "        modelArgs['filters'] //= 2\n",
    "\n",
    "    outputs = Conv2DTranspose(filters=1, kernel_size=modelArgs['kernel_size'], activation='sigmoid', padding='same', name='decoder_output')(x)\n",
    "\n",
    "    # use reparameterization trick to push the sampling out as input\n",
    "    # note that \"output_shape\" isn't necessary with the TensorFlow backend\n",
    "    z = Lambda(sampling, output_shape=(modelArgs[\"output_shape\"],), name='z')([z_mean, z_log_var])\n",
    "\n",
    "\n",
    "\n",
    "    ## INSTANTIATE___________________________________\n",
    "\n",
    "    ## 1) instantiate encoder model    \n",
    "    encoder = Model(inputs, [z_mean, z_log_var, z], name='encoder')\n",
    "    encoder.summary()\n",
    "    #plot_model(encoder, to_file='vae_cnn_encoder.png', show_shapes=True)\n",
    "\n",
    "\n",
    "    ## 2) instantiate decoder model\n",
    "    decoder = Model(latent_inputs, outputs, name='decoder')\n",
    "    decoder.summary()\n",
    "    #plot_model(decoder, to_file='vae_cnn_decoder.png', show_shapes=True)\n",
    "\n",
    "\n",
    "    ## 3) instantiate VAE model\n",
    "    outputs = decoder(encoder(inputs)[2])\n",
    "    vae = Model(inputs, outputs, name='conv_vae')\n",
    "    #vae.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     14,
     24,
     53,
     59
    ]
   },
   "outputs": [],
   "source": [
    "## Configs  \n",
    "trainArgs = {\"loss\": \"mse\", \"weights\": \"train\", \"early_stop\": 5, \"batch_size\": 128, \"epochs\": 10, \"beta\": 10, \"data_split\": 0.2}\n",
    "\n",
    "## MLP: beta =\n",
    "## CNN: beta = (latent 2 / beta 25)\n",
    "\n",
    "## Train and Validation Split _______________________________________________\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(G, T, test_size= trainArgs[\"data_split\"], random_state=1, shuffle=True)\n",
    "\n",
    "models = (encoder, decoder)\n",
    "data = (x_test, y_test)\n",
    "\n",
    "\n",
    "## VAE loss = mse_loss or xent_loss + kl_loss_______________________\n",
    "\n",
    "if trainArgs[\"loss\"] == \"mse\":\n",
    "    \n",
    "    if modelArgs[\"nn_architecture\"] == \"mlp\":\n",
    "        reconstruction_loss = mse(inputs, outputs)\n",
    "        reconstruction_loss *= modelArgs[\"input_shape\"]\n",
    "        \n",
    "    if modelArgs[\"nn_architecture\"] == \"conv\":\n",
    "        reconstruction_loss = mse(K.flatten(inputs), K.flatten(outputs))\n",
    "        reconstruction_loss *= modelArgs[\"input_shape\"][0] * modelArgs[\"input_shape\"][1]\n",
    "        \n",
    "if trainArgs[\"loss\"] == \"binary_crossentropy\":\n",
    "    \n",
    "    if modelArgs[\"nn_architecture\"] == \"mlp\":\n",
    "        reconstruction_loss = binary_crossentropy(inputs, outputs)\n",
    "        reconstruction_loss *= modelArgs[\"input_shape\"]\n",
    "        \n",
    "    if modelArgs[\"nn_architecture\"] == \"conv\":\n",
    "        reconstruction_loss = binary_crossentropy(K.flatten(inputs),K.flatten(outputs))\n",
    "        reconstruction_loss *= modelArgs[\"input_shape\"][0] * modelArgs[\"input_shape\"][1]\n",
    "\n",
    "\n",
    "\n",
    "## LOSS _____________________________________________\n",
    "\n",
    "kl_loss = 1 + z_log_var - K.square(z_mean) - K.exp(z_log_var)\n",
    "kl_loss = K.sum(kl_loss, axis=-1)\n",
    "kl_loss *= -0.5\n",
    "vae_loss = K.mean(reconstruction_loss + (trainArgs[\"beta\"] * kl_loss))\n",
    "vae.add_loss(vae_loss)\n",
    "vae.compile(optimizer='adam', metrics=['accuracy'])\n",
    "vae.summary()\n",
    "#plot_model(vae,to_file='vae_mlp.png',show_shapes=True)\n",
    "\n",
    "\n",
    "\n",
    "## TRAIN______________________________________________\n",
    "\n",
    "# load the autoencoder weights\n",
    "\n",
    "if trainArgs[\"weights\"] == \"load\":\n",
    "    \n",
    "    vae.load_weights(\"models/weights/vae_mlp_mnist_latent_dim_\" + str(modelArgs[\"latent_dim\"]) + \".h5\")\n",
    "\n",
    "# train the autoencoder\n",
    "\n",
    "elif trainArgs[\"weights\"] == \"train\":\n",
    "    \n",
    "    # Set callback functions to early stop training and save the best model so far\n",
    "    callbacks = [EarlyStopping(monitor='val_loss', patience=trainArgs[\"early_stop\"]), ModelCheckpoint(filepath=\"models/weights/vae_mlp_mnist_latent_dim_\" + str(modelArgs[\"latent_dim\"]) + \".h5\", save_best_only=True)]\n",
    "    \n",
    "    vae.fit(x_train, epochs=trainArgs[\"epochs\"], batch_size=trainArgs[\"batch_size\"], callbacks=callbacks, validation_data=(x_test, None))\n",
    "    vae.save_weights(\"models/weights/vae_mlp_mnist_latent_dim_\" + str(modelArgs[\"latent_dim\"]) + \".h5\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Latent Space Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iterate through single data dimension and oberseve single latent space dimension  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     2,
     45
    ]
   },
   "outputs": [],
   "source": [
    "def latent_space_feature_correlation(analyzeArgs, modelArgs, models,batch_size=128,model_name=\"vae_graph\"):\n",
    "\n",
    "    encoder, decoder = models  # trained models\n",
    "    \n",
    "    \n",
    "    \n",
    "    if analyzeArgs[\"root_params\"] == 1 or modelArgs[\"latent_dim\"] == 1:\n",
    "        \n",
    "        ## Generate Graph Data_______________________________\n",
    "            \n",
    "        P = np.linspace(0,1,analyzeArgs[\"n_config_graphs\"])  # array 0.1, 0.2 - 1 / n_config_graphs  \n",
    "        n = dataArgs[\"n_max\"]\n",
    "        \n",
    "        ## growth and topol parameters\n",
    "        growth_topol_params = [\"p\",\"density\", \"diameter\", \"cluster_coef\", \"assort\", \"#edges\", \"avg_degree\"]\n",
    "        \n",
    "        ## store graphs and targets\n",
    "        # shape: n_config_graphs, params, upper_A_size\n",
    "        G = np.zeros((analyzeArgs[\"n_config_graphs\"], *calculate_A_shape(dataArgs[\"n_max\"], dataArgs[\"diag_offset\"])))\n",
    "        Growth_Topol = np.zeros((analyzeArgs[\"n_config_graphs\"], len(growth_topol_params)))\n",
    "    \n",
    "        for i, p in enumerate(P):\n",
    "\n",
    "            ## Generate Graph Type ______________________________________________\n",
    "\n",
    "            g = get_graph(int(n), p, draw = False)\n",
    "\n",
    "            g, a = sort_adjacency(g)\n",
    "            a = pad_matrix(a, dataArgs[\"n_max\"], dataArgs[\"diag_value\"])  # pad adjacency matrix to allow less nodes than n_max and fill diagonal\n",
    "            upper_a = reshape_A(a, dataArgs[\"diag_offset\"])\n",
    "\n",
    "\n",
    "            ## Generate Ground Truth features____________________________________\n",
    "\n",
    "            density = nx.density(g)\n",
    "\n",
    "            if nx.is_connected(g):\n",
    "                diameter = nx.diameter(g)\n",
    "            else:\n",
    "                diameter = -1\n",
    "\n",
    "            cluster_coef = nx.average_clustering(g)\n",
    "\n",
    "            if g.number_of_edges() > 0:\n",
    "                assort = nx.degree_assortativity_coefficient(g, x='out', y='in')\n",
    "            else:\n",
    "                assort = 0\n",
    "\n",
    "            edges = g.number_of_edges()\n",
    "\n",
    "            avg_degree = sum(i for i in nx.degree_centrality(g).values()) / len(nx.degree_centrality(g).keys())\n",
    "\n",
    "\n",
    "            ## toDO: add more graph topologies\n",
    "\n",
    "            ## Build Data Arrays___________________________________________________\n",
    "\n",
    "            G[i] = upper_a\n",
    "\n",
    "            Growth_Topol[i,0] = p\n",
    "            Growth_Topol[i,1] = density\n",
    "            Growth_Topol[i,2] = diameter\n",
    "            Growth_Topol[i,3] = cluster_coef\n",
    "            Growth_Topol[i,4] = assort\n",
    "            Growth_Topol[i,5] = edges\n",
    "            Growth_Topol[i,6] = avg_degree\n",
    "\n",
    "  \n",
    "    \n",
    "        ## ENCODER - 2D Digit Classes ______________________________________________\n",
    "\n",
    "        # display a 2D plot of the digit classes in the latent space\n",
    "        z_mean, _, _ = encoder.predict(G, batch_size = batch_size)\n",
    "        \n",
    "        \n",
    "        ## Measure the Mutual Information Gap ____________________________________________\n",
    "        if analyzeArgs[\"metric\"] == \"mig\":\n",
    "            mig = compute_mig(P, np.squeeze(z_mean))\n",
    "            \n",
    "        \n",
    "        ## Visualize Latent Variables x Graph Properties ____________________________\n",
    "        fig, ax = plt.subplots(nrows= z_mean.shape[1], ncols= Growth_Topol.shape[1], figsize=(20, 10))\n",
    "\n",
    "        for latent_z, row in enumerate(ax):  \n",
    "            \n",
    "            if z_mean.shape[1] == 1:   # only one latent variable\n",
    "                \n",
    "                if latent_z == 0:\n",
    "                    y = z_mean[:,0]\n",
    "                    x = Growth_Topol[:,latent_z]\n",
    "                    row.plot(x, y) \n",
    "\n",
    "                else:\n",
    "                    y = z_mean[:,0]\n",
    "                    x = Growth_Topol[:,latent_z]\n",
    "                    #row.scatter(x, y) \n",
    "                    sns.regplot(x, y, color=\"steelblue\", ax=row)\n",
    "\n",
    "                    ## plot trend line\n",
    "                    #x = np.nan_to_num(x)\n",
    "                    #y = np.nan_to_num(y)\n",
    "\n",
    "                    #z = np.polyfit(x, y, 1)\n",
    "                    #p = np.poly1d(z)\n",
    "                    #row.plot(x,p(x),\"steelblue\")\n",
    "                    \n",
    "                ## compute correlation and standardized covariance\n",
    "                corr = round(pearsonr(x,y)[0],3)\n",
    "                cov = round(np.cov(x, y)[0][1]/max(x),3)\n",
    "                row.annotate(\"corr:\"+str(corr)+\", cov:\"+str(cov), xy=(0, 1), xytext=(12, -12), va='top',xycoords='axes fraction', textcoords='offset points')\n",
    "\n",
    "                    \n",
    "            else:                     # multiple latent variables\n",
    "                \n",
    "                for feature, col in enumerate(row):\n",
    "\n",
    "                    if feature == 0:\n",
    "                        y = z_mean[:,latent_z]\n",
    "                        x = Growth_Topol[:,feature]\n",
    "                        col.plot(x, y) \n",
    "\n",
    "                    else:\n",
    "                        y = z_mean[:,latent_z]\n",
    "                        x = Growth_Topol[:,feature]\n",
    "                        #col.scatter(x, y) \n",
    "                        sns.regplot(x, y, color=\"steelblue\", ax=col)\n",
    "\n",
    "                        ## plot trend line\n",
    "                        #x = np.nan_to_num(x)\n",
    "                        #y = np.nan_to_num(y)\n",
    "\n",
    "                        #z = np.polyfit(x, y, 1)\n",
    "                        #p = np.poly1d(z)\n",
    "                        #col.plot(x,p(x),\"steelblue\")\n",
    "            \n",
    "                \n",
    "                    ## compute correlation and standardized covariance\n",
    "                    corr = round(pearsonr(x,y)[0],3)\n",
    "                    cov = round(np.cov(x, y)[0][1]/max(x),3)\n",
    "                    col.annotate(\"corr:\"+str(corr)+\", cov:\"+str(cov), xy=(0, 1), xytext=(12, -12), va='top',xycoords='axes fraction', textcoords='offset points')\n",
    "\n",
    "\n",
    "\n",
    "        ## add row and column titles _____________________\n",
    "        \n",
    "        if z_mean.shape[1] == 1:   # only one latent variable\n",
    "                \n",
    "            cols = [t for t in growth_topol_params]\n",
    "            \n",
    "            for axis, col in zip(ax[:,], cols):\n",
    "                axis.set_title(col, fontweight='bold')\n",
    "                \n",
    "        \n",
    "        if z_mean.shape[1] != 1:   # more than one latent variable\n",
    "            \n",
    "            rows = ['z_{}'.format(row) for row in range(z_mean.shape[-1])]\n",
    "            cols = [t for t in growth_topol_params]\n",
    "\n",
    "            for axis, row in zip(ax[:,0], rows):\n",
    "                axis.set_ylabel(row, rotation=0, size='large', fontweight='bold')\n",
    "            \n",
    "            for axis, col in zip(ax[0], cols):\n",
    "                axis.set_title(col, fontweight='bold')\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "\n",
    "    if analyzeArgs[\"root_params\"] == 2 and modelArgs[\"latent_dim\"] != 1:\n",
    "        \n",
    "        ## Generate Graph Data_______________________________\n",
    "    \n",
    "        N = np.linspace(1,dataArgs[\"n_max\"],analyzeArgs[\"n_config_graphs\"], dtype=int)  # array 1,2,3,4,5 - n_max / n_config_graphs\n",
    "        P = np.linspace(0,1,analyzeArgs[\"n_config_graphs\"])  # array 0.1, 0.2 - 1 / n_config_graphs \n",
    "        \n",
    "        ## growth and topol parameters\n",
    "        growth_params = [\"p\", \"n\"]\n",
    "        topol_params = [\"density\", \"diameter\", \"cluster_coef\", \"assort\", \"#edges\", \"avg_degree\"]\n",
    "\n",
    "        ## store graphs and targets\n",
    "        # shape: n_config_graphs, params, upper_A_size\n",
    "        G = np.zeros((analyzeArgs[\"n_config_graphs\"]**len(growth_params), *calculate_A_shape(dataArgs[\"n_max\"], dataArgs[\"diag_offset\"])))\n",
    "        Growth = np.zeros((analyzeArgs[\"n_config_graphs\"]**len(growth_params), len(growth_params)))\n",
    "        Topol = np.zeros((analyzeArgs[\"n_config_graphs\"]**len(growth_params), len(topol_params)))\n",
    "\n",
    "        ## iterate through topological features\n",
    "        graph_configs = np.asarray(list(itertools.product(P,N)))\n",
    "        \n",
    "\n",
    "        for i, (p,n) in enumerate(graph_configs):\n",
    "\n",
    "            ## Generate Graph Type ______________________________________________\n",
    "\n",
    "            g = get_graph(int(n), p, draw = False)\n",
    "\n",
    "            g, a = sort_adjacency(g)\n",
    "            a = pad_matrix(a, dataArgs[\"n_max\"], dataArgs[\"diag_value\"])  # pad adjacency matrix to allow less nodes than n_max and fill diagonal\n",
    "            upper_a = reshape_A(a, dataArgs[\"diag_offset\"])\n",
    "\n",
    "\n",
    "            ## Generate Ground Truth features____________________________________\n",
    "\n",
    "            density = nx.density(g)\n",
    "\n",
    "            if nx.is_connected(g):\n",
    "                diameter = nx.diameter(g)\n",
    "            else:\n",
    "                diameter = -1\n",
    "\n",
    "            cluster_coef = nx.average_clustering(g)\n",
    "\n",
    "            if g.number_of_edges() > 0:\n",
    "                assort = nx.degree_assortativity_coefficient(g, x='out', y='in')\n",
    "            else:\n",
    "                assort = 0\n",
    "\n",
    "            edges = g.number_of_edges()\n",
    "\n",
    "            avg_degree = sum(i for i in nx.degree_centrality(g).values()) / len(nx.degree_centrality(g).keys())\n",
    "\n",
    "\n",
    "            ## toDO: add more graph topologies\n",
    "\n",
    "            ## Build Data Arrays___________________________________________________\n",
    "\n",
    "            G[i] = upper_a\n",
    "\n",
    "            Growth[i,0] = p\n",
    "            Growth[i,1] = int(n)\n",
    "\n",
    "            Topol[i,0] = density\n",
    "            Topol[i,1] = diameter\n",
    "            Topol[i,2] = cluster_coef\n",
    "            Topol[i,3] = assort\n",
    "            Topol[i,4] = edges\n",
    "            Topol[i,5] = avg_degree\n",
    "  \n",
    "    \n",
    "        ## ENCODER - 2D Digit Classes ______________________________________________\n",
    "\n",
    "        # display a 2D plot of the digit classes in the latent space\n",
    "        z_mean, _, _ = encoder.predict(G, batch_size = batch_size)\n",
    "        \n",
    "                \n",
    "        ## Measure the Mutual Information Gap ____________________________________________\n",
    "        if analyzeArgs[\"metric\"] == \"mig\":\n",
    "            #mi = compute_mi(P, np.squeeze(z_mean))\n",
    "            mig = compute_mig(Growth, z_mean)\n",
    "        \n",
    "        \n",
    "        ##  Reshape Array according to Parameters  \n",
    "        z_mean_growth = np.reshape(z_mean, (analyzeArgs[\"n_config_graphs\"], analyzeArgs[\"n_config_graphs\"], -1))\n",
    "        Growth = np.reshape(Growth,(analyzeArgs[\"n_config_graphs\"], analyzeArgs[\"n_config_graphs\"], -1))\n",
    "            \n",
    "        ## 1.) Growth Parameters________________________________________________________\n",
    "\n",
    "        ## Visualize Latent Variables x Growth Parameters ____________________________\n",
    "\n",
    "        fig, ax = plt.subplots(nrows= z_mean_growth.shape[-1] , ncols= len(growth_params))\n",
    "\n",
    "        for latent_z, row in enumerate(ax):        \n",
    "            for feature, col in enumerate(row):\n",
    "\n",
    "                if feature == 0:\n",
    "                    feature_1 = 1\n",
    "                if feature == 1:\n",
    "                    feature_1 = 0\n",
    "\n",
    "                y = np.mean(z_mean_growth[:,:,latent_z], axis= feature_1)\n",
    "                x = np.mean(Growth[:,:,feature], axis= feature_1)\n",
    "                col.plot(x, y)  \n",
    "\n",
    "                ## compute correlation and standardized covariance\n",
    "                corr = round(pearsonr(x,y)[0],3)\n",
    "                cov = round(np.cov(x, y)[0][1]/max(x),3)\n",
    "                col.annotate(\"corr:\"+str(corr)+\", cov:\"+str(cov), xy=(0, 1), xytext=(12, -12), va='top',xycoords='axes fraction', textcoords='offset points')\n",
    "\n",
    "\n",
    "        ## add row and column titles _____________________\n",
    "\n",
    "        rows = ['z_{}'.format(row) for row in range(z_mean_growth.shape[-1])]\n",
    "        cols = [t for t in growth_params]\n",
    "\n",
    "        for axis, col in zip(ax[0], cols):\n",
    "            axis.set_title(col, fontweight='bold')\n",
    "\n",
    "        for axis, row in zip(ax[:,0], rows):\n",
    "            axis.set_ylabel(row, rotation=0, size='large', fontweight='bold')\n",
    "\n",
    "\n",
    "\n",
    "        ## 2.) Graph Topologies________________________________________________________\n",
    "\n",
    "        ## Visualize Latent Variables x Growth Parameters ____________________________\n",
    "\n",
    "        ##  Reshape Array according to Parameters  \n",
    "        #Topol = np.reshape(Topol,(n_config_graphs, n_config_graphs, -1))\n",
    "\n",
    "        fig, ax = plt.subplots(nrows= z_mean.shape[-1] , ncols= len(topol_params), figsize=(30,10))\n",
    "\n",
    "        for latent_z, row in enumerate(ax):        \n",
    "            for feature, col in enumerate(row):\n",
    "\n",
    "                ## toDO: change sorting\n",
    "                y = z_mean[:,latent_z]\n",
    "                x = Topol[:,feature]\n",
    "                sns.regplot(x, y, color=\"steelblue\", ax=col)\n",
    "                #col.scatter(x, y) \n",
    "\n",
    "                # set axes range\n",
    "                #plt.xlim(-4, 4)\n",
    "                #plt.ylim(-4, 4)\n",
    "\n",
    "               # try:\n",
    "               #     ## plot trend line\n",
    "               #     x = np.nan_to_num(x)\n",
    "               #     y = np.nan_to_num(y)\n",
    "\n",
    "               #     z = np.polyfit(x, y, 1)\n",
    "               #     p = np.poly1d(z)\n",
    "               #     col.plot(x,p(x),\"steelblue\")\n",
    "               # except:\n",
    "               #     pass\n",
    "\n",
    "\n",
    "                ## compute correlation and standardized covariance\n",
    "                corr = round(pearsonr(x,y)[0],3)\n",
    "                cov = round(np.cov(x, y)[0][1]/max(x),3)\n",
    "                col.annotate(\"corr:\"+str(corr)+\", cov:\"+str(cov), xy=(0, 1), xytext=(12, -12), va='top',xycoords='axes fraction', textcoords='offset points')\n",
    "\n",
    "\n",
    "\n",
    "        ## add row and column titles _____________________\n",
    "\n",
    "        rows = ['z_{}'.format(row) for row in range(z_mean.shape[-1])]\n",
    "        cols = [t for t in topol_params]\n",
    "\n",
    "        for axis, col in zip(ax[0], cols):\n",
    "            axis.set_title(col, fontweight='bold')\n",
    "\n",
    "        for axis, row in zip(ax[:,0], rows):\n",
    "            axis.set_ylabel(row, rotation=0, size='large', fontweight='bold')\n",
    "\n",
    "\n",
    "            \n",
    "            \n",
    "\n",
    "\n",
    "## PLOT RESULTS ________________________________________\n",
    "\n",
    "analyzeArgs = {\"root_params\": 2, \"n_config_graphs\": 40, \"metric\": \"mig\"}\n",
    "latent_space_feature_correlation(analyzeArgs, modelArgs, models, batch_size=trainArgs[\"batch_size\"], model_name=\"vae_mlp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize the Latent Space in 2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "analyzeArgs = {\"save_plots\": False}\n",
    "vis2D(analyzeArgs, modelArgs, models, data, batch_size=trainArgs[\"batch_size\"], model_name=\"vae_mlp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Latent Generative Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "analyzeArgs = {\"z\": [0,1]}\n",
    "visDistr(modelArgs, analyzeArgs, models,data,trainArgs[\"batch_size\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate a Single Graph Instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "analyzeArgs = {\"activations\": [0], \"z\": [0]}\n",
    "generate_single(analyzeArgs, modelArgs, dataArgs, models, color_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Interpolated Manifold from Latent Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# range, normal, z\n",
    "analyzeArgs = {\"z\": [0,1], \"sample\": \"normal\", \"act_range\": [-4, 4], \"act_scale\": 1, \"size_of_manifold\": 10, \"save_plots\": False}\n",
    "generate_manifold(analyzeArgs, modelArgs, dataArgs, models, data, color_map, batch_size=trainArgs[\"batch_size\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzeArgs = {\"z\": [0,1], \"plot\": \"topol\", \"sample\": \"range\", \"act_range\": [-4, 4], \"act_scale\": 1, \"size_of_manifold\": 10, \"save_plots\": False}\n",
    "generate_topol_manifold(analyzeArgs, modelArgs, dataArgs, models, data, color_map, batch_size=trainArgs[\"batch_size\"])\n",
    "\n",
    "## \"density\", \"cluster_coef\", \"assort\", \"avg_degree\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
