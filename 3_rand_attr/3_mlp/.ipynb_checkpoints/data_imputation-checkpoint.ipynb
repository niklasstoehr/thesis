{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Imputation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "## libs \n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "## Keras\n",
    "from keras.layers import Lambda, Input, Dense, Conv2D, Conv2DTranspose, Flatten, Reshape\n",
    "from keras.models import Model\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.losses import mse, binary_crossentropy\n",
    "from keras.utils import plot_model\n",
    "from keras import backend as K\n",
    "\n",
    "## Basic\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import argparse\n",
    "import os\n",
    "import random\n",
    "import itertools\n",
    "import time\n",
    "\n",
    "# Computation\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mutual_info_score\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "import scipy\n",
    "from scipy.stats.stats import pearsonr \n",
    "\n",
    "## Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import seaborn as sns\n",
    "\n",
    "## Network Processing\n",
    "import networkx as nx\n",
    "from networkx.generators import random_graphs\n",
    "\n",
    "## node colour\n",
    "orig_cmap = plt.cm.PuBu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Supporting Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "## supporting functions\n",
    "from support.preprocessing import sort_adj, reshape_A, calculate_A_shape, reconstruct_adjacency, pad_matrix, unpad_matrix, pad_attr, unpad_attr, prepare_in_out\n",
    "from support.metrics import compute_mig, compute_mi\n",
    "from support.graph_generating import generate_single_features, generate_manifold_features\n",
    "from support.latent_space import vis2D, visDistr\n",
    "from support.comparing import compare_manifold_adjacency, compare_topol_manifold\n",
    "from support.plotting import shiftedColorMap\n",
    "\n",
    "## graph sampling\n",
    "from sampling import ForestFire, Metropolis_Hastings, Random_Walk, Snowball, Ties, Base_Samplers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def get_graph(n,p,draw): \n",
    "\n",
    "    g = random_graphs.erdos_renyi_graph(n, p, seed=None, directed=False)\n",
    "\n",
    "    if draw:\n",
    "        f = np.random.rand(n)\n",
    "        orig_cmap = plt.cm.PuBu\n",
    "        fixed_cmap = shiftedColorMap(orig_cmap, start=min(f), midpoint=0.5, stop=max(f), name='fixed')\n",
    "        nx.draw(g, node_color=f, font_color='white', cmap = fixed_cmap)\n",
    "        plt.show()\n",
    "    \n",
    "    return g\n",
    "\n",
    "g = get_graph(n = 10, p = 0.4, draw = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def generate_features(dataArgs, g, n, p):\n",
    "    \n",
    "        if dataArgs[\"feature_dependence\"] == \"random\":\n",
    "            f = np.random.rand(n, dataArgs[\"n_features\"])                   ## float\n",
    "            #F[i] = np.random.randint(2, size=(dataArgs[\"n_max\"],dataArgs[\"n_features\"]))   ## int\n",
    "            \n",
    "        if dataArgs[\"feature_dependence\"] == \"norm_degree\":\n",
    "            if dataArgs[\"n_features\"] == 1:\n",
    "                \n",
    "                f = np.asarray([int(x[1]) for x in sorted(g.degree())])  \n",
    "                f = (f) / (max(f)+1)\n",
    "                f = np.reshape(f, (f.shape[-1],1))\n",
    "                \n",
    "                    \n",
    "        if dataArgs[\"feature_dependence\"] == \"degree\":\n",
    "            if dataArgs[\"n_features\"] == 1:\n",
    "                \n",
    "                f = np.asarray([int(x[1]) for x in sorted(g.degree())])  \n",
    "                f = (f+1) / (dataArgs[\"n_max\"]+1)\n",
    "                f = np.reshape(f, (f.shape[-1],1))\n",
    "                \n",
    "                \n",
    "        if dataArgs[\"feature_dependence\"] == \"p\":  \n",
    "            if dataArgs[\"n_features\"] == 1:\n",
    "                f = np.ones((n , 1)) * p\n",
    "                \n",
    "        return f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_in_out(diag_offset, A_shape, F_shape):\n",
    "\n",
    "    if diag_offset == 0:  # matrix input\n",
    "        return (F_shape[1], A_shape[0]) , (F_shape[1], A_shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def generate_data(dataArgs): \n",
    "    \n",
    "    \n",
    "    ## Data ________________________________\n",
    "\n",
    "    G = np.zeros((dataArgs[\"n_graphs\"], *calculate_A_shape(dataArgs[\"n_max\"], dataArgs[\"diag_offset\"])))\n",
    "    F = np.zeros((dataArgs[\"n_graphs\"], dataArgs[\"n_max\"], dataArgs[\"n_features\"]))\n",
    "    print(\"feature_dependence:\",dataArgs[\"feature_dependence\"] )\n",
    "    \n",
    "    \n",
    "    ## Generate Graph Data_______________________________\n",
    "\n",
    "    for i in tqdm(range(0,dataArgs[\"n_graphs\"])):\n",
    "        \n",
    "        ## Generate Graph Type ______________________________________________\n",
    "\n",
    "        if dataArgs[\"fix_n\"] == True:\n",
    "            n = dataArgs[\"n_max\"] # generate fixed number of nodes n_max\n",
    "        else:\n",
    "            n = random.randint(1, dataArgs[\"n_max\"]) # generate number of nodes n between 1 and n_max and\n",
    "\n",
    "            \n",
    "        p = np.random.rand(1)  # float in range 0 - 1 \n",
    "        g = get_graph(n, p, draw = False)\n",
    "                     \n",
    "        #nx.draw(g, cmap=plt.get_cmap('PuBu'), node_color=np.squeeze(f), font_color='white')\n",
    "        #plt.show()\n",
    "        \n",
    "        g, a = sort_adj(g)\n",
    "        a = pad_matrix(a, dataArgs[\"n_max\"], dataArgs[\"diag_value\"])  # pad adjacency matrix to allow less nodes than n_max and fill diagonal        \n",
    "        a_transformed = reshape_A(a, diag_offset = dataArgs[\"diag_offset\"])\n",
    "        \n",
    "        \n",
    "        ## Generate / Load Node Features ______________________________________________\n",
    "        f = generate_features(dataArgs, g, n, p)\n",
    "        \n",
    "        ## pad features with zeroes\n",
    "        f = pad_attr(f, dataArgs)\n",
    "\n",
    "        \n",
    "        ## Build Data Arrays___________________________________________________\n",
    "\n",
    "        F[i] = f\n",
    "        G[i] = a_transformed\n",
    "\n",
    "\n",
    "    ## Input and Output Size ___________________________________________________________\n",
    "\n",
    "    input_shape, output_shape = prepare_in_out(dataArgs[\"diag_offset\"], calculate_A_shape(dataArgs[\"n_max\"], dataArgs[\"diag_offset\"]), F.shape)\n",
    "    print(\"input_shape:\", input_shape, \", output_shape:\", output_shape)\n",
    "    \n",
    "    ## scale features in F for smoother training\n",
    "    #scaler = MinMaxScaler()\n",
    "    #scaler.fit(F)\n",
    "    #F = scaler.transform(F)\n",
    "    \n",
    "    return G, F, input_shape,output_shape\n",
    "    \n",
    "dataArgs = {\"n_graphs\": 100, \"n_max\": 12, \"feature_dependence\": \"degree\", \"fix_n\": False, \"diag_offset\": 0, \"diag_value\": 1, \"clip\": True, \"n_features\": 1}  #\"diag_offset\" - 1 == full adjacency\n",
    "G, F, input_shape, output_shape = generate_data(dataArgs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def imput_data(imputeArgs, modelArgs, dataArgs, G, F):\n",
    "        \n",
    "    G_imputed = np.copy(G)\n",
    "    F_imputed = np.copy(F)\n",
    "    \n",
    "    if imputeArgs[\"impute\"] == \"features\":\n",
    "        \n",
    "        for i, (g,f) in enumerate(zip(G_imputed, F_imputed)):\n",
    "            \n",
    "            f = np.squeeze(f)\n",
    "            num_features = len(f[f > 0])\n",
    "            impute_num = int(imputeArgs[\"impute_frac\"] * num_features)\n",
    "            impute_f_ind = random.sample(range(num_features), impute_num)  ## impute features\n",
    "\n",
    "            row, col = np.diag_indices(f.shape[0])\n",
    "\n",
    "            f[impute_f_ind] = imputeArgs[\"impute_value\"]  ## replace features\n",
    "            F_imputed[i] = np.reshape(f,(f.shape[-1], 1))\n",
    "        \n",
    "        \n",
    "    \n",
    "    if imputeArgs[\"impute\"] == \"structure\":\n",
    "    \n",
    "        for i, (g,f) in enumerate(zip(G_imputed, F_imputed)):\n",
    "                        \n",
    "            f = np.squeeze(f)            \n",
    "            num_nodes = len(f[f > 0])\n",
    "            impute_num = int(imputeArgs[\"impute_frac\"] * num_nodes)\n",
    "            impute_n_ind = random.sample(range(num_nodes), impute_num)  ## impute nodes\n",
    "            \n",
    "            ## remove edges of imputed nodes\n",
    "            for impute_n in impute_n_ind:\n",
    "                \n",
    "                ## remove rows\n",
    "                g[impute_n,:impute_n] = 0\n",
    "                g[impute_n,impute_n+1:] = 0\n",
    "                \n",
    "                ## remove columns\n",
    "                g[:impute_n,impute_n] = 0\n",
    "                g[impute_n+1:,impute_n] = 0\n",
    "\n",
    "            G_imputed[i] = g\n",
    "        \n",
    "    return G_imputed, F_imputed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# beta-VAE Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "## libs\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "## Keras\n",
    "from keras.layers import Lambda, Input, Dense, Conv2D, Conv2DTranspose, Flatten, Reshape, concatenate\n",
    "from keras.models import Model\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.losses import mse, binary_crossentropy\n",
    "from keras.utils import plot_model\n",
    "from keras import backend as K\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "class VAE():\n",
    "\n",
    "    # reparameterization trick\n",
    "    # instead of sampling from Q(z|X), sample eps = N(0,I)\n",
    "    # then z = z_mean + sqrt(var)*eps\n",
    "\n",
    "    def sampling(self, args):\n",
    "        \"\"\"Reparameterization trick by sampling fr an isotropic unit Gaussian.\n",
    "        # Arguments\n",
    "            args (tensor): mean and log of variance of Q(z|X)\n",
    "        # Returns\n",
    "            z (tensor): sampled latent vector\n",
    "        \"\"\"\n",
    "\n",
    "        z_mean, z_log_var = args\n",
    "        batch = K.shape(z_mean)[0]\n",
    "        dim = K.int_shape(z_mean)[1]\n",
    "        # by default, random_normal has mean=0 and std=1.0\n",
    "        epsilon = K.random_normal(shape=(batch, dim))\n",
    "        return z_mean + K.exp(0.5 * z_log_var) * epsilon\n",
    "\n",
    "\n",
    "\n",
    "    def __init__(self, modelArgs, trainArgs, g_train, g_train_imputed, g_test, g_test_imputed, f_train, f_train_imputed, f_test, f_test_imputed):\n",
    "\n",
    "        ## MODEL ______________________________________________________________\n",
    "        \n",
    "        \n",
    "        ## 1.1) build attr encoder model\n",
    "        attr_input = Input(shape= (modelArgs[\"input_shape\"][0],), name='attr_input')\n",
    "        x1 = Dense(12, activation='relu')(attr_input)\n",
    "        x1 = Dense(8, activation='relu')(x1)        \n",
    "        \n",
    "        ## 1.2) build topol encoder model\n",
    "        topol_input = Input(shape= (modelArgs[\"input_shape\"][1],), name='topol_input')\n",
    "        x2 = Dense(64, activation='relu')(topol_input)\n",
    "        x2 = Dense(32, activation='relu')(x2)\n",
    "        x2 = Dense(8, activation='relu')(x2)\n",
    "        \n",
    "        \n",
    "        encoder_combined = concatenate([x1, x2])\n",
    "        z_mean = Dense(modelArgs[\"latent_dim\"], name='z_mean')(encoder_combined)\n",
    "        z_log_var = Dense(modelArgs[\"latent_dim\"], name='z_log_var')(encoder_combined)\n",
    "        \n",
    "        # use reparameterization trick to push the sampling out as input\n",
    "        # note that \"output_shape\" isn't necessary with the TensorFlow backend\n",
    "        z = Lambda(self.sampling, output_shape=(modelArgs[\"latent_dim\"],), name='z')([z_mean, z_log_var])\n",
    "        \n",
    "        latent_inputs = Input(shape=(modelArgs[\"latent_dim\"],), name='z_sampling')\n",
    "        \n",
    "        ## 2.1) build attr decoder model\n",
    "        y1 = Dense(8, activation='relu')(latent_inputs)\n",
    "        y1 = Dense(32, activation='relu')(y1)\n",
    "        y1 = Dense(64, activation='relu')(y1)\n",
    "        attr_output = Dense(modelArgs[\"output_shape\"][0], activation='sigmoid')(y1)\n",
    "        \n",
    "        ## 2.2) build topol decoder model\n",
    "        y2 = Dense(8, activation='relu')(latent_inputs)\n",
    "        y2 = Dense(32, activation='relu')(y2)\n",
    "        y2 = Dense(64, activation='relu')(y2)\n",
    "        topol_output = Dense(modelArgs[\"output_shape\"][1], activation='sigmoid')(y2)\n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "        ## INSTANTIATE___________________________________\n",
    "\n",
    "        ## 1) instantiate topol encoder model\n",
    "        attr_topol_encoder = Model([attr_input, topol_input], [z_mean, z_log_var, z], name='attr_topol_encoder')\n",
    "        attr_topol_encoder.summary()\n",
    "\n",
    "        ## 2) instantiate topology decoder model\n",
    "        attr_topol_decoder = Model(latent_inputs, [attr_output, topol_output], name='attr_topol_decoder')\n",
    "        attr_topol_decoder.summary()\n",
    "        \n",
    "        ## 3) instantiate VAE model\n",
    "        attr_topol_outputs = attr_topol_decoder(attr_topol_encoder([attr_input, topol_input])[2])\n",
    "        vae = Model([attr_input, topol_input], attr_topol_outputs, name='vae')\n",
    "\n",
    "    \n",
    "\n",
    "        ## LOSS FUNCTIONS ______________________________________\n",
    "        \n",
    "        def loss_func(y_true, y_pred):\n",
    "            \n",
    "            y_true_attr = y_true[0]\n",
    "            y_pred_attr = y_pred[0]\n",
    "            \n",
    "            y_true_topol = y_true[1]\n",
    "            y_pred_topol = y_pred[1]\n",
    "            \n",
    "\n",
    "            ## ATTR RECONSTRUCTION LOSS_______________________            \n",
    "            ## mean squared error\n",
    "            attr_reconstruction_loss = mse(K.flatten(y_true_attr), K.flatten(y_pred_attr))\n",
    "            attr_reconstruction_loss *= modelArgs[\"input_shape\"][0]\n",
    "            \n",
    "            ## TOPOL RECONSTRUCTION LOSS_______________________\n",
    "            ## binary cross-entropy\n",
    "            topol_reconstruction_loss = binary_crossentropy(K.flatten(y_true_topol), K.flatten(y_pred_topol))\n",
    "            topol_reconstruction_loss *= modelArgs[\"input_shape\"][0]\n",
    "                     \n",
    "            ## KL LOSS _____________________________________________\n",
    "            kl_loss = 1 + z_log_var - K.square(z_mean) - K.exp(z_log_var)\n",
    "            kl_loss = K.sum(kl_loss, axis=-1)\n",
    "            kl_loss *= -0.5\n",
    "\n",
    "            ## COMPLETE LOSS __________________________________________________\n",
    "\n",
    "            loss = K.mean(trainArgs[\"loss_weights\"][0] * attr_reconstruction_loss + trainArgs[\"loss_weights\"][1] * topol_reconstruction_loss + trainArgs[\"loss_weights\"][2] * kl_loss)\n",
    "            \n",
    "            return loss\n",
    "        \n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "        ## MODEL COMPILE______________________________________________\n",
    "        \n",
    "        #vae.compile(optimizer='adam', loss={\"attr_decoder\": attr_loss_func, \"topol_decoder\": topol_loss_func}, loss_weights=trainArgs[\"loss_weights\"])\n",
    "        vae.compile(optimizer='adam', loss= loss_func) #, loss_weights=trainArgs[\"loss_weights\"])\n",
    "        vae.summary()\n",
    "        \n",
    "        \n",
    "\n",
    "        ## TRAIN______________________________________________\n",
    "\n",
    "        # load the autoencoder weights\n",
    "\n",
    "        if trainArgs[\"weights\"] == \"load\":\n",
    "\n",
    "            vae.load_weights(\"models/weights/vae_mlp_mnist_latent_dim_\" + str(modelArgs[\"latent_dim\"]) + \".h5\")\n",
    "\n",
    "        # train the autoencoder\n",
    "\n",
    "        elif trainArgs[\"weights\"] == \"train\":\n",
    "\n",
    "            # Set callback functions to early stop training and save the best model so far\n",
    "            callbacks = [EarlyStopping(monitor='val_loss', patience=trainArgs[\"early_stop\"]), ModelCheckpoint(filepath=\"models/weights/vae_mlp_mnist_latent_dim_\" + str(modelArgs[\"latent_dim\"]) + \".h5\",save_best_only=True)]\n",
    "            \n",
    "            #vae.fit([f_train, g_train], {\"attr_decoder\": f_train, \"topol_decoder\": g_train}, epochs=trainArgs[\"epochs\"],batch_size=trainArgs[\"batch_size\"], callbacks=callbacks,validation_data=([f_test, g_test], {\"attr_decoder\": f_test, \"topol_decoder\": g_test}))\n",
    "            vae.fit([f_train_imputed, g_train_imputed], [f_train, g_train], epochs=trainArgs[\"epochs\"],batch_size=trainArgs[\"batch_size\"], callbacks=callbacks,validation_data=([f_test_imputed, g_test_imputed], [f_test, g_test]))\n",
    "            vae.save_weights(\"models/weights/vae_mlp_mnist_latent_dim_\" + str(modelArgs[\"latent_dim\"]) + \".h5\")\n",
    "\n",
    "            self.model = (attr_topol_encoder, attr_topol_decoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Split and Impute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainArgs = {\"loss_weights\": [10,1,5], \"weights\": \"train\", \"early_stop\": 1, \"batch_size\": 128, \"epochs\": 50, \"data_split\": 0.2}\n",
    "modelArgs = {\"nn_architecture\": \"mlp\", \"latent_dim\": 2, \"filters\": 16, \"kernel_size\": 3, \"input_shape\": input_shape, \"output_shape\": output_shape, \"param_loss\": False,}\n",
    "imputeArgs = {\"impute\": \"features\", \"impute_frac\": 1.0, \"impute_value\": 0.0, }\n",
    "\n",
    "from support.keras_dgl.utils import *\n",
    "from support.keras_dgl.layers import MultiGraphCNN\n",
    "\n",
    "## Train and Validation Split _______________________________________________\n",
    "g_train, g_test, f_train, f_test = train_test_split(G, F, test_size=trainArgs[\"data_split\"], random_state=1, shuffle=True)\n",
    "\n",
    "## impute the data\n",
    "g_train_imputed, f_train_imputed = imput_data(imputeArgs, modelArgs, dataArgs, g_train, f_train)\n",
    "g_test_imputed, f_test_imputed = imput_data(imputeArgs, modelArgs, dataArgs, g_test, f_test)\n",
    "\n",
    "## squeeze attributes\n",
    "f_train_imputed = np.squeeze(f_train_imputed)\n",
    "f_test_imputed = np.squeeze(f_test_imputed)\n",
    "\n",
    "data = [f_test_imputed, g_test_imputed]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "vae = VAE(modelArgs, trainArgs, g_train, g_train_imputed, g_test, g_test_imputed, f_train, np.squeeze(f_train_imputed), f_test, np.squeeze(f_test_imputed))\n",
    "\n",
    "models = vae.model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Single Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "analyzeArgs = {\"z\": [0,1], \"activations\": [20,-2], \"normalize_feature\": False}\n",
    "generate_single_features(analyzeArgs, modelArgs, dataArgs, models, orig_cmap)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Graph Manifold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# range, normal, z\n",
    "analyzeArgs = {\"z\": [0,1], \"sample\": \"range\", \"act_range\": [-4, 4], \"act_scale\": 3, \"size_of_manifold\": 7, \"save_plots\": False, \"normalize_feature\": False}\n",
    "generate_manifold_features(analyzeArgs, modelArgs, dataArgs, models, data, orig_cmap, batch_size=trainArgs[\"batch_size\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Impaint Graphs (Extreme Cases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def impaint(analyzeArgs, imputeArgs, modelArgs, models, batch_size=128):\n",
    "\n",
    "    print(\"impainting the\", imputeArgs[\"impute\"])\n",
    "    \n",
    "    ## unpack models__________________________\n",
    "    \n",
    "    encoder, decoder = models  # trained models\n",
    "    \n",
    "    ## generate feature data___________________________\n",
    "    f = np.reshape([analyzeArgs[\"f\"]] * analyzeArgs[\"n\"], (analyzeArgs[\"n\"], 1))\n",
    "    \n",
    "    ## pad features with zeroes\n",
    "    f = pad_attr(f, dataArgs)\n",
    "\n",
    "    \n",
    "    ## generate graph data___________________________\n",
    "    G = np.zeros((1, *calculate_A_shape(dataArgs[\"n_max\"], dataArgs[\"diag_offset\"])))\n",
    "    F = np.zeros((1, dataArgs[\"n_max\"], dataArgs[\"n_features\"]))\n",
    "\n",
    "    g = get_graph(n = analyzeArgs[\"n\"], p = analyzeArgs[\"p\"], draw = False)\n",
    "    \n",
    "    g, a = sort_adj(g)\n",
    "    a = pad_matrix(a, dataArgs[\"n_max\"], dataArgs[\"diag_value\"])  # pad adjacency matrix to allow less nodes than n_max and fill diagonal\n",
    "    a = reshape_A(a, diag_offset = dataArgs[\"diag_offset\"])\n",
    "    \n",
    "    G[0] = a\n",
    "    F[0] = f\n",
    "    \n",
    "    \n",
    "    ## impute data_______________________\n",
    "    \n",
    "    a_imputed, f_imputed = imput_data(imputeArgs, modelArgs, dataArgs, G, F)\n",
    "    f_imputed = np.reshape(f_imputed, (1,-1))\n",
    "    print(a_imputed.shape, f_imputed.shape)\n",
    "    \n",
    "    \n",
    "    ## ENCODER_________________________________\n",
    "    z_mean, _, _ = encoder.predict([f_imputed, a_imputed], batch_size = batch_size)\n",
    "\n",
    "    \n",
    "    ## DECODER_________________________________    \n",
    "    \n",
    "    [f_decoded, a_decoded] = decoder.predict(z_mean, batch_size = batch_size)\n",
    "    f_decoded = np.reshape(f_decoded, (-1, 1))\n",
    "\n",
    "    \n",
    "    ## GRAPH RECONSTRUCTION______________________\n",
    "    \n",
    "    ## reconstruct graph from output\n",
    "    reconstructed_a = reconstruct_adjacency(a_decoded, dataArgs[\"clip\"], dataArgs[\"diag_offset\"])\n",
    "    reconstructed_a, n_nodes = unpad_matrix(reconstructed_a, dataArgs[\"diag_value\"], 0.2, dataArgs[\"fix_n\"])\n",
    "    reconstructed_g = nx.from_numpy_matrix(reconstructed_a)\n",
    "    \n",
    "    ## reconstruct attributes\n",
    "    reconstructed_f = unpad_attr(f_decoded, n_nodes, analyzeArgs, dataArgs)\n",
    "    \n",
    "    ## create imputed graph\n",
    "    \n",
    "    ## reconstruct upper triangular adjacency matrix\n",
    "    a_imputed = reconstruct_adjacency(a_imputed[0], dataArgs[\"clip\"], dataArgs[\"diag_offset\"])\n",
    "    a_imputed, n_nodes = unpad_matrix(a_imputed, dataArgs[\"diag_value\"], 0.5, dataArgs[\"fix_n\"])\n",
    "    \n",
    "    g_imputed = nx.from_numpy_matrix(a_imputed)\n",
    "    \n",
    "    f_imputed = np.reshape(f_imputed, (f_imputed.shape[1]))\n",
    "    f_imputed = f_imputed[:analyzeArgs[\"n\"]]\n",
    "    \n",
    "    print(\"original attributes:\", f_imputed)\n",
    "    \n",
    "    \n",
    "    ## GRAPH DRAWING_____________________________\n",
    "    \n",
    "    ## 1) draw imputed graph\n",
    "    if reconstructed_f.shape[0] > 0:\n",
    "        fixed_cmap = shiftedColorMap(orig_cmap, start=min(f_imputed), midpoint=0.5, stop=max(f_imputed),name='fixed')\n",
    "    else:\n",
    "        fixed_cmap = shiftedColorMap(orig_cmap, start=0.5, midpoint=0.5, stop=0.5, name='fixed')    \n",
    "    nx.draw(g_imputed, node_color=f_imputed, font_color='white', cmap = fixed_cmap)\n",
    "    plt.show()\n",
    "    \n",
    "    ## 2) draw reconstructed graph\n",
    "    if reconstructed_f.shape[0] > 0:\n",
    "        fixed_cmap = shiftedColorMap(orig_cmap, start=min(reconstructed_f), midpoint=0.5, stop=max(reconstructed_f),name='fixed')\n",
    "    else:\n",
    "        fixed_cmap = shiftedColorMap(orig_cmap, start=0.5, midpoint=0.5, stop=0.5, name='fixed') \n",
    "    print(\"reconstructed attributes::\", reconstructed_f)\n",
    "    nx.draw(reconstructed_g, node_color=reconstructed_f, font_color='white', cmap = fixed_cmap)\n",
    "    plt.show()\n",
    "\n",
    "        \n",
    "  ## PLOT RESULTS ________________________________________\n",
    "\n",
    "imputeArgs[\"impute_frac\"] = 1.0\n",
    "analyzeArgs = {\"n\": 12, \"p\": 1.0, \"f\": 0.4, \"normalize_feature\": False}\n",
    "impaint(analyzeArgs, imputeArgs, modelArgs, models, batch_size=trainArgs[\"batch_size\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
