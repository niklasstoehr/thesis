{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attribute and Topology Dependency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Real-World Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_complete = nx.read_edgelist(\"/Users/niklasstoehr/Programming/thesis/4_real_attr/data/mag/edges_py.csv\",  nodetype=int, delimiter = \",\")\n",
    "a_complete = nx.adjacency_matrix(g_complete)\n",
    "\n",
    "df_nodes  = pd.read_csv(\"/Users/niklasstoehr/Programming/thesis/4_real_attr/data/mag/nodes_py.csv\", header = None)\n",
    "nodes = df_nodes.values\n",
    "nodes = [l.tolist() for l in list(nodes)]\n",
    "nodes_num = [i[0] for i in nodes]\n",
    "\n",
    "g_complete.add_nodes_from(nodes_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_attr_dict = dict()\n",
    "\n",
    "for num, name, c in nodes:\n",
    "    node_attr_dict[num] = c\n",
    "    \n",
    "print(len(node_attr_dict.keys()))\n",
    "    \n",
    "nx.set_node_attributes(g_complete, node_attr_dict, \"citations\")\n",
    "nx.get_node_attributes(g_complete,'citations')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "\n",
    "class Base_Samplers():\n",
    "\n",
    "    def __init__(self, g_complete, a_complete):\n",
    "        self.g_complete = g_complete\n",
    "        self.n_complete = len(g_complete)\n",
    "        self.a_complete = a_complete\n",
    "\n",
    "    def biased_random_walk(self, n, p, q):\n",
    "\n",
    "        g = nx.Graph()\n",
    "        node_1 = np.random.choice(self.g_complete.nodes())\n",
    "        g.add_node(node_1)\n",
    "\n",
    "        neigh_1 = [n for n in self.g_complete.neighbors(node_1)]\n",
    "        node_2 = np.random.choice(neigh_1)\n",
    "        g.add_node(node_2)\n",
    "        g.add_edge(node_1, node_2)\n",
    "\n",
    "        timeout = time.time() + 0.2\n",
    "\n",
    "        while len(g) < n and time.time() < timeout:\n",
    "\n",
    "            neigh_2 = [n for n in self.g_complete.neighbors(node_2)]\n",
    "            neigh_2_prob = np.zeros((len(neigh_2)))\n",
    "\n",
    "            if len(neigh_2) <= 1:  ## break the process if only neighbor is origin\n",
    "                break\n",
    "\n",
    "            for i, neigh in enumerate(neigh_2):\n",
    "\n",
    "                if neigh == node_1:\n",
    "                    neigh_2_prob[i] = 1 / p  ## go back prob -> step back\n",
    "\n",
    "                elif self.g_complete.has_edge(node_1, neigh):\n",
    "                    neigh_2_prob[i] = 1  ## common neighbor prob -> community exploration\n",
    "\n",
    "                elif self.g_complete.has_edge(node_1, neigh) == False:\n",
    "                    neigh_2_prob[i] = 1 / q  ## no common neighbor prob -> free exploration\n",
    "\n",
    "            neigh_2_prob = neigh_2_prob / sum(neigh_2_prob)  ## normalize probability\n",
    "            node_3 = np.random.choice(neigh_2, p=neigh_2_prob)\n",
    "\n",
    "            g.add_node(node_3)\n",
    "            g.add_edge(node_2, node_3)\n",
    "\n",
    "            node_1 = node_2  ## update node 1 and node 2 for next iteration\n",
    "            node_2 = node_3\n",
    "\n",
    "        return g\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "sampleArgs = {\"sample\": \"biased_random_walk\", \"jump_bias\": \"random_walk_induced_graph_sampling\", \"n\": 1000, \"p\": 20.0, \"q\": 100.0, \"source_starts\": 2, \"source_returns\": 4, \"depth\": 2}\n",
    "\n",
    "##exact_n: forestfire, random_walk_induced_graph_sampling, random_walk_sampling_with_fly_back, adjacency, select\n",
    "##approx_n: snowball, bfs, walk, jump\n",
    "\n",
    "def get_graph(sampleArgs,g_complete,a_complete):\n",
    "    \n",
    "    if sampleArgs[\"sample\"] == \"biased_random_walk\":\n",
    "        sampler = Base_Samplers(g_complete,a_complete)\n",
    "        #sampler = Base_Samplers(g_complete,a_complete)\n",
    "        g = sampler.biased_random_walk(sampleArgs[\"n\"], sampleArgs[\"p\"], sampleArgs[\"q\"])\n",
    "    \n",
    "    #nx.set_node_attributes(g, node_attr_dict, \"citations\")\n",
    "    #f = list(nx.get_node_attributes(g,'citations').values())\n",
    "    return g\n",
    "\n",
    "start_time = time.time()\n",
    "g = get_graph(sampleArgs, g_complete, a_complete)\n",
    "\n",
    "print(\"-- n_max should be >=\", len(g), \"--\")\n",
    "print(\"-- function get_graph takes %s secs --\" % round((time.time() - start_time),  5))\n",
    "\n",
    "#if len(g) <= 200:\n",
    "#    nx.draw(g, node_color = color_map, with_labels = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "## libs \n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "## Keras\n",
    "from keras.layers import Lambda, Input, Dense, Conv2D, Conv2DTranspose, Flatten, Reshape\n",
    "from keras.models import Model\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.losses import mse, binary_crossentropy\n",
    "from keras.utils import plot_model\n",
    "from keras import backend as K\n",
    "\n",
    "## Basic\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import argparse\n",
    "import os\n",
    "import random\n",
    "import itertools\n",
    "import time\n",
    "import math\n",
    "import sys\n",
    "\n",
    "## Computation\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mutual_info_score\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "import scipy\n",
    "from scipy.stats.stats import pearsonr \n",
    "import scipy.stats as stats\n",
    "from scipy.stats import wasserstein_distance\n",
    "\n",
    "## Data Wrangling\n",
    "import pandas as pd\n",
    "\n",
    "## Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "## Network Processing\n",
    "import networkx as nx\n",
    "from networkx.generators import random_graphs\n",
    "\n",
    "## node colour\n",
    "orig_cmap = plt.cm.PuBu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Supporting Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "## supporting functions\n",
    "from support.preprocessing import sort_adj, reshape_A, calculate_A_shape, reconstruct_adjacency, pad_matrix, unpad_matrix, pad_attr, unpad_attr, prepare_in_out\n",
    "from support.metrics import compute_mig, compute_mi\n",
    "from support.graph_generating import generate_single_features, generate_manifold_features\n",
    "from support.latent_space import vis2D, visDistr\n",
    "from support.comparing import compare_manifold_adjacency, compare_topol_manifold\n",
    "from support.plotting import shiftedColorMap\n",
    "\n",
    "## graph sampling\n",
    "from sampling import ForestFire, Metropolis_Hastings, Random_Walk, Snowball, Ties, Base_Samplers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def get_graph(n,p,draw): \n",
    "\n",
    "    g = random_graphs.erdos_renyi_graph(n, p, seed=None, directed=False)\n",
    "\n",
    "    if draw:\n",
    "        f = np.random.rand(n)\n",
    "        orig_cmap = plt.cm.PuBu\n",
    "        fixed_cmap = shiftedColorMap(orig_cmap, start=min(f), midpoint=0.5, stop=max(f), name='fixed')\n",
    "        nx.draw(g, node_color=f, font_color='white', cmap = fixed_cmap)\n",
    "        plt.show()\n",
    "    \n",
    "    return g\n",
    "\n",
    "g = get_graph(n = 10, p = 0.4, draw = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def generate_features(dataArgs, g, n, p):\n",
    "    \n",
    "        if dataArgs[\"feature_dependence\"] == \"random\":\n",
    "            f = np.random.rand(n, dataArgs[\"n_features\"])                   ## float\n",
    "            #F[i] = np.random.randint(2, size=(dataArgs[\"n_max\"],dataArgs[\"n_features\"]))   ## int\n",
    "            \n",
    "        if dataArgs[\"feature_dependence\"] == \"norm_degree\":\n",
    "            if dataArgs[\"n_features\"] == 1:\n",
    "                \n",
    "                f = np.asarray([int(x[1]) for x in sorted(g.degree())])  \n",
    "                f = (f) / (max(f)+1)\n",
    "                f = np.reshape(f, (f.shape[-1],1))\n",
    "                \n",
    "        \n",
    "        if dataArgs[\"feature_dependence\"] == \"degree\":\n",
    "            if dataArgs[\"n_features\"] == 1:\n",
    "                \n",
    "                f = np.asarray([int(x[1]) for x in sorted(g.degree())])  \n",
    "                f = (f+1) / (dataArgs[\"n_max\"]+1)\n",
    "                f = np.reshape(f, (f.shape[-1],1))\n",
    "    \n",
    "                \n",
    "        if dataArgs[\"feature_dependence\"] == \"uniform\":\n",
    "            if dataArgs[\"n_features\"] == 1:\n",
    "                \n",
    "                f = np.ones((len(g)))  \n",
    "                f = f * np.random.rand()\n",
    "                f = np.reshape(f, (f.shape[-1],1))\n",
    "                \n",
    "                \n",
    "        if dataArgs[\"feature_dependence\"] == \"p\":  \n",
    "            if dataArgs[\"n_features\"] == 1:\n",
    "                f = np.ones((n , 1)) * p\n",
    "                \n",
    "        return f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_in_out(diag_offset, A_shape, F_shape):\n",
    "\n",
    "    if diag_offset == 0:  # matrix input\n",
    "        return (F_shape[1], A_shape[0]) , (F_shape[1], A_shape[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def generate_data(dataArgs): \n",
    "    \n",
    "    \n",
    "    ## Data ________________________________\n",
    "\n",
    "    G = np.zeros((dataArgs[\"n_graphs\"], *calculate_A_shape(dataArgs[\"n_max\"], dataArgs[\"diag_offset\"])))\n",
    "    F = np.zeros((dataArgs[\"n_graphs\"], dataArgs[\"n_max\"], dataArgs[\"n_features\"]))\n",
    "    print(\"feature_dependence:\",dataArgs[\"feature_dependence\"] )\n",
    "    \n",
    "    \n",
    "    ## Generate Graph Data_______________________________\n",
    "\n",
    "    for i in tqdm(range(0,dataArgs[\"n_graphs\"])):\n",
    "        \n",
    "        ## Generate Graph Type ______________________________________________\n",
    "\n",
    "        if dataArgs[\"fix_n\"] == True:\n",
    "            n = dataArgs[\"n_max\"] # generate fixed number of nodes n_max\n",
    "        else:\n",
    "            n = random.randint(1, dataArgs[\"n_max\"]) # generate number of nodes n between 1 and n_max and\n",
    "\n",
    "            \n",
    "        p = np.random.rand(1)  # float in range 0 - 1 \n",
    "        g = get_graph(n, p, draw = False)\n",
    "                     \n",
    "        #nx.draw(g, cmap=plt.get_cmap('PuBu'), node_color=np.squeeze(f), font_color='white')\n",
    "        #plt.show()\n",
    "        \n",
    "        g, a = sort_adj(g)\n",
    "        a = pad_matrix(a, dataArgs[\"n_max\"], dataArgs[\"diag_value\"])  # pad adjacency matrix to allow less nodes than n_max and fill diagonal        \n",
    "        a_transformed = reshape_A(a, diag_offset = dataArgs[\"diag_offset\"])\n",
    "        \n",
    "        \n",
    "        ## Generate / Load Node Features ______________________________________________\n",
    "        f = generate_features(dataArgs, g, n, p)\n",
    "        \n",
    "        ## pad features with zeroes\n",
    "        f = pad_attr(f, dataArgs)\n",
    "\n",
    "        \n",
    "        ## Build Data Arrays___________________________________________________\n",
    "\n",
    "        F[i] = f\n",
    "        G[i] = a_transformed\n",
    "\n",
    "\n",
    "    ## Input and Output Size ___________________________________________________________\n",
    "\n",
    "    input_shape, output_shape = prepare_in_out(dataArgs[\"diag_offset\"], calculate_A_shape(dataArgs[\"n_max\"], dataArgs[\"diag_offset\"]), F.shape)\n",
    "    print(\"input_shape:\", input_shape, \", output_shape:\", output_shape)\n",
    "    \n",
    "    ## scale features in F for smoother training\n",
    "    #scaler = MinMaxScaler()\n",
    "    #scaler.fit(F)\n",
    "    #F = scaler.transform(F)\n",
    "    \n",
    "    return G, F, input_shape,output_shape\n",
    "    \n",
    "dataArgs = {\"n_graphs\": 1000, \"n_max\": 30, \"feature_dependence\": \"uniform\", \"fix_n\": False, \"diag_offset\": 0, \"diag_value\": 1, \"clip\": True, \"n_features\": 1}  #\"diag_offset\" - 1 == full adjacency\n",
    "G, F, input_shape, output_shape = generate_data(dataArgs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# beta-VAE Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "## libs\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "## Keras\n",
    "from keras.layers import Lambda, Input, Dense, Conv2D, Conv2DTranspose, Flatten, Reshape, concatenate\n",
    "from keras.models import Model\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.losses import mse, binary_crossentropy\n",
    "from keras.utils import plot_model\n",
    "from keras import backend as K\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "class VAE():\n",
    "\n",
    "    # reparameterization trick\n",
    "    # instead of sampling from Q(z|X), sample eps = N(0,I)\n",
    "    # then z = z_mean + sqrt(var)*eps\n",
    "\n",
    "    def sampling(self, args):\n",
    "        \"\"\"Reparameterization trick by sampling fr an isotropic unit Gaussian.\n",
    "        # Arguments\n",
    "            args (tensor): mean and log of variance of Q(z|X)\n",
    "        # Returns\n",
    "            z (tensor): sampled latent vector\n",
    "        \"\"\"\n",
    "\n",
    "        z_mean, z_log_var = args\n",
    "        batch = K.shape(z_mean)[0]\n",
    "        dim = K.int_shape(z_mean)[1]\n",
    "        # by default, random_normal has mean=0 and std=1.0\n",
    "        epsilon = K.random_normal(shape=(batch, dim))\n",
    "        return z_mean + K.exp(0.5 * z_log_var) * epsilon\n",
    "\n",
    "\n",
    "\n",
    "    def __init__(self, modelArgs, trainArgs, fg_train, f_train, g_train, fg_test, f_test, g_test):\n",
    "\n",
    "        ## MODEL ______________________________________________________________\n",
    "        \n",
    "        \n",
    "        ## 1.1) build attr encoder model\n",
    "        attr_input = Input(shape= (modelArgs[\"input_shape\"][0],), name='attr_input')\n",
    "        x1 = Dense(12, activation='relu')(attr_input)\n",
    "        x1 = Dense(8, activation='relu')(x1)        \n",
    "        \n",
    "        ## 1.2) build topol encoder model\n",
    "        topol_input = Input(shape= (modelArgs[\"input_shape\"][1],), name='topol_input')\n",
    "        x2 = Dense(64, activation='relu')(topol_input)\n",
    "        x2 = Dense(32, activation='relu')(x2)\n",
    "        x2 = Dense(8, activation='relu')(x2)\n",
    "        \n",
    "        \n",
    "        encoder_combined = concatenate([x1, x2])\n",
    "        z_mean = Dense(modelArgs[\"latent_dim\"], name='z_mean')(encoder_combined)\n",
    "        z_log_var = Dense(modelArgs[\"latent_dim\"], name='z_log_var')(encoder_combined)\n",
    "        \n",
    "        # use reparameterization trick to push the sampling out as input\n",
    "        # note that \"output_shape\" isn't necessary with the TensorFlow backend\n",
    "        z = Lambda(self.sampling, output_shape=(modelArgs[\"latent_dim\"],), name='z')([z_mean, z_log_var])\n",
    "        \n",
    "        latent_inputs = Input(shape=(modelArgs[\"latent_dim\"],), name='z_sampling')\n",
    "        \n",
    "        ## 2.1) build attr decoder model\n",
    "        y1 = Dense(8, activation='relu')(latent_inputs)\n",
    "        y1 = Dense(32, activation='relu')(y1)\n",
    "        y1 = Dense(64, activation='relu')(y1)\n",
    "        attr_output = Dense(modelArgs[\"output_shape\"][0], activation='sigmoid')(y1)\n",
    "        \n",
    "        ## 2.2) build topol decoder model\n",
    "        y2 = Dense(8, activation='relu')(latent_inputs)\n",
    "        y2 = Dense(32, activation='relu')(y2)\n",
    "        y2 = Dense(64, activation='relu')(y2)\n",
    "        topol_output = Dense(modelArgs[\"output_shape\"][1], activation='sigmoid')(y2)\n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "        ## INSTANTIATE___________________________________\n",
    "\n",
    "        ## 1) instantiate topol encoder model\n",
    "        attr_topol_encoder = Model([attr_input, topol_input], [z_mean, z_log_var, z], name='attr_topol_encoder')\n",
    "        attr_topol_encoder.summary()\n",
    "\n",
    "        ## 2) instantiate topology decoder model\n",
    "        attr_topol_decoder = Model(latent_inputs, [attr_output, topol_output], name='attr_topol_decoder')\n",
    "        attr_topol_decoder.summary()\n",
    "        \n",
    "        ## 3) instantiate VAE model\n",
    "        attr_topol_outputs = attr_topol_decoder(attr_topol_encoder([attr_input, topol_input])[2])\n",
    "        vae = Model([attr_input, topol_input], attr_topol_outputs, name='vae')\n",
    "\n",
    "    \n",
    "\n",
    "        ## LOSS FUNCTIONS ______________________________________\n",
    "        \n",
    "        def loss_func(y_true, y_pred):\n",
    "            \n",
    "            y_true_attr = y_true[0]\n",
    "            y_pred_attr = y_pred[0]\n",
    "            \n",
    "            y_true_topol = y_true[1]\n",
    "            y_pred_topol = y_pred[1]\n",
    "            \n",
    "\n",
    "            ## ATTR RECONSTRUCTION LOSS_______________________            \n",
    "            ## mean squared error\n",
    "            attr_reconstruction_loss = mse(K.flatten(y_true_attr), K.flatten(y_pred_attr))\n",
    "            attr_reconstruction_loss *= modelArgs[\"input_shape\"][0]\n",
    "            \n",
    "            ## TOPOL RECONSTRUCTION LOSS_______________________\n",
    "            ## binary cross-entropy\n",
    "            topol_reconstruction_loss = binary_crossentropy(K.flatten(y_true_topol), K.flatten(y_pred_topol))\n",
    "            topol_reconstruction_loss *= modelArgs[\"input_shape\"][0]\n",
    "                     \n",
    "            ## KL LOSS _____________________________________________\n",
    "            kl_loss = 1 + z_log_var - K.square(z_mean) - K.exp(z_log_var)\n",
    "            kl_loss = K.sum(kl_loss, axis=-1)\n",
    "            kl_loss *= -0.5\n",
    "\n",
    "            ## COMPLETE LOSS __________________________________________________\n",
    "\n",
    "            loss = K.mean(trainArgs[\"loss_weights\"][0] * attr_reconstruction_loss + trainArgs[\"loss_weights\"][1] * topol_reconstruction_loss + trainArgs[\"loss_weights\"][2] * kl_loss)\n",
    "            \n",
    "            return loss\n",
    "        \n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "        ## MODEL COMPILE______________________________________________\n",
    "        \n",
    "        #vae.compile(optimizer='adam', loss={\"attr_decoder\": attr_loss_func, \"topol_decoder\": topol_loss_func}, loss_weights=trainArgs[\"loss_weights\"])\n",
    "        vae.compile(optimizer='adam', loss= loss_func) #, loss_weights=trainArgs[\"loss_weights\"])\n",
    "        vae.summary()\n",
    "        \n",
    "        \n",
    "\n",
    "        ## TRAIN______________________________________________\n",
    "\n",
    "        # load the autoencoder weights\n",
    "\n",
    "        if trainArgs[\"weights\"] == \"load\":\n",
    "\n",
    "            vae.load_weights(\"models/weights/vae_mlp_mnist_latent_dim_\" + str(modelArgs[\"latent_dim\"]) + \".h5\")\n",
    "\n",
    "        # train the autoencoder\n",
    "\n",
    "        elif trainArgs[\"weights\"] == \"train\":\n",
    "\n",
    "            # Set callback functions to early stop training and save the best model so far\n",
    "            callbacks = [EarlyStopping(monitor='val_loss', patience=trainArgs[\"early_stop\"]), ModelCheckpoint(filepath=\"models/weights/vae_mlp_mnist_latent_dim_\" + str(modelArgs[\"latent_dim\"]) + \".h5\",save_best_only=True)]\n",
    "\n",
    "            #vae.fit([f_train, g_train], {\"attr_decoder\": f_train, \"topol_decoder\": g_train}, epochs=trainArgs[\"epochs\"],batch_size=trainArgs[\"batch_size\"], callbacks=callbacks,validation_data=([f_test, g_test], {\"attr_decoder\": f_test, \"topol_decoder\": g_test}))\n",
    "            vae.fit([f_train, g_train], [f_train, g_train], epochs=trainArgs[\"epochs\"],batch_size=trainArgs[\"batch_size\"], callbacks=callbacks,validation_data=([f_test, g_test], [f_test, g_test]))\n",
    "            vae.save_weights(\"models/weights/vae_mlp_mnist_latent_dim_\" + str(modelArgs[\"latent_dim\"]) + \".h5\")\n",
    "\n",
    "            self.model = (attr_topol_encoder, attr_topol_decoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "trainArgs = {\"loss_weights\": [100,1,5], \"weights\": \"train\", \"early_stop\": 2, \"batch_size\": 8, \"epochs\": 50, \"data_split\": 0.2}\n",
    "modelArgs = {\"nn_architecture\": \"mlp\", \"latent_dim\": 2, \"filters\": 16, \"kernel_size\": 3, \"input_shape\": input_shape, \"output_shape\": output_shape, \"param_loss\": False,}\n",
    "\n",
    "from support.keras_dgl.utils import *\n",
    "from support.keras_dgl.layers import MultiGraphCNN\n",
    "\n",
    "## Train and Validation Split _______________________________________________\n",
    "\n",
    "F = np.squeeze(F) ## remove last dimension\n",
    "g_train, g_test, f_train, f_test = train_test_split(G, F, test_size=trainArgs[\"data_split\"], random_state=1, shuffle=True)\n",
    "\n",
    "fg_train = np.concatenate((f_train, g_train), axis=1)\n",
    "fg_test = np.concatenate((f_test, g_test), axis=1)\n",
    "\n",
    "data = (f_test, g_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "vae = VAE(modelArgs, trainArgs, fg_train, f_train, g_train, fg_test, f_test, g_test)\n",
    "models = vae.model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Latent Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzeArgs = {\"z\": [0,1]}\n",
    "visDistr(modelArgs, analyzeArgs, models,data,trainArgs[\"batch_size\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Single"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzeArgs = {\"z\": [0,1], \"activations\": [3,20], \"normalize_feature\": False}\n",
    "generate_single_features(analyzeArgs, modelArgs, dataArgs, models, orig_cmap)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Manifold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## DECODER - Latent Space Interpolation____________________________\n",
    "\n",
    "def generate_manifold_features(analyzeArgs, modelArgs, dataArgs, models, data, orig_cmap, batch_size=128):\n",
    "    print(\"latent dimensions:\", modelArgs[\"latent_dim\"])\n",
    "    (encoder, decoder) = models\n",
    "\n",
    "    (f_test, g_test) = data\n",
    "    z_mean, z_log_var, _ = encoder.predict([f_test, g_test], batch_size=batch_size)\n",
    "\n",
    "    ## Latent Space Dimension is 2 ______________________\n",
    "\n",
    "    if modelArgs[\"latent_dim\"] == 2:\n",
    "\n",
    "        ## 1) create adjacency plots_______________________________________________\n",
    "\n",
    "        # display a 30x30 2D manifold of digits\n",
    "        n = dataArgs[\"n_max\"]  # number of nodes\n",
    "        figure = np.zeros((analyzeArgs[\"size_of_manifold\"] * n, analyzeArgs[\"size_of_manifold\"] * n, 3))\n",
    "\n",
    "        # linearly spaced coordinates corresponding to the 2D plot\n",
    "        # of digit classes in the latent space\n",
    "        if analyzeArgs[\"sample\"] == \"range\":\n",
    "            grid_x = np.linspace(1, 30, analyzeArgs[\"size_of_manifold\"], dtype = int)[::-1]\n",
    "            grid_y = np.linspace(6, 18,analyzeArgs[\"size_of_manifold\"])#[::-1]  ## revert\n",
    "\n",
    "        ## 2) create graph plots_______________________________________________\n",
    "\n",
    "        fig, axs = plt.subplots(analyzeArgs[\"size_of_manifold\"], analyzeArgs[\"size_of_manifold\"], figsize=(8, 8), dpi = 300)\n",
    "        # fig.subplots_adjust(hspace = .5, wspace=.001)\n",
    "\n",
    "        sampleArgs[\"n\"] = dataArgs[\"n_max\"]\n",
    "        g_presample = get_graph(sampleArgs, g_complete, a_complete)\n",
    "        print(\"len(g_presample)\", len(g_presample), \"\\n\")\n",
    "        \n",
    "        sampleArgs[\"p\"] = 100\n",
    "        sampleArgs[\"q\"] = 200\n",
    "        \n",
    "        for i, yi in enumerate(grid_y):\n",
    "            for j, xi in enumerate(grid_x):\n",
    "                \n",
    "                sampleArgs[\"n\"] = xi\n",
    "                sampleArgs[\"p\"] = (i) * (20)  + 10\n",
    "                sampleArgs[\"q\"] = (len(grid_y)-1 - i) * (30) +1 \n",
    "                print(\"\\n\", sampleArgs[\"p\"], sampleArgs[\"q\"])\n",
    "                g = get_graph(sampleArgs, g_presample, a_complete)\n",
    "                \n",
    "\n",
    "                #topology = sum(i for i in nx.degree_centrality(g).values()) / len(nx.degree_centrality(g).keys())\n",
    "\n",
    "                \n",
    "                import time    \n",
    "                start_time = time.clock()\n",
    "                \n",
    "                while (len(g) < sampleArgs[\"n\"] - 4): #or (topology < yi - 5) or (topology > yi + 5):\n",
    "                    #g = get_graph(sampleArgs, g_complete, a_complete)\n",
    "                    \n",
    "                    g = get_graph(sampleArgs, g_complete, a_complete)\n",
    "                    #topology = sum(i for i in nx.degree_centrality(g).values()) / len(nx.degree_centrality(g).keys())\n",
    "                    #topology = nx.average_clustering(g)\n",
    "                    \n",
    "                    if time.clock() > start_time + 2.0:\n",
    "                        break\n",
    "                \n",
    "                g, a = sort_adj(g)\n",
    "                \n",
    "                #reconstructed_f = np.ones((len(g)))\n",
    "                degrees = np.asarray([x[1] for x in g.degree()], dtype = float)\n",
    "                degrees += np.asarray([x for x in list(nx.betweenness_centrality(g).values())])\n",
    "                degrees = (degrees / yi) + np.abs(np.random.normal(0.0, 0.05, len(g)))\n",
    "                reconstructed_f = degrees #* yi\n",
    "                #print(len(g), reconstructed_f)\n",
    "                reconstructed_f = np.clip(reconstructed_f, a_min = 0.0, a_max = 1.0) \n",
    "                print(reconstructed_f)\n",
    "                \n",
    "\n",
    "                a = pad_matrix(a, dataArgs[\"n_max\"], dataArgs[\"diag_value\"])  # pad adjacency matrix to allow less nodes than n_max and fill diagonal\n",
    "                #a_transformed = reshape_A(a, diag_offset = dataArgs[\"diag_offset\"])\n",
    "                \n",
    "                \n",
    "                ## build fixed cmap\n",
    "                fixed_cmap = shiftedColorMap(orig_cmap, start=min(reconstructed_f), midpoint=0.5,stop=max(reconstructed_f), name='fixed')\n",
    "                #fixed_cmap = shiftedColorMap(orig_cmap, start=0.0, midpoint=0.5,stop=1.0, name='fixed')\n",
    "                \n",
    "                ## adjust colour reconstructed_a_padded according to features\n",
    "                feature_a = np.copy(a)\n",
    "                feature_a = np.tile(feature_a[:, :, None], [1, 1, 3])  ## broadcast 1 channel to 3\n",
    "\n",
    "                for node in range(0, len(g)):\n",
    "                    color = fixed_cmap(reconstructed_f[node])[:3]\n",
    "                    feature_a[node, :node + 1] = feature_a[node, :node + 1] * color\n",
    "                    feature_a[:node, node] = feature_a[:node, node] * color\n",
    "                \n",
    "                \n",
    "                ## 1) create adjacency plots_____________________________________\n",
    "\n",
    "                figure[i * n: (i + 1) * n,\n",
    "                j * n: (j + 1) * n] = feature_a\n",
    "\n",
    "                # compute index for the subplot, and set this subplot as current\n",
    "                plt.sca(axs[i, j])\n",
    "                nx.draw(g, node_size=12, node_color=reconstructed_f, width=0.2, font_color='white', cmap=fixed_cmap)\n",
    "                axs[i, j].set_axis_off()\n",
    "\n",
    "        start_range = n // 2\n",
    "        end_range = (analyzeArgs[\"size_of_manifold\"] - 1) * n + start_range + 1\n",
    "        pixel_range = np.arange(start_range, end_range, n)\n",
    "        sample_range_x = np.round(grid_x, 1)\n",
    "        sample_range_y = np.round(grid_y, 1)\n",
    "\n",
    "        # Plot_____________________________\n",
    "\n",
    "        plt.figure(figsize=(10, 10), dpi = 300)\n",
    "        #plt.xticks(pixel_range, sample_range_x)\n",
    "        #plt.yticks(pixel_range, sample_range_y)\n",
    "        #plt.xlabel(\"z_0\", fontweight='bold')\n",
    "        #plt.ylabel(\"z_1\", fontweight='bold')\n",
    "        plt.imshow(figure, cmap='Greys_r')\n",
    "        plt.axis('off')\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# range, normal, z\n",
    "analyzeArgs = {\"z\": [0,1], \"sample\": \"range\", \"act_range\": [-4, 4], \"act_scale\": 1, \"size_of_manifold\": 7, \"save_plots\": False, \"normalize_feature\": False}\n",
    "generate_manifold_features(analyzeArgs, modelArgs, dataArgs, models, data, orig_cmap, batch_size=trainArgs[\"batch_size\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modification (Nullhypothesis Testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def modification_comparison(analyzeArgs, modelArgs, models, data, batch_size=128,model_name=\"vae_graph\"):\n",
    "\n",
    "    ## unpack models and data__________________________\n",
    "    \n",
    "    encoder, decoder = models  # trained models\n",
    "    (F_org, A) = data\n",
    "    \n",
    "    print(F_org.shape, A.shape)\n",
    "\n",
    "    F_mod = np.copy(F_org)\n",
    "    \n",
    "    for i, (f,a) in enumerate(zip(F_mod, A)):\n",
    "\n",
    "        ## Generate Graph Type ______________________________________________\n",
    "            \n",
    "        reconstructed_a_padded = reconstruct_adjacency(a, dataArgs[\"clip\"], dataArgs[\"diag_offset\"])\n",
    "        reconstructed_a, nodes_n = unpad_matrix(reconstructed_a_padded, dataArgs[\"diag_value\"], 0.2, dataArgs[\"fix_n\"])\n",
    "        \n",
    "        f = np.reshape(f, (-1, 1))        \n",
    "        f = unpad_attr(f, nodes_n, analyzeArgs, dataArgs)\n",
    "        \n",
    "        \n",
    "        if analyzeArgs[\"f_variation\"] == \"random\":\n",
    "            for mod in range(0, int(analyzeArgs[\"mod_degree\"] * nodes_n)):\n",
    "                \n",
    "                node = np.random.randint(low = 0, high = nodes_n, size = 2)\n",
    "                rand_f = np.random.rand()\n",
    "                f[node] = rand_f\n",
    "        \n",
    "        \n",
    "        if analyzeArgs[\"f_variation\"] == \"shuffle\":\n",
    "            for mod in range(0, int(analyzeArgs[\"mod_degree\"] * nodes_n)):\n",
    "                swap = np.random.randint(low = 0, high = nodes_n, size = 2)\n",
    "                temp = np.copy(f[swap[0]])\n",
    "                f[swap[0]] = f[swap[1]]\n",
    "                f[swap[1]] = temp\n",
    "\n",
    "\n",
    "        elif analyzeArgs[\"f_variation\"] == \"equal\":\n",
    "    \n",
    "            norm_std = analyzeArgs[\"mod_degree\"]\n",
    "            norm_mean = np.mean(f) \n",
    "            \n",
    "            for i in range(0,len(f)):\n",
    "                \n",
    "                if f[i] > norm_mean:\n",
    "                    f[i] = f[i] - (norm_std * (np.abs(f[i] - norm_mean)))\n",
    "                else:\n",
    "                    f[i] = f[i] + (norm_std * (np.abs(f[i] - norm_mean )))\n",
    "                    \n",
    "                    \n",
    "        elif analyzeArgs[\"f_variation\"] == \"uniform\":\n",
    "    \n",
    "            f = np.ones(f.shape[0]) * analyzeArgs[\"mod_degree\"]\n",
    "            f = np.reshape(f, (f.shape[-1],1))\n",
    "        \n",
    "        \n",
    "        ## pad features with zeroes\n",
    "        f = np.reshape(f,(-1,1))\n",
    "        f = pad_attr(f, dataArgs)\n",
    "        \n",
    "        \n",
    "    fig, axs = plt.subplots(analyzeArgs[\"n_graphs\"],2, figsize=(15, 15), facecolor='w', edgecolor='k')\n",
    "    fig.subplots_adjust(hspace = .5, wspace=.001)\n",
    "\n",
    "    for i in range(0, analyzeArgs[\"n_graphs\"]):\n",
    "        \n",
    "        ## reconstruct upper triangular adjacency matrix\n",
    "        reconstructed_a = reconstruct_adjacency(A[i], dataArgs[\"clip\"], dataArgs[\"diag_offset\"])\n",
    "        reconstructed_a, nodes_n = unpad_matrix(reconstructed_a, dataArgs[\"diag_value\"], 0.1, dataArgs[\"fix_n\"])\n",
    "        \n",
    "        ## reconstruct graph\n",
    "        reconstructed_a = np.reshape(reconstructed_a, (reconstructed_a.shape[0], reconstructed_a.shape[1]))\n",
    "        g = nx.from_numpy_matrix(reconstructed_a)\n",
    "        \n",
    "        \n",
    "        ## original features\n",
    "        f_org = np.reshape(F_org[i], (-1,1))\n",
    "        reconstructed_f_org = unpad_attr(f_org, nodes_n, analyzeArgs, dataArgs)\n",
    "        \n",
    "        if reconstructed_f_org.shape[0] > 0:\n",
    "            fixed_cmap = shiftedColorMap(orig_cmap, start=min(reconstructed_f_org), midpoint=0.5, stop=max(reconstructed_f_org),name='fixed')\n",
    "        else:\n",
    "            fixed_cmap = shiftedColorMap(orig_cmap, start=0.5, midpoint=0.5, stop=0.5, name='fixed')\n",
    "        \n",
    "        plt.sca(axs[i, 0])\n",
    "        nx.draw(g, node_size=64, node_color=reconstructed_f_org, width=0.2, font_color='white', cmap=fixed_cmap)\n",
    "        axs[i, 0].set_axis_off()\n",
    "        \n",
    "        \n",
    "\n",
    "        ## modified features\n",
    "        f_mod = np.reshape(F_mod[i], (-1,1))\n",
    "        reconstructed_f_mod = unpad_attr(f_mod, nodes_n, analyzeArgs, dataArgs)\n",
    "        \n",
    "        if reconstructed_f_mod.shape[0] > 0:\n",
    "            fixed_cmap = shiftedColorMap(orig_cmap, start=min(reconstructed_f_mod), midpoint=0.5, stop=max(reconstructed_f_mod),name='fixed')\n",
    "        else:\n",
    "            fixed_cmap = shiftedColorMap(orig_cmap, start=0.5, midpoint=0.5, stop=0.5, name='fixed')\n",
    "        \n",
    "        plt.sca(axs[i, 1])\n",
    "        nx.draw(g, node_size=64, node_color=reconstructed_f_mod, width=0.2, font_color='white', cmap=fixed_cmap)\n",
    "        axs[i, 1].set_axis_off()\n",
    "        \n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    ## ENCODER - 2D Digit Classes ______________________________________________\n",
    "        \n",
    "    z_mean, z_log_var, _ = encoder.predict([F_org, A], batch_size = batch_size)\n",
    "    z_mean_mod, z_log_var_mod, _ = encoder.predict([F_mod, A], batch_size = batch_size)\n",
    "        \n",
    "        \n",
    "    ## toDO: measure the correlation between latent variable and the generative factor / features\n",
    "    \n",
    "    ## Feature Change ____________________________ \n",
    "        \n",
    "    z_mean = np.abs(z_mean[:analyzeArgs[\"n_graphs\"]])\n",
    "    z_mean_mod = np.abs(z_mean_mod[:analyzeArgs[\"n_graphs\"]])\n",
    "    z_var = np.exp(0.5 * z_log_var)[:analyzeArgs[\"n_graphs\"]]\n",
    "    z_var_mod = np.exp(0.5 * z_log_var_mod)[:analyzeArgs[\"n_graphs\"]]\n",
    "    \n",
    "    for latent_i in range(0, modelArgs[\"latent_dim\"]):   \n",
    "        \n",
    "        Normal_org = np.array([])\n",
    "        Normal_mod = np.array([])\n",
    "        total_mean_shift = 0\n",
    "        total_emd = 0\n",
    "                \n",
    "        for graph_i in range(0, analyzeArgs[\"n_graphs\"]):\n",
    "\n",
    "            ## compute shift and EMD\n",
    "            mean_shift = np.abs(z_mean[graph_i, latent_i] - z_mean_mod[graph_i, latent_i])\n",
    "            total_mean_shift += mean_shift\n",
    "\n",
    "            #x = np.linspace(z_mean[graph_i, latent_i] - 3*math.sqrt(z_var[graph_i, latent_i]), z_mean[graph_i, latent_i] + 3*math.sqrt(z_var[graph_i, latent_i]), 1000)\n",
    "            #y = stats.norm.pdf(x, z_mean[graph_i, latent_i], math.sqrt(z_var[graph_i, latent_i]))\n",
    "            #plt.plot(x, y, color='midnightblue', linestyle='solid', linewidth=4)\n",
    "            normal_org = np.random.normal(z_mean[graph_i, latent_i], z_var[graph_i, latent_i], analyzeArgs[\"norm_samples\"])\n",
    "            Normal_org = np.hstack((Normal_org, normal_org))\n",
    "            #sns.distplot(normal_org)\n",
    "            \n",
    "            #x_mod = np.linspace(z_mean_mod[graph_i, latent_i] - 3*math.sqrt(z_var_mod[graph_i, latent_i]), z_mean_mod[graph_i, latent_i] + 3*math.sqrt(z_var_mod[graph_i, latent_i]), 1000)\n",
    "            #y_mod = stats.norm.pdf(x_mod, z_mean_mod[graph_i, latent_i], math.sqrt(z_var_mod[graph_i, latent_i]))\n",
    "            #plt.plot(x_mod, y_mod, color='steelblue', linestyle='dashed', linewidth=4)\n",
    "            normal_mod = np.random.normal(z_mean_mod[graph_i, latent_i], z_var_mod[graph_i, latent_i], analyzeArgs[\"norm_samples\"])\n",
    "            Normal_mod = np.hstack((Normal_mod, normal_mod))\n",
    "            #sns.distplot(normal_mod)\n",
    "            \n",
    "            #plt.xlabel('latent posterior z_' + str(latent_i), fontweight = \"bold\")\n",
    "            #plt.annotate(\"mean shift:\" + str(mean_shift) , xy=(0, 1), xytext=(12, -12), va='top',xycoords='axes fraction', textcoords='offset points')\n",
    "            total_emd += wasserstein_distance(normal_org, normal_mod)\n",
    "\n",
    "        sns.set(style=\"white\", rc={\"axes.facecolor\": (0, 0, 0, 0)})\n",
    "\n",
    "        # Create the data\n",
    "        #g = np.tile(list(\"ABCDEFGHIJ\"), 1000) # size 1000\n",
    "        x = np.arange(0, analyzeArgs[\"n_graphs\"], 1)\n",
    "        x = np.repeat(x, analyzeArgs[\"norm_samples\"])\n",
    "        df = pd.DataFrame(dict(org=Normal_org, mod=Normal_mod, x=x))\n",
    "\n",
    "        #m = df.g.map(ord)  ## offset\n",
    "        #df[\"x\"] += m\n",
    "\n",
    "        # Initialize the FacetGrid object\n",
    "        pal = sns.cubehelix_palette(analyzeArgs[\"n_graphs\"], rot=-.25, light=.7)\n",
    "        g = sns.FacetGrid(df, row=\"x\", hue=\"x\", aspect=15, height=.75, palette=pal)\n",
    "\n",
    "        # Draw the densities in a few steps\n",
    "        #g.map(sns.kdeplot, \"org\", clip_on=False, shade=True, color=\"red\", alpha=1.0, lw=1.5, bw=.2)\n",
    "        #g.map(sns.kdeplot, \"org\", clip_on=False, shade=True, alpha=1.0, lw=2, bw=.2)\n",
    "        g.map(sns.distplot, \"org\", hist = False, kde = True, kde_kws={'linewidth': 0, \"shade\": True, \"alpha\": 1.0})\n",
    "        #g.map(sns.kdeplot, \"mod\", clip_on=False, shade=True, alpha=1.0, lw=1.5, bw=.2)\n",
    "        #g.map(sns.kdeplot, \"mod\", clip_on=False, shade=False, color=\"orange\", alpha=1.0, lw=2, bw=.2)\n",
    "        g.map(sns.distplot, \"mod\", color=\"dimgrey\", hist = False, kde = True, bins = 1000, kde_kws={'linestyle':':', 'linewidth': 4})\n",
    "        g.map(plt.axhline, y=0, lw=2, clip_on=False)\n",
    "\n",
    "\n",
    "        # Define and use a simple function to label the plot in axes coordinates\n",
    "        def label(x, color, label):\n",
    "            ax = plt.gca()\n",
    "            ax.text(0, .2, label, fontweight=\"bold\", color=color,\n",
    "                    ha=\"left\", va=\"center\", transform=ax.transAxes)\n",
    "\n",
    "        #g.map(label, \"x\")\n",
    "\n",
    "        # Set the subplots to overlap\n",
    "        g.fig.subplots_adjust(hspace=-.5)\n",
    "\n",
    "        # Remove axes details that don't play well with overlap\n",
    "        g.set_titles(\"\")\n",
    "        g.set(yticks=[])\n",
    "        g.despine(bottom=True, left=True)\n",
    "        \n",
    "        print(\"\\nlatent variable z_\", latent_i)\n",
    "        print(\"average_mean_shift:\", total_mean_shift / analyzeArgs[\"n_graphs\"])\n",
    "        print(\"average_wasserstein_dist:\", total_emd / analyzeArgs[\"n_graphs\"])\n",
    "        \n",
    "        plt.pause(0.05)\n",
    "\n",
    "\n",
    "        \n",
    "## PLOT RESULTS ________________________________________\n",
    "print(\"ground truth attribute dependence: \", dataArgs[\"feature_dependence\"])\n",
    "analyzeArgs = {\"mod_degree\": 1.0, \"n_graphs\": 5, \"f_variation\": \"random\", \"norm_samples\": 10000, \"metric\": \"none\", \"normalize_feature\": False}\n",
    "modification_comparison(analyzeArgs, modelArgs, models, data, batch_size=trainArgs[\"batch_size\"], model_name=\"vae_mlp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Latent Variable Correlation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def feature_latent_correlation(analyzeArgs, modelArgs, models, data, batch_size=128,model_name=\"vae_graph\"):\n",
    "\n",
    "    \n",
    "    ## unpack models and data__________________________\n",
    "    \n",
    "    encoder, decoder = models  # trained models\n",
    "    #F, [A_fil, A] = data\n",
    "    \n",
    "    ## generate input network _________________________\n",
    "    \n",
    "    g = get_graph(analyzeArgs[\"n\"], analyzeArgs[\"p\"], draw = False) \n",
    "    g, a = sort_adj(g)\n",
    "    a = pad_matrix(a, dataArgs[\"n_max\"], dataArgs[\"diag_value\"])  # pad adjacency matrix to allow less nodes than n_max and fill diagonal        \n",
    "    a_transformed = reshape_A(a, diag_offset = dataArgs[\"diag_offset\"])\n",
    "    A = np.reshape(a_transformed, (1, *a_transformed.shape))\n",
    "    A = np.tile(A, (analyzeArgs[\"mod_steps\"],1))\n",
    "        \n",
    "    f = generate_features(dataArgs, g, analyzeArgs[\"n\"], analyzeArgs[\"p\"]) \n",
    "    print(\"original features:\\n\", f)\n",
    "    \n",
    "    F_mod = np.zeros((analyzeArgs[\"mod_steps\"], dataArgs[\"n_max\"], dataArgs[\"n_features\"]))\n",
    "        \n",
    "                 \n",
    "    ## Modify Features__________________________________\n",
    "                 \n",
    "    mod_degree = np.linspace(0,dataArgs[\"n_max\"],analyzeArgs[\"mod_steps\"], dtype = int) \n",
    "    \n",
    "    for i, mod in enumerate(mod_degree):\n",
    "        \n",
    "        f_mod = np.copy(f)\n",
    "\n",
    "        if analyzeArgs[\"f_variation\"] == \"random\":\n",
    "            for m in range(0, mod):\n",
    "                swap = np.random.randint(low = 0, high = f_mod.shape[0], size = 2)\n",
    "                temp = f[swap[0]]\n",
    "                f_mod[swap[0]] = f_mod[swap[1]]\n",
    "                f_mod[swap[1]] = temp\n",
    "\n",
    "\n",
    "        elif analyzeArgs[\"f_variation\"] == \"equal\":\n",
    "\n",
    "            mod_ratio = mod / max(mod_degree)\n",
    "            norm_mean = np.mean(f)\n",
    "\n",
    "            for i in range(0,len(f_mod)):\n",
    "\n",
    "                if f_mod[i] > norm_mean:\n",
    "                    f_mod[i] = f_mod[i] - (mod_ratio * (np.abs(f_mod[i] - norm_mean)))\n",
    "                else:\n",
    "                    f_mod[i] = f_mod[i] + (mod_ratio * (np.abs(f_mod[i] - norm_mean )))\n",
    "\n",
    "\n",
    "        elif analyzeArgs[\"f_variation\"] == \"uniform\":\n",
    "\n",
    "            f_mod = np.ones(f_mod.shape[0]) * (mod / max(mod_degree))\n",
    "            f_mod = np.reshape(f_mod, (f_mod.shape[-1],1))\n",
    "\n",
    "        ## pad features with zeroes\n",
    "        f_mod = pad_attr(f_mod, dataArgs)\n",
    "        F_mod[i] = f_mod    \n",
    "        \n",
    "\n",
    "    ## ENCODER ______________________________________________\n",
    "    z_mean, _, _ = encoder.predict([np.squeeze(F_mod), A], batch_size = batch_size)\n",
    "\n",
    "\n",
    "    ## Measure the Mutual Information Gap ____________________________________________\n",
    "    if analyzeArgs[\"metric\"] == \"mig\":\n",
    "        mig = compute_mig(mod_degree, np.squeeze(z_mean))\n",
    "        \n",
    "        \n",
    "    ## toDO: measure the correlation between latent variable and the generative factor / features\n",
    "\n",
    "    ## Visualize Latent Variables x Feature Change ____________________________\n",
    "\n",
    "    fig, ax = plt.subplots(nrows= z_mean.shape[-1] , ncols= 1, figsize=(10,20))\n",
    "\n",
    "    for latent_z, row in enumerate(ax):        \n",
    "\n",
    "            y = z_mean[:,latent_z]\n",
    "            x = mod_degree / max(mod_degree)\n",
    "            sns.regplot(x, y, color=\"steelblue\", ax=row, ci = None, x_ci='sd', x_estimator=np.mean)\n",
    "\n",
    "            ## compute correlation and standardized covariance\n",
    "            corr = round(pearsonr(x,y)[0],3)\n",
    "            cov = round(np.cov(x, y)[0][1]/max(x),3)\n",
    "            std = round(np.std(y),3)\n",
    "            row.annotate(\"corr:\"+str(corr)+\", std:\"+str(std), xy=(0, 1), xytext=(12, -12), va='top',xycoords='axes fraction', textcoords='offset points', fontweight='bold')\n",
    "\n",
    "            \n",
    "\n",
    "    ## add row and column titles _____________________\n",
    "\n",
    "    rows = ['z_{}'.format(row) for row in range(z_mean.shape[-1])]\n",
    "    cols = [t for t in [\"feature modification percentage\"]]\n",
    "\n",
    "    for axis, col in zip(ax, cols):\n",
    "        axis.set_title(col, fontweight='bold')\n",
    "\n",
    "    for axis, row in zip(ax, rows):\n",
    "        axis.set_ylabel(row, rotation=0, size='large', fontweight='bold')\n",
    "\n",
    "\n",
    "        \n",
    "## PLOT RESULTS ________________________________________\n",
    "\n",
    "dataArgs[\"feature_dependence\"] = \"degree\"  #\"degree\", \"random\", \"p\"\n",
    "analyzeArgs = {\"n\": 5, \"p\": 0.4, \"mod_steps\": 1000, \"f_variation\": \"random\", \"metric\": \"none\", \"normalize_feature\": False}\n",
    "feature_latent_correlation(analyzeArgs, modelArgs, models, data, batch_size=trainArgs[\"batch_size\"], model_name=\"vae_mlp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
