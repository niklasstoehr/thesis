{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Imputation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "## libs \n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "## Keras\n",
    "from keras.layers import Lambda, Input, Dense, Conv2D, Conv2DTranspose, Flatten, Reshape\n",
    "from keras.models import Model\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.losses import mse, binary_crossentropy\n",
    "from keras.utils import plot_model\n",
    "from keras import backend as K\n",
    "\n",
    "## Basic\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import argparse\n",
    "import os\n",
    "import random\n",
    "import itertools\n",
    "import time\n",
    "\n",
    "# Computation\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mutual_info_score\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "import scipy\n",
    "from scipy.stats.stats import pearsonr \n",
    "import pandas as pd\n",
    "\n",
    "## Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import seaborn as sns\n",
    "\n",
    "## Network Processing\n",
    "import networkx as nx\n",
    "from networkx.generators import random_graphs\n",
    "\n",
    "## node colour\n",
    "orig_cmap = plt.cm.PuBu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Supporting Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "## supporting functions\n",
    "from support.preprocessing import sort_adj, reshape_A, calculate_A_shape, reconstruct_adjacency, pad_matrix, unpad_matrix, pad_attr, unpad_attr, prepare_in_out\n",
    "from support.metrics import compute_mig, compute_mi\n",
    "from support.graph_generating import generate_single_features, generate_manifold_features\n",
    "from support.latent_space import vis2D, visDistr\n",
    "from support.comparing import compare_manifold_adjacency, compare_topol_manifold\n",
    "from support.plotting import shiftedColorMap\n",
    "\n",
    "## graph sampling\n",
    "from sampling import ForestFire, Metropolis_Hastings, Random_Walk, Snowball, Ties, Base_Samplers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_adj(g):\n",
    "    node_k1 = dict(g.degree())  # sort by degree\n",
    "    node_k2 = nx.average_neighbor_degree(g)  # sort by neighbor degree\n",
    "    node_closeness = nx.closeness_centrality(g)\n",
    "    node_betweenness = nx.betweenness_centrality(g)\n",
    "\n",
    "    node_sorting = list()\n",
    "\n",
    "    ## sort topology amd attributes _________________\n",
    "\n",
    "    # for node_id in range(0, len(g)):\n",
    "    for node_id in g.nodes():\n",
    "        node_sorting.append(\n",
    "            (node_id, node_k1[node_id], node_k2[node_id], node_closeness[node_id], node_betweenness[node_id]))\n",
    "\n",
    "    node_descending = sorted(node_sorting, key=lambda x: (x[1], x[2], x[3], x[4]), reverse=True)\n",
    "\n",
    "    mapping = dict()\n",
    "    # f_sorted = f\n",
    "\n",
    "    for i, node in enumerate(node_descending):\n",
    "        mapping[node[0]] = i\n",
    "        # f_sorted[i] = node_descending[i][5]\n",
    "\n",
    "    a = nx.adjacency_matrix(g, nodelist=mapping.keys()).todense()\n",
    "\n",
    "    return g, a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Real-World Graph Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     19
    ]
   },
   "outputs": [],
   "source": [
    "g_complete = nx.read_edgelist(\"/Users/niklasstoehr/Programming/thesis/4_real_attr/data/mag/edges_py.csv\",  nodetype=int, delimiter = \",\")\n",
    "#a_complete = nx.adjacency_matrix(g_complete)\n",
    "a_complete = None\n",
    "\n",
    "df_nodes  = pd.read_csv(\"/Users/niklasstoehr/Programming/thesis/4_real_attr/data/mag/nodes_py.csv\", header = None)\n",
    "nodes = df_nodes.values\n",
    "nodes = [l.tolist() for l in list(nodes)]\n",
    "nodes_num = [i[0] for i in nodes]\n",
    "\n",
    "g_complete.add_nodes_from(nodes_num)\n",
    "node_attr_dict = dict()\n",
    "\n",
    "for num, name, c in nodes:\n",
    "    node_attr_dict[num] = c\n",
    "        \n",
    "nx.set_node_attributes(g_complete, node_attr_dict, \"citations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     3
    ]
   },
   "outputs": [],
   "source": [
    "sampleArgs = {\"sample\": \"biased_random_walk\", \"jump_bias\": \"random_walk_induced_graph_sampling\", \"n\": 10, \"p\": 20.0, \"q\": 100.0, \"source_starts\": 2, \"source_returns\": 4, \"depth\": 2}\n",
    "\n",
    "\n",
    "def get_graph(sampleArgs,g_complete,a_complete):\n",
    "    \n",
    "    if sampleArgs[\"sample\"] == \"biased_random_walk\":\n",
    "        sampler = Base_Samplers.Base_Samplers(g_complete,a_complete)\n",
    "        #sampler = Base_Samplers(g_complete,a_complete)\n",
    "        g = sampler.biased_random_walk(sampleArgs[\"n\"], sampleArgs[\"p\"], sampleArgs[\"q\"])\n",
    "\n",
    "    if sampleArgs[\"sample\"] == \"forestfire\":\n",
    "        sampler = ForestFire.ForestFire(g_complete,a_complete)\n",
    "        g = sampler.forestfire(sampleArgs[\"n\"])\n",
    "\n",
    "    if sampleArgs[\"sample\"] == \"snowball\":\n",
    "        sampler = Snowball.Snowball(g_complete,a_complete)\n",
    "        g = sampler.snowball(sampleArgs[\"source_starts\"], sampleArgs[\"source_returns\"])\n",
    "\n",
    "    if sampleArgs[\"sample\"] == \"random_walk_induced_graph_sampling\":\n",
    "        sampler = Random_Walk.Random_Walk(g_complete,a_complete)\n",
    "        g = sampler.random_walk_induced_graph_sampling(sampleArgs[\"n\"])\n",
    "\n",
    "    if sampleArgs[\"sample\"] == \"random_walk_sampling_with_fly_back\":\n",
    "        sampler = Random_Walk.Random_Walk(g_complete,a_complete)\n",
    "        g = sampler.random_walk_sampling_with_fly_back(sampleArgs[\"n\"], sampleArgs[\"p\"])\n",
    "        \n",
    "    if sampleArgs[\"sample\"] == \"standard_bfs\":\n",
    "        sampler = Base_Samplers.Base_Samplers(g_complete,a_complete)\n",
    "        g = sampler.standard_bfs(sampleArgs[\"source_starts\"], sampleArgs[\"depth\"]) \n",
    "        \n",
    "    if sampleArgs[\"sample\"] == \"bfs\":\n",
    "        sampler = Base_Samplers.Base_Samplers(g_complete,a_complete)\n",
    "        g = sampler.bfs(sampleArgs[\"n\"]) \n",
    "        \n",
    "    if sampleArgs[\"sample\"] == \"walk\":\n",
    "        sampler = Base_Samplers.Base_Samplers(g_complete,a_complete)\n",
    "        g = sampler.walk(sampleArgs[\"source_starts\"], sampleArgs[\"source_returns\"], sampleArgs[\"p\"])        \n",
    "        \n",
    "    if sampleArgs[\"sample\"] == \"jump\":\n",
    "        sampler = Base_Samplers.Base_Samplers(g_complete,a_complete)\n",
    "        g = sampler.jump(sampleArgs[\"source_starts\"], sampleArgs[\"p\"], sampleArgs[\"jump_bias\"])\n",
    "        \n",
    "    if sampleArgs[\"sample\"] == \"adjacency\":\n",
    "        sampler = Base_Samplers.Base_Samplers(g_complete,a_complete)\n",
    "        g = sampler.adjacency(sampleArgs[\"n\"]) \n",
    "        \n",
    "    if sampleArgs[\"sample\"] == \"select\":\n",
    "        sampler = Base_Samplers.Base_Samplers(g_complete,a_complete)\n",
    "        g = sampler.adjacency(sampleArgs[\"n\"]) \n",
    "    \n",
    "    return g \n",
    "\n",
    "g = get_graph(sampleArgs,g_complete,a_complete)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "dataArgs = {\"n_graphs\": 100, \"n_max\": 12, \"fix_n\": False, \"diag_offset\": -2, \"diag_value\": 1, \"clip\": True, \"n_features\": 1}  #\"diag_offset\" - 1 == full adjacency\n",
    "\n",
    "def generate_data(dataArgs): \n",
    "    \n",
    "    ## Data ________________________________\n",
    "\n",
    "    G = np.zeros((dataArgs[\"n_graphs\"], *calculate_A_shape(dataArgs[\"n_max\"], dataArgs[\"diag_offset\"])))\n",
    "    F = np.zeros((dataArgs[\"n_graphs\"], dataArgs[\"n_max\"], dataArgs[\"n_features\"]))    \n",
    "    \n",
    "    ## Generate Graph Data_______________________________\n",
    "\n",
    "    for i in tqdm(range(0,dataArgs[\"n_graphs\"])):\n",
    "        \n",
    "        ## Generate Graph Type ______________________________________________\n",
    "\n",
    "        g = get_graph(sampleArgs,g_complete,a_complete)\n",
    "        \n",
    "        g, a = sort_adj(g)\n",
    "        a = pad_matrix(a, dataArgs[\"n_max\"], dataArgs[\"diag_value\"])  # pad adjacency matrix to allow less nodes than n_max and fill diagonal        \n",
    "        a_transformed = reshape_A(a, diag_offset = dataArgs[\"diag_offset\"])\n",
    "        \n",
    "        \n",
    "        ## Generate / Load Node Features ______________________________________________\n",
    "        f = np.asarray([node_attr_dict[x] for x in list(g.nodes())])\n",
    "        #f = np.asarray(nx.get_node_attributes(g_complete, \"citations\"))\n",
    "        \n",
    "        ## pad features with zeroes\n",
    "        f = np.reshape(f, (f.shape[0], 1))\n",
    "        f = pad_attr(f, dataArgs)\n",
    "\n",
    "        \n",
    "        ## Build Data Arrays___________________________________________________\n",
    "\n",
    "        F[i] = f\n",
    "        G[i] = a_transformed\n",
    "\n",
    "\n",
    "    ## Input and Output Size ___________________________________________________________\n",
    "\n",
    "    input_shape, output_shape = prepare_in_out(dataArgs[\"diag_offset\"], calculate_A_shape(dataArgs[\"n_max\"], dataArgs[\"diag_offset\"]), F.shape)\n",
    "    print(\"input_shape:\", input_shape, \", output_shape:\", output_shape)\n",
    "    \n",
    "    ## scale features in F for smoother training\n",
    "    #scaler = MinMaxScaler()\n",
    "    #scaler.fit(F)\n",
    "    #F = scaler.transform(F)\n",
    "    \n",
    "    return G, F, input_shape,output_shape\n",
    "    \n",
    "G, F, input_shape, output_shape = generate_data(dataArgs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_features(dataArgs, g, n):\n",
    "    \n",
    "        if dataArgs[\"feature_dependence\"] == \"random\":\n",
    "            f = np.random.rand(n, dataArgs[\"n_features\"])                   ## float\n",
    "            #F[i] = np.random.randint(2, size=(dataArgs[\"n_max\"],dataArgs[\"n_features\"]))   ## int\n",
    "            \n",
    "        if dataArgs[\"feature_dependence\"] == \"norm_degree\":\n",
    "            if dataArgs[\"n_features\"] == 1:\n",
    "                \n",
    "                f = np.asarray([int(x[1]) for x in sorted(g.degree())])  \n",
    "                f = (f) / (max(f)+1)\n",
    "                f = np.reshape(f, (f.shape[-1],1))\n",
    "                \n",
    "                    \n",
    "        if dataArgs[\"feature_dependence\"] == \"degree\":\n",
    "            if dataArgs[\"n_features\"] == 1:\n",
    "                \n",
    "                f = np.asarray([int(x[1]) for x in sorted(g.degree())])  \n",
    "                f = (f+1) / (dataArgs[\"n_max\"]+1)\n",
    "                f = np.reshape(f, (f.shape[-1],1))\n",
    "                \n",
    "                \n",
    "        return f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def imput_data(imputeArgs, modelArgs, dataArgs, G, F):\n",
    "        \n",
    "    G_imputed = np.copy(G)\n",
    "    F_imputed = np.copy(F)\n",
    "    \n",
    "    if imputeArgs[\"impute\"] == \"features\":\n",
    "        \n",
    "        for i, (g,f) in enumerate(zip(G_imputed, F_imputed)):\n",
    "            \n",
    "            f = np.squeeze(f)\n",
    "            num_features = len(f[f > 0])\n",
    "            impute_num = int(imputeArgs[\"impute_frac\"] * num_features)\n",
    "            impute_f_ind = random.sample(range(num_features), impute_num)  ## impute features\n",
    "\n",
    "            row, col = np.diag_indices(f.shape[0])\n",
    "\n",
    "            f[impute_f_ind] = imputeArgs[\"impute_value\"]  ## replace features\n",
    "            F_imputed[i] = np.reshape(f,(f.shape[-1], 1))\n",
    "        \n",
    "        \n",
    "    \n",
    "    if imputeArgs[\"impute\"] == \"structure\":\n",
    "    \n",
    "        for i, (g,f) in enumerate(zip(G_imputed, F_imputed)):\n",
    "                        \n",
    "            f = np.squeeze(f)            \n",
    "            num_nodes = len(f[f > 0])\n",
    "            impute_num = int(imputeArgs[\"impute_frac\"] * num_nodes)\n",
    "            impute_n_ind = random.sample(range(num_nodes), impute_num)  ## impute nodes\n",
    "            \n",
    "            ## remove edges of imputed nodes\n",
    "            for impute_n in impute_n_ind:\n",
    "                \n",
    "                ## remove rows\n",
    "                g[impute_n,:impute_n] = 0\n",
    "                g[impute_n,impute_n+1:] = 0\n",
    "                \n",
    "                ## remove columns\n",
    "                g[:impute_n,impute_n] = 0\n",
    "                g[impute_n+1:,impute_n] = 0\n",
    "\n",
    "            G_imputed[i] = g\n",
    "        \n",
    "    return G_imputed, F_imputed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# beta-VAE Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "## Model Setup\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "## Keras\n",
    "from keras.layers import Lambda, Input, Dense, Conv2D, Conv2DTranspose, Flatten, Reshape, Dropout, Activation, concatenate\n",
    "from keras.models import Model\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.losses import mse, binary_crossentropy\n",
    "from keras.utils import plot_model\n",
    "from keras import backend as K\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from support.keras_dgl.utils import *\n",
    "from support.keras_dgl.layers import MultiGraphCNN\n",
    "\n",
    "\n",
    "class VAE():\n",
    "\n",
    "    # reparameterization trick\n",
    "    # instead of sampling from Q(z|X), sample eps = N(0,I)\n",
    "    # then z = z_mean + sqrt(var)*eps\n",
    "\n",
    "    def sampling(self, args):\n",
    "        \"\"\"Reparameterization trick by sampling fr an isotropic unit Gaussian.\n",
    "        # Arguments\n",
    "            args (tensor): mean and log of variance of Q(z|X)\n",
    "        # Returns\n",
    "            z (tensor): sampled latent vector\n",
    "        \"\"\"\n",
    "\n",
    "        z_mean, z_log_var = args\n",
    "        batch = K.shape(z_mean)[0]\n",
    "        dim = K.int_shape(z_mean)[1]\n",
    "        # by default, random_normal has mean=0 and std=1.0\n",
    "        epsilon = K.random_normal(shape=(batch, dim))\n",
    "        return z_mean + K.exp(0.5 * z_log_var) * epsilon\n",
    "\n",
    "    \n",
    "    \n",
    "    def __init__(self, modelArgs, trainArgs, g_train, g_train_mod_imputed, g_test, g_test_mod_imputed, f_train, f_train_imputed, f_test, f_test_imputed):\n",
    "\n",
    "        ## MODEL ______________________________________________________________       \n",
    "            \n",
    "        ## Graph Neural Network Architecture __________________________________\n",
    "            \n",
    "        ## 1) build encoder model____________________________________\n",
    "\n",
    "        # build graph_conv_filters\n",
    "        SYM_NORM = True\n",
    "        num_filters = modelArgs['gnn_filters']\n",
    "        graph_conv_filters = preprocess_adj_tensor_with_identity(np.squeeze(g_train), SYM_NORM)\n",
    "\n",
    "        # build model\n",
    "        X_input = Input(shape=(f_train.shape[1], f_train.shape[2]))\n",
    "        graph_conv_filters_input = Input(shape=(graph_conv_filters.shape[1], graph_conv_filters.shape[2]))\n",
    "\n",
    "        # define inputs of features and graph topologies\n",
    "        inputs = [X_input, graph_conv_filters_input]\n",
    "\n",
    "        x = MultiGraphCNN(100, num_filters, activation='elu')([X_input, graph_conv_filters_input])\n",
    "        x = Dropout(0.2)(x)\n",
    "        x = MultiGraphCNN(100, num_filters, activation='elu')([x, graph_conv_filters_input])\n",
    "        x = Dropout(0.2)(x)\n",
    "        x = Lambda(lambda x: K.mean(x, axis=1))(x)  # adding a node invariant layer to make sure output does not depends upon the node order in a graph.\n",
    "        x = Dense(8, activation='relu')(x)\n",
    "        z_mean = Dense(modelArgs[\"latent_dim\"], name='z_mean')(x)\n",
    "        z_log_var = Dense(modelArgs[\"latent_dim\"], name='z_log_var')(x)\n",
    "\n",
    "        # use reparameterization trick to push the sampling out as input\n",
    "        # note that \"output_shape\" isn't necessary with the TensorFlow backend\n",
    "        z = Lambda(self.sampling, output_shape=(modelArgs[\"latent_dim\"],), name='z')([z_mean, z_log_var])\n",
    "        \n",
    "        latent_inputs = Input(shape=(modelArgs[\"latent_dim\"],), name='z_sampling')\n",
    "\n",
    "        \n",
    "        ## 2.1) build attribute decoder model __________________________\n",
    "\n",
    "        y = Dense(4, activation='relu')(latent_inputs)\n",
    "        y = Dense(6, activation='relu')(latent_inputs)\n",
    "        y = Dense(10, activation='relu')(latent_inputs)\n",
    "        y = Dense(modelArgs[\"output_shape\"][0][0], activation='sigmoid')(y)\n",
    "        attr_output = Reshape(modelArgs[\"output_shape\"][0])(y)\n",
    "        \n",
    "        \n",
    "        \n",
    "        ## 2.2) build topology decoder model __________________________\n",
    "\n",
    "        ## shape info needed to build decoder model\n",
    "        inputs_2D_encoder = Input(shape=modelArgs[\"input_shape\"][1], name='encoder_input')\n",
    "        x_2D = inputs_2D_encoder\n",
    "        for i in range(2):\n",
    "            modelArgs['conv_filters'] *= 2\n",
    "            x_2D = Conv2D(filters=modelArgs['conv_filters'], kernel_size=modelArgs['kernel_size'], activation='relu',strides=2, padding='same')(x_2D)\n",
    "        shape_2D = K.int_shape(x_2D)\n",
    "\n",
    "        x_2D = Dense(shape_2D[1] * shape_2D[2] * shape_2D[3], activation='relu')(latent_inputs)\n",
    "        x_2D = Reshape((shape_2D[1], shape_2D[2], shape_2D[3]))(x_2D)\n",
    "\n",
    "        for i in range(2):\n",
    "            x_2D = Conv2DTranspose(filters=modelArgs['conv_filters'], kernel_size=modelArgs['kernel_size'],activation='relu', strides=2, padding='same')(x_2D)\n",
    "            modelArgs['conv_filters'] //= 2\n",
    "\n",
    "        topol_output = Conv2DTranspose(filters=1, kernel_size=modelArgs['kernel_size'], activation='sigmoid',padding='same', name='decoder_output')(x_2D)\n",
    "\n",
    "\n",
    "\n",
    "     \n",
    "    \n",
    "        ## INSTANTIATE___________________________________\n",
    "\n",
    "        ## 1) instantiate joint encoder model\n",
    "        encoder = Model(inputs, [z_mean, z_log_var, z], name='encoder')\n",
    "        encoder.summary()\n",
    "\n",
    "        ## 2) instantiate topology decoder model\n",
    "        decoder = Model(latent_inputs, [attr_output, topol_output], name='decoder')\n",
    "        decoder.summary()\n",
    "        \n",
    "        ## 3) instantiate VAE model\n",
    "        attr_topol_outputs = decoder(encoder(inputs)[2])\n",
    "        vae = Model(inputs, attr_topol_outputs, name='vae')\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "        ## LOSS FUNCTIONS ______________________________________\n",
    "        \n",
    "        def loss_func(y_true, y_pred):\n",
    "            \n",
    "            y_true_attr = y_true[0]\n",
    "            y_pred_attr = y_pred[0]\n",
    "            \n",
    "            y_true_topol = y_true[1]\n",
    "            y_pred_topol = y_pred[1]            \n",
    "\n",
    "            ## ATTR RECONSTRUCTION LOSS_______________________            \n",
    "            ## mean squared error\n",
    "            attr_reconstruction_loss = mse(K.flatten(y_true_attr), K.flatten(y_pred_attr))\n",
    "            attr_reconstruction_loss *= modelArgs[\"input_shape\"][0][0]\n",
    "            \n",
    "            ## TOPOL RECONSTRUCTION LOSS_______________________\n",
    "            ## binary cross-entropy\n",
    "            topol_reconstruction_loss = binary_crossentropy(K.flatten(y_true_topol), K.flatten(y_pred_topol))\n",
    "            topol_reconstruction_loss *= (modelArgs[\"input_shape\"][1][0] * modelArgs[\"input_shape\"][1][1])\n",
    "                     \n",
    "            ## KL LOSS _____________________________________________\n",
    "            kl_loss = 1 + z_log_var - K.square(z_mean) - K.exp(z_log_var)\n",
    "            kl_loss = K.sum(kl_loss, axis=-1)\n",
    "            kl_loss *= -0.5\n",
    "\n",
    "            ## COMPLETE LOSS __________________________________________________\n",
    "\n",
    "            loss = K.mean(trainArgs[\"loss_weights\"][0] * attr_reconstruction_loss + trainArgs[\"loss_weights\"][1] * topol_reconstruction_loss + trainArgs[\"loss_weights\"][2] * kl_loss)\n",
    "            \n",
    "            return loss\n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "        ## MODEL COMPILE______________________________________________\n",
    "        \n",
    "        vae.compile(optimizer='adam', loss=loss_func)\n",
    "        vae.summary()\n",
    "        \n",
    "        \n",
    "\n",
    "        ## TRAIN______________________________________________\n",
    "\n",
    "        # load the autoencoder weights\n",
    "\n",
    "        if trainArgs[\"weights\"] == \"load\":\n",
    "\n",
    "            vae.load_weights(\"models/weights/vae_mlp_mnist_latent_dim_\" + str(modelArgs[\"latent_dim\"]) + \".h5\")\n",
    "\n",
    "        # train the autoencoder\n",
    "\n",
    "        elif trainArgs[\"weights\"] == \"train\":\n",
    "\n",
    "            # Set callback functions to early stop training and save the best model so far\n",
    "            callbacks = [EarlyStopping(monitor='val_loss', patience=trainArgs[\"early_stop\"]), ModelCheckpoint(filepath=\"models/weights/vae_mlp_mnist_latent_dim_\" + str(modelArgs[\"latent_dim\"]) + \".h5\",save_best_only=True)]\n",
    "\n",
    "            vae.fit([f_train_imputed, g_train_mod_imputed], [f_train, g_train], epochs=trainArgs[\"epochs\"],batch_size=trainArgs[\"batch_size\"], callbacks=callbacks,validation_data=([f_test_imputed, g_test_mod_imputed], [f_test, g_test]))\n",
    "            vae.save_weights(\"models/weights/vae_mlp_mnist_latent_dim_\" + str(modelArgs[\"latent_dim\"]) + \".h5\")\n",
    "\n",
    "            self.model = (encoder, decoder)\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Split and Impute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainArgs = {\"loss_weights\": [100,1,5], \"weights\": \"train\", \"early_stop\": 1, \"batch_size\": 128, \"epochs\": 50, \"data_split\": 0.2}\n",
    "modelArgs = {\"nn_architecture\": \"gnn\", \"latent_dim\": 2, \"gnn_filters\": 2, \"conv_filters\": 16, \"kernel_size\": 3, \"input_shape\": input_shape, \"output_shape\": output_shape, \"param_loss\": False,}\n",
    "imputeArgs = {\"impute\": \"features\", \"impute_frac\": 1.0, \"impute_value\": 0.0, }\n",
    "\n",
    "from support.keras_dgl.utils import *\n",
    "from support.keras_dgl.layers import MultiGraphCNN\n",
    "\n",
    "## Train and Validation Split _______________________________________________\n",
    "g_train, g_test, f_train, f_test = train_test_split(G, F, test_size=trainArgs[\"data_split\"], random_state=1, shuffle=True)\n",
    "\n",
    "## impute the data\n",
    "g_train_imputed, f_train_imputed = imput_data(imputeArgs, modelArgs, dataArgs, g_train, f_train)\n",
    "g_test_imputed, f_test_imputed = imput_data(imputeArgs, modelArgs, dataArgs, g_test, f_test)\n",
    "\n",
    "# build graph_conv_filters\n",
    "SYM_NORM = True\n",
    "g_train_mod_imputed = preprocess_adj_tensor_with_identity(np.squeeze(g_train_imputed), SYM_NORM)\n",
    "g_test_mod_imputed = preprocess_adj_tensor_with_identity(np.squeeze(g_test_imputed), SYM_NORM)\n",
    "\n",
    "data = (f_test_imputed, [g_test_mod_imputed, g_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "vae = VAE(modelArgs, trainArgs, g_train, g_train_mod_imputed, g_test, g_test_mod_imputed, f_train, f_train_imputed, f_test, f_test_imputed)\n",
    "\n",
    "models = vae.model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Single Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "analyzeArgs = {\"z\": [0,1], \"activations\": [20,-2], \"normalize_feature\": False}\n",
    "generate_single_features(analyzeArgs, modelArgs, dataArgs, models, orig_cmap)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Impaint Graphs (Extreme Cases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def impaint(analyzeArgs, imputeArgs, modelArgs, models, batch_size=128):\n",
    "\n",
    "    print(\"impainting the\", imputeArgs[\"impute\"])\n",
    "    \n",
    "    ## unpack models__________________________\n",
    "    \n",
    "    encoder, decoder = models  # trained models\n",
    "    \n",
    "    ## generate feature data___________________________\n",
    "    f = np.reshape([analyzeArgs[\"f\"]] * analyzeArgs[\"n\"], (analyzeArgs[\"n\"], 1))\n",
    "    \n",
    "    ## pad features with zeroes\n",
    "    f = pad_attr(f, dataArgs)\n",
    "\n",
    "    \n",
    "    ## generate graph data___________________________\n",
    "    G = np.zeros((1, *calculate_A_shape(dataArgs[\"n_max\"], dataArgs[\"diag_offset\"])))\n",
    "    F = np.zeros((1, dataArgs[\"n_max\"], dataArgs[\"n_features\"]))\n",
    "\n",
    "    g = get_graph(sampleArgs,g_complete,a_complete)\n",
    "    \n",
    "    g, a = sort_adj(g)\n",
    "    a = pad_matrix(a, dataArgs[\"n_max\"], dataArgs[\"diag_value\"])  # pad adjacency matrix to allow less nodes than n_max and fill diagonal\n",
    "    a = reshape_A(a, diag_offset = dataArgs[\"diag_offset\"])\n",
    "    \n",
    "    G[0] = a\n",
    "    F[0] = f\n",
    "    \n",
    "    \n",
    "    ## impute data_______________________\n",
    "    \n",
    "    a_imputed, f_imputed = imput_data(imputeArgs, modelArgs, dataArgs, G, F)\n",
    "    \n",
    "    \n",
    "    ## ENCODER_________________________________\n",
    "    \n",
    "    # build graph_conv_filters\n",
    "    SYM_NORM = True\n",
    "    a_imputed_mod = preprocess_adj_tensor_with_identity(np.squeeze(a_imputed), SYM_NORM)\n",
    "    z_mean, _, _ = encoder.predict([f_imputed, a_imputed_mod], batch_size = batch_size)\n",
    "\n",
    "    \n",
    "    ## DECODER_________________________________    \n",
    "    \n",
    "    [f_decoded, a_decoded] = decoder.predict(z_mean)\n",
    "    a_decoded = np.squeeze(a_decoded[0])\n",
    "    f_decoded = f_decoded[0]\n",
    "    \n",
    "    \n",
    "    ## GRAPH RECONSTRUCTION______________________\n",
    "    \n",
    "    ## reconstruct graph from output\n",
    "    reconstructed_a = reconstruct_adjacency(a_decoded, dataArgs[\"clip\"], dataArgs[\"diag_offset\"])\n",
    "    reconstructed_a, n_nodes = unpad_matrix(reconstructed_a, dataArgs[\"diag_value\"], 0.2, dataArgs[\"fix_n\"])\n",
    "    reconstructed_g = nx.from_numpy_matrix(reconstructed_a)\n",
    "    \n",
    "    ## reconstruct attributes\n",
    "    reconstructed_f = unpad_attr(f_decoded, n_nodes, analyzeArgs, dataArgs)\n",
    "    \n",
    "    ## create imputed graph\n",
    "    a_imputed, n_nodes = unpad_matrix(np.squeeze(a_imputed[0]), dataArgs[\"diag_value\"], 0.2, dataArgs[\"fix_n\"])\n",
    "    g_imputed = nx.from_numpy_matrix(a_imputed)\n",
    "    \n",
    "    f_imputed = np.reshape(f_imputed, (f_imputed.shape[1]))\n",
    "    f_imputed = f_imputed[:analyzeArgs[\"n\"]]\n",
    "    \n",
    "    print(\"original attributes:\", f_imputed)\n",
    "    \n",
    "    \n",
    "    ## GRAPH DRAWING_____________________________\n",
    "    \n",
    "    ## 1) draw imputed graph\n",
    "    if reconstructed_f.shape[0] > 0:\n",
    "        fixed_cmap = shiftedColorMap(orig_cmap, start=min(f_imputed), midpoint=0.5, stop=max(f_imputed),name='fixed')\n",
    "    else:\n",
    "        fixed_cmap = shiftedColorMap(orig_cmap, start=0.5, midpoint=0.5, stop=0.5, name='fixed')    \n",
    "    nx.draw(g_imputed, node_color=f_imputed, font_color='white', cmap = fixed_cmap)\n",
    "    plt.show()\n",
    "    \n",
    "    ## 2) draw reconstructed graph\n",
    "    if reconstructed_f.shape[0] > 0:\n",
    "        fixed_cmap = shiftedColorMap(orig_cmap, start=min(reconstructed_f), midpoint=0.5, stop=max(reconstructed_f),name='fixed')\n",
    "    else:\n",
    "        fixed_cmap = shiftedColorMap(orig_cmap, start=0.5, midpoint=0.5, stop=0.5, name='fixed') \n",
    "    print(\"reconstructed attributes::\", reconstructed_f)\n",
    "    nx.draw(reconstructed_g, node_color=reconstructed_f, font_color='white', cmap = fixed_cmap)\n",
    "    plt.show()\n",
    "\n",
    "        \n",
    "  ## PLOT RESULTS ________________________________________\n",
    "\n",
    "imputeArgs[\"impute_frac\"] = 1.0\n",
    "analyzeArgs = {\"n\": 10, \"p\": 1.0, \"f\": 0.4, \"normalize_feature\": False}\n",
    "impaint(analyzeArgs, imputeArgs, modelArgs, models, batch_size=trainArgs[\"batch_size\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
