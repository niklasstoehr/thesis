{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Real-World Graph: Wikipedia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "## libs \n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "## Keras\n",
    "from keras.layers import Lambda, Input, Dense, Conv2D, Conv2DTranspose, Flatten, Reshape\n",
    "from keras.models import Model\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.losses import mse, binary_crossentropy\n",
    "from keras.utils import plot_model\n",
    "from keras import backend as K\n",
    "\n",
    "## Basic\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import argparse\n",
    "import os\n",
    "import random\n",
    "import itertools\n",
    "import time\n",
    "\n",
    "# Computation\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mutual_info_score\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "\n",
    "import scipy\n",
    "from scipy.stats.stats import pearsonr \n",
    "\n",
    "## Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "## Network Processing\n",
    "import networkx as nx\n",
    "from networkx.generators import random_graphs\n",
    "\n",
    "## node colour\n",
    "color_map = [\"steelblue\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Supporting Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "## supporting functions\n",
    "from support.preprocessing import sort_adjacency, reshape_A, calculate_A_shape, reconstruct_adjacency, pad_matrix, unpad_matrix, prepare_in_out\n",
    "from support.metrics import compute_mig, compute_mi\n",
    "from support.graph_generating import generate_single, generate_manifold, generate_topol_manifold, generate_topol_manifold\n",
    "from support.latent_space import vis2D, visDistr\n",
    "from support.comparing import compare_manifold_adjacency, compare_topol_manifold\n",
    "\n",
    "## graph sampling\n",
    "from sampling import ForestFire, Metropolis_Hastings, Random_Walk, Snowball, Ties, Base_Samplers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Graph Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "## load graph data\n",
    "e_file = open('data/wiki/wikipedia.edges','rb') \n",
    "g_complete = nx.read_edgelist(e_file, nodetype=int, delimiter = \",\", data=(('weight',int),))\n",
    "#g_complete = nx.convert_node_labels_to_integers(g_complete, first_label=0, ordering='default', label_attribute=None)\n",
    "e_file.close()\n",
    "#g_complete = nx.read_edgelist(e_file)\n",
    "\n",
    "n_complete = len(g_complete)\n",
    "e_complete = len(g_complete.edges())\n",
    "#a_complete = nx.adjacency_matrix(g_complete)\n",
    "#max_degree = max([d for n, d in g_complete.degree()])\n",
    "\n",
    "print(\"number of nodes:\", n_complete)\n",
    "print(\"number of edges:\", e_complete)\n",
    "#print(\"max_degree:\", max_degree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample Subgraph\n",
    "\n",
    "**exact_n:** biased_random_walk, bfs, forestfire, random_walk_induced_graph_sampling, random_walk_sampling_with_fly_back, adjacency, select\n",
    "\n",
    "**approx_n:**  snowball, standard_bfs, walk, jump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     5
    ]
   },
   "outputs": [],
   "source": [
    "sampleArgs = {\"sample\": \"biased_random_walk\", \"jump_bias\": \"random_walk_induced_graph_sampling\", \"n\": 30, \"p\": 20.0, \"q\": 100.0, \"source_starts\": 2, \"source_returns\": 4, \"depth\": 2}\n",
    "\n",
    "##exact_n: forestfire, random_walk_induced_graph_sampling, random_walk_sampling_with_fly_back, adjacency, select\n",
    "##approx_n: snowball, bfs, walk, jump\n",
    "\n",
    "def get_graph(sampleArgs,g_complete,a_complete):\n",
    "    \n",
    "    if sampleArgs[\"sample\"] == \"biased_random_walk\":\n",
    "        sampler = Base_Samplers.Base_Samplers(g_complete,a_complete)\n",
    "        #sampler = Base_Samplers(g_complete,a_complete)\n",
    "        g = sampler.biased_random_walk(sampleArgs[\"n\"], sampleArgs[\"p\"], sampleArgs[\"q\"])\n",
    "\n",
    "    if sampleArgs[\"sample\"] == \"forestfire\":\n",
    "        sampler = ForestFire.ForestFire(g_complete,a_complete)\n",
    "        g = sampler.forestfire(sampleArgs[\"n\"])\n",
    "\n",
    "    if sampleArgs[\"sample\"] == \"snowball\":\n",
    "        sampler = Snowball.Snowball(g_complete,a_complete)\n",
    "        g = sampler.snowball(sampleArgs[\"source_starts\"], sampleArgs[\"source_returns\"])\n",
    "\n",
    "    if sampleArgs[\"sample\"] == \"random_walk_induced_graph_sampling\":\n",
    "        sampler = Random_Walk.Random_Walk(g_complete,a_complete)\n",
    "        g = sampler.random_walk_induced_graph_sampling(sampleArgs[\"n\"])\n",
    "\n",
    "    if sampleArgs[\"sample\"] == \"random_walk_sampling_with_fly_back\":\n",
    "        sampler = Random_Walk.Random_Walk(g_complete,a_complete)\n",
    "        g = sampler.random_walk_sampling_with_fly_back(sampleArgs[\"n\"], sampleArgs[\"p\"])\n",
    "        \n",
    "    if sampleArgs[\"sample\"] == \"standard_bfs\":\n",
    "        sampler = Base_Samplers.Base_Samplers(g_complete,a_complete)\n",
    "        g = sampler.standard_bfs(sampleArgs[\"source_starts\"], sampleArgs[\"depth\"]) \n",
    "        \n",
    "    if sampleArgs[\"sample\"] == \"bfs\":\n",
    "        sampler = Base_Samplers.Base_Samplers(g_complete,a_complete)\n",
    "        g = sampler.bfs(sampleArgs[\"n\"]) \n",
    "        \n",
    "    if sampleArgs[\"sample\"] == \"walk\":\n",
    "        sampler = Base_Samplers.Base_Samplers(g_complete,a_complete)\n",
    "        g = sampler.walk(sampleArgs[\"source_starts\"], sampleArgs[\"source_returns\"], sampleArgs[\"p\"])        \n",
    "        \n",
    "    if sampleArgs[\"sample\"] == \"jump\":\n",
    "        sampler = Base_Samplers.Base_Samplers(g_complete,a_complete)\n",
    "        g = sampler.jump(sampleArgs[\"source_starts\"], sampleArgs[\"p\"], sampleArgs[\"jump_bias\"])\n",
    "        \n",
    "    if sampleArgs[\"sample\"] == \"adjacency\":\n",
    "        sampler = Base_Samplers.Base_Samplers(g_complete,a_complete)\n",
    "        g = sampler.adjacency(sampleArgs[\"n\"]) \n",
    "        \n",
    "    if sampleArgs[\"sample\"] == \"select\":\n",
    "        sampler = Base_Samplers.Base_Samplers(g_complete,a_complete)\n",
    "        g = sampler.adjacency(sampleArgs[\"n\"]) \n",
    "    \n",
    "    return g \n",
    "\n",
    "start_time = time.time()\n",
    "g = get_graph(sampleArgs,g_complete,a_complete)\n",
    "\n",
    "print(\"-- n_max should be >=\", len(g), \"--\")\n",
    "print(\"-- function get_graph takes %s secs --\" % round((time.time() - start_time),  5))\n",
    "\n",
    "if len(g) <= 200:\n",
    "    nx.draw(g, node_color = color_map, with_labels = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "dataArgs = {\"n_graphs\": 10000, \"n_max\": 30, \"n_lower\": 1, \"n_upper\": 30, \"iter_n\": True, \"fix_n\": True, \"diag_offset\": 0, \"diag_value\": 1, \"clip\": True}\n",
    "                                                                        # none, exact_n, approx_n             #\"diag_offset\" - 1, 0, 1\n",
    "\n",
    "def generate_data(dataArgs, sampleArgs, g_complete, a_complete): \n",
    "    \n",
    "    \n",
    "    if dataArgs[\"iter_n\"] == False:\n",
    "    \n",
    "        ## Data ________________________________\n",
    "\n",
    "        G = np.zeros((dataArgs[\"n_graphs\"], *calculate_A_shape(dataArgs[\"n_max\"], diag_offset = dataArgs[\"diag_offset\"])))\n",
    "\n",
    "        ## Ground Truth Labels ______________________________\n",
    "\n",
    "        topol_params = [\"#nodes\", \"#edges\", \"density\", \"diameter\", \"cluster_coef\", \"assort\", \"avg_degree\"]\n",
    "        T = np.zeros((dataArgs[\"n_graphs\"], len(topol_params)))\n",
    "\n",
    "        ## Generate Graph Data_______________________________\n",
    "\n",
    "        for i in tqdm(range(0,dataArgs[\"n_graphs\"])):\n",
    "\n",
    "            ## Generate Graph Type ______________________________________________\n",
    "\n",
    "            g = get_graph(sampleArgs, g_complete, a_complete)\n",
    "\n",
    "            g, a = sort_adjacency(g)\n",
    "            a = pad_matrix(a, dataArgs[\"n_max\"], dataArgs[\"diag_value\"])  # pad adjacency matrix to allow less nodes than n_max and fill diagonal\n",
    "            a_transformed = reshape_A(a, diag_offset = dataArgs[\"diag_offset\"])\n",
    "\n",
    "            ## Generate Ground Truth features____________________________________\n",
    "\n",
    "            nodes = len(g)\n",
    "            edges = g.number_of_edges()\n",
    "\n",
    "            density = nx.density(g)\n",
    "\n",
    "            if nx.is_connected(g):\n",
    "                diameter = nx.diameter(g)\n",
    "            else:\n",
    "                diameter = -1\n",
    "\n",
    "            cluster_coef = nx.average_clustering(g)\n",
    "\n",
    "            if g.number_of_edges() > 0:\n",
    "                assort = nx.degree_assortativity_coefficient(g, x='out', y='in')\n",
    "            else:\n",
    "                assort = 0\n",
    "\n",
    "            avg_degree = sum(i for i in nx.degree_centrality(g).values()) / len(nx.degree_centrality(g).keys())\n",
    "\n",
    "\n",
    "            ## toDO: add more graph topologies\n",
    "\n",
    "            ## Build Data Arrays___________________________________________________\n",
    "\n",
    "            G[i] = a_transformed\n",
    "\n",
    "            T[i,0] = nodes\n",
    "            T[i,1] = edges\n",
    "            T[i,2] = density\n",
    "            T[i,3] = diameter\n",
    "            T[i,4] = cluster_coef\n",
    "            T[i,5] = assort\n",
    "            T[i,6] = avg_degree\n",
    "            \n",
    "            \n",
    "            \n",
    "    elif dataArgs[\"iter_n\"] == True:      ## iterate different n to generate range\n",
    "    \n",
    "        ## Data ________________________________\n",
    "\n",
    "        G = np.zeros((dataArgs[\"n_graphs\"], *calculate_A_shape(dataArgs[\"n_max\"], diag_offset = dataArgs[\"diag_offset\"])))\n",
    "\n",
    "        ## Ground Truth Labels ______________________________\n",
    "\n",
    "        topol_params = [\"#nodes\", \"#edges\", \"density\", \"diameter\", \"cluster_coef\", \"assort\", \"avg_degree\"]\n",
    "        T = np.zeros((dataArgs[\"n_graphs\"], len(topol_params)))\n",
    "\n",
    "        ## Generate Graph Data_______________________________\n",
    "        \n",
    "        # generate N range\n",
    "        if dataArgs[\"n_upper\"] <= dataArgs[\"n_max\"]:\n",
    "            N = np.linspace(dataArgs[\"n_lower\"], dataArgs[\"n_upper\"], dataArgs[\"n_graphs\"], dtype = int)\n",
    "        else:\n",
    "            sys.exit(\"dataArgs[n_upper] > dataArgs[n_max]\")\n",
    "\n",
    "        for i, n in enumerate(tqdm(N)):\n",
    "\n",
    "            ## Generate Graph Type ______________________________________________\n",
    "\n",
    "            sampleArgs[\"random_n\"] = n\n",
    "            sampleArgs[\"source_starts\"] = n\n",
    "            sampleArgs[\"n\"] = n\n",
    "                \n",
    "            g = get_graph(sampleArgs, g_complete, a_complete)\n",
    "\n",
    "            g, a = sort_adjacency(g)\n",
    "            a = pad_matrix(a, dataArgs[\"n_max\"], dataArgs[\"diag_value\"])  # pad adjacency matrix to allow less nodes than n_max and fill diagonal\n",
    "            a_transformed = reshape_A(a, diag_offset = dataArgs[\"diag_offset\"])\n",
    "\n",
    "            ## Generate Ground Truth features____________________________________\n",
    "\n",
    "            nodes = len(g)\n",
    "            edges = g.number_of_edges()\n",
    "\n",
    "            density = nx.density(g)\n",
    "\n",
    "            if nx.is_connected(g):\n",
    "                diameter = nx.diameter(g)\n",
    "            else:\n",
    "                diameter = -1\n",
    "\n",
    "            cluster_coef = nx.average_clustering(g)\n",
    "\n",
    "            if g.number_of_edges() > 0:\n",
    "                assort = nx.degree_assortativity_coefficient(g, x='out', y='in')\n",
    "            else:\n",
    "                assort = 0\n",
    "\n",
    "            avg_degree = sum(i for i in nx.degree_centrality(g).values()) / len(nx.degree_centrality(g).keys())\n",
    "\n",
    "\n",
    "            ## toDO: add more graph topologies\n",
    "\n",
    "            ## Build Data Arrays___________________________________________________\n",
    "\n",
    "            G[i] = a_transformed\n",
    "\n",
    "            T[i,0] = nodes\n",
    "            T[i,1] = edges\n",
    "            T[i,2] = density\n",
    "            T[i,3] = diameter\n",
    "            T[i,4] = cluster_coef\n",
    "            T[i,5] = assort\n",
    "            T[i,6] = avg_degree\n",
    "\n",
    "\n",
    "\n",
    "    ## Input and Output Size ___________________________________________________________\n",
    "\n",
    "    T, input_shape, output_shape = prepare_in_out(T, dataArgs[\"diag_offset\"], calculate_A_shape(dataArgs[\"n_max\"], dataArgs[\"diag_offset\"]))\n",
    "    print(\"input_shape:\", input_shape, \", output_shape:\", output_shape)\n",
    "    \n",
    "    return G,T,topol_params,input_shape,output_shape\n",
    "    \n",
    "G, T, topol_params, input_shape, output_shape = generate_data(dataArgs, sampleArgs, g_complete, a_complete)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# beta-VAE (MLP, Conv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelArgs = {\"nn_architecture\": \"mlp\", \"param_loss\": False, \"latent_dim\": 3, \"filters\": 16, \"kernel_size\": 3, \"input_shape\": input_shape, \"output_shape\": output_shape}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0,
     6,
     68
    ]
   },
   "outputs": [],
   "source": [
    "# network parameters\n",
    "\n",
    "# reparameterization trick\n",
    "# instead of sampling from Q(z|X), sample eps = N(0,I)\n",
    "# then z = z_mean + sqrt(var)*eps\n",
    "\n",
    "def sampling(args):\n",
    "    \"\"\"Reparameterization trick by sampling fr an isotropic unit Gaussian.\n",
    "    # Arguments\n",
    "        args (tensor): mean and log of variance of Q(z|X)\n",
    "    # Returns\n",
    "        z (tensor): sampled latent vector\n",
    "    \"\"\"\n",
    "\n",
    "    z_mean, z_log_var = args\n",
    "    batch = K.shape(z_mean)[0]\n",
    "    dim = K.int_shape(z_mean)[1]\n",
    "    # by default, random_normal has mean=0 and std=1.0\n",
    "    epsilon = K.random_normal(shape=(batch, dim))\n",
    "    return z_mean + K.exp(0.5 * z_log_var) * epsilon\n",
    "\n",
    "\n",
    "\n",
    "## MODEL ______________________________________________________________\n",
    "\n",
    "## Multi-layer Perceptron without convolutions__________________________________\n",
    "\n",
    "if modelArgs[\"nn_architecture\"] == \"mlp\":\n",
    "\n",
    "    ## 1) build encoder model\n",
    "    inputs = Input(shape=modelArgs[\"input_shape\"], name='encoder_input')\n",
    "    x = Dense(128, activation='relu')(inputs)\n",
    "    x = Dense(64, activation='relu')(x)\n",
    "    z_mean = Dense(modelArgs[\"latent_dim\"], name='z_mean')(x)\n",
    "    z_log_var = Dense(modelArgs[\"latent_dim\"], name='z_log_var')(x)\n",
    "\n",
    "    ## 2) build decoder model\n",
    "    latent_inputs = Input(shape=(modelArgs[\"latent_dim\"],), name='z_sampling')\n",
    "    y = Dense(64, activation='relu')(latent_inputs)\n",
    "    y = Dense(128, activation='relu')(y)\n",
    "    outputs = Dense(modelArgs[\"output_shape\"], activation='sigmoid')(y)\n",
    "\n",
    "    # use reparameterization trick to push the sampling out as input\n",
    "    # note that \"output_shape\" isn't necessary with the TensorFlow backend\n",
    "    z = Lambda(sampling, output_shape=(modelArgs[\"latent_dim\"],), name='z')([z_mean, z_log_var])\n",
    "\n",
    "\n",
    "    ## INSTANTIATE ________________________________________________\n",
    "\n",
    "    ## 1) instantiate encoder model\n",
    "    encoder = Model(inputs, [z_mean, z_log_var, z], name='encoder')\n",
    "    encoder.summary()\n",
    "    #plot_model(encoder, to_file='vae_mlp_encoder.png', show_shapes=True)\n",
    "\n",
    "    ## 2) instantiate decoder model\n",
    "    decoder = Model(latent_inputs, outputs, name='decoder')\n",
    "    decoder.summary()\n",
    "    #plot_model(decoder, to_file='vae_mlp_decoder.png', show_shapes=True)\n",
    "\n",
    "    ## 3) instantiate VAE model\n",
    "    outputs = decoder(encoder(inputs)[2])\n",
    "    vae = Model(inputs, outputs, name='vae_graph')\n",
    "    #vae.summary()\n",
    "\n",
    "\n",
    "\n",
    "## Convolutional Neural Network_________________________________\n",
    "\n",
    "if modelArgs[\"nn_architecture\"] == \"2D_conv\":\n",
    "\n",
    "\n",
    "    ## 1) build encoder model____________________________________\n",
    "\n",
    "    inputs = Input(shape=modelArgs[\"input_shape\"], name='encoder_input')\n",
    "    x = inputs\n",
    "\n",
    "    for i in range(2):\n",
    "        modelArgs['filters'] *= 2\n",
    "        x = Conv2D(filters=modelArgs['filters'], kernel_size=modelArgs['kernel_size'], activation='relu', strides=2, padding='same')(x)\n",
    "\n",
    "    # shape info needed to build decoder model\n",
    "    shape = K.int_shape(x)\n",
    "\n",
    "    # generate latent vector Q(z|X)\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(16, activation='relu')(x)\n",
    "    z_mean = Dense(modelArgs[\"latent_dim\"], name='z_mean')(x)\n",
    "    z_log_var = Dense(modelArgs[\"latent_dim\"], name='z_log_var')(x)\n",
    "\n",
    "\n",
    "\n",
    "    ## 2) build decoder model____________________________________\n",
    "\n",
    "    latent_inputs = Input(shape=(modelArgs[\"latent_dim\"],), name='z_sampling')\n",
    "    x = Dense(shape[1] * shape[2] * shape[3], activation='relu')(latent_inputs)\n",
    "    x = Reshape((shape[1], shape[2], shape[3]))(x)\n",
    "\n",
    "    for i in range(2):\n",
    "        x = Conv2DTranspose(filters=modelArgs['filters'], kernel_size=modelArgs['kernel_size'], activation='relu', strides=2, padding='same')(x)\n",
    "        modelArgs['filters'] //= 2\n",
    "\n",
    "    outputs = Conv2DTranspose(filters=1, kernel_size=modelArgs['kernel_size'], activation='sigmoid', padding='same', name='decoder_output')(x)\n",
    "\n",
    "    # use reparameterization trick to push the sampling out as input\n",
    "    # note that \"output_shape\" isn't necessary with the TensorFlow backend\n",
    "    z = Lambda(sampling, output_shape=(modelArgs[\"output_shape\"],), name='z')([z_mean, z_log_var])\n",
    "\n",
    "\n",
    "\n",
    "    ## INSTANTIATE___________________________________\n",
    "\n",
    "    ## 1) instantiate encoder model    \n",
    "    encoder = Model(inputs, [z_mean, z_log_var, z], name='encoder')\n",
    "    encoder.summary()\n",
    "    #plot_model(encoder, to_file='vae_cnn_encoder.png', show_shapes=True)\n",
    "\n",
    "\n",
    "    ## 2) instantiate decoder model\n",
    "    decoder = Model(latent_inputs, outputs, name='decoder')\n",
    "    decoder.summary()\n",
    "    #plot_model(decoder, to_file='vae_cnn_decoder.png', show_shapes=True)\n",
    "\n",
    "\n",
    "    ## 3) instantiate VAE model\n",
    "    outputs = decoder(encoder(inputs)[2])\n",
    "    vae = Model(inputs, outputs, name='conv_vae')\n",
    "    #vae.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainArgs = {\"beta\": 3, \"loss\": \"binary_crossentropy\", \"weights\": \"train\", \"early_stop\": 1, \"batch_size\": 4, \"epochs\": 12, \"data_split\": 0.1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0,
     13,
     23,
     25,
     29,
     52,
     58
    ]
   },
   "outputs": [],
   "source": [
    "## MLP: beta =\n",
    "## CNN: beta = (latent 2 / beta 25)\n",
    "\n",
    "## Train and Validation Split _______________________________________________\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(G, T, test_size= trainArgs[\"data_split\"], random_state=1, shuffle=True)\n",
    "\n",
    "models = (encoder, decoder)\n",
    "data = (x_test, y_test)\n",
    "\n",
    "\n",
    "## VAE loss = mse_loss or xent_loss + kl_loss_______________________\n",
    "\n",
    "if trainArgs[\"loss\"] == \"mse\":\n",
    "    \n",
    "    if modelArgs[\"nn_architecture\"] == \"mlp\":\n",
    "        reconstruction_loss = mse(inputs, outputs)\n",
    "        reconstruction_loss *= modelArgs[\"input_shape\"]\n",
    "        \n",
    "    if modelArgs[\"nn_architecture\"] == \"2D_conv\":\n",
    "        reconstruction_loss = mse(K.flatten(inputs), K.flatten(outputs))\n",
    "        reconstruction_loss *= modelArgs[\"input_shape\"][0] * modelArgs[\"input_shape\"][1]\n",
    "        \n",
    "if trainArgs[\"loss\"] == \"binary_crossentropy\":\n",
    "    \n",
    "    if modelArgs[\"nn_architecture\"] == \"mlp\":\n",
    "        reconstruction_loss = binary_crossentropy(inputs, outputs)\n",
    "        reconstruction_loss *= modelArgs[\"input_shape\"]\n",
    "        \n",
    "    if modelArgs[\"nn_architecture\"] == \"2D_conv\":\n",
    "        reconstruction_loss = binary_crossentropy(K.flatten(inputs),K.flatten(outputs))\n",
    "        reconstruction_loss *= modelArgs[\"input_shape\"][0] * modelArgs[\"input_shape\"][1]\n",
    "\n",
    "\n",
    "\n",
    "## LOSS _____________________________________________\n",
    "\n",
    "kl_loss = 1 + z_log_var - K.square(z_mean) - K.exp(z_log_var)\n",
    "kl_loss = K.sum(kl_loss, axis=-1)\n",
    "kl_loss *= -0.5\n",
    "vae_loss = K.mean(reconstruction_loss + (trainArgs[\"beta\"] * kl_loss))\n",
    "vae.add_loss(vae_loss)\n",
    "vae.compile(optimizer='adam', metrics=['accuracy'])\n",
    "vae.summary()\n",
    "#plot_model(vae,to_file='vae_mlp.png',show_shapes=True)\n",
    "\n",
    "\n",
    "\n",
    "## TRAIN______________________________________________\n",
    "\n",
    "# load the autoencoder weights\n",
    "\n",
    "if trainArgs[\"weights\"] == \"load\":\n",
    "    \n",
    "    vae.load_weights(\"models/weights/vae_mlp_mnist_latent_dim_\" + str(modelArgs[\"latent_dim\"]) + \".h5\")\n",
    "\n",
    "# train the autoencoder\n",
    "\n",
    "elif trainArgs[\"weights\"] == \"train\":\n",
    "    \n",
    "    # Set callback functions to early stop training and save the best model so far\n",
    "    callbacks = [EarlyStopping(monitor='val_loss', patience=trainArgs[\"early_stop\"]), ModelCheckpoint(filepath=\"models/weights/vae_mlp_mnist_latent_dim_\" + str(modelArgs[\"latent_dim\"]) + \".h5\", save_best_only=True)]\n",
    "    \n",
    "    vae.fit(x_train, epochs=trainArgs[\"epochs\"], batch_size=trainArgs[\"batch_size\"], callbacks=callbacks, validation_data=(x_test, None))\n",
    "    vae.save_weights(\"models/weights/vae_mlp_mnist_latent_dim_\" + str(modelArgs[\"latent_dim\"]) + \".h5\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Latent Space Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iterate through single data dimension and oberseve single latent space dimension  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     2
    ]
   },
   "outputs": [],
   "source": [
    "analyzeArgs = {\"n_graphs\": 1000, \"metric\": \"mig\"}\n",
    "\n",
    "def latent_space_feature_correlation(analyzeArgs, modelArgs, models, data, topol_params, batch_size=128,model_name=\"vae_graph\"):\n",
    "\n",
    "    encoder, decoder = models  # trained models\n",
    "    x_test, y_test = data      # data\n",
    "                \n",
    "    G = x_test[:analyzeArgs[\"n_graphs\"],:]\n",
    "    T = y_test[:analyzeArgs[\"n_graphs\"]]\n",
    "    \n",
    "    ## ENCODER - 2D Digit Classes ______________________________________________\n",
    "\n",
    "    # display a 2D plot of the digit classes in the latent space\n",
    "    z_mean, _, _ = encoder.predict(G, batch_size = batch_size)\n",
    "\n",
    "\n",
    "    ## Graph Topologies________________________________________________________\n",
    "    \n",
    "    ## Visualize Latent Variables x Topological Parameters \n",
    "    \n",
    "    if z_mean.shape[-1] == 1:  # 1 latent variable\n",
    "\n",
    "        fig, ax = plt.subplots(nrows= z_mean.shape[1] , ncols= len(topol_params), figsize=(20,10))\n",
    "        \n",
    "        for latent_z, row in enumerate(ax):        \n",
    "\n",
    "                ## toDO: change sorting\n",
    "                y = z_mean[:,0]\n",
    "                x = T[:,latent_z]\n",
    "                sns.regplot(x, y, color=\"steelblue\", ax=row)\n",
    "\n",
    "\n",
    "                ## compute correlation and standardized covariance\n",
    "                corr = round(pearsonr(x,y)[0],3)\n",
    "                cov = round(np.cov(x, y)[0][1]/max(x),3)\n",
    "                row.annotate(\"corr:\"+str(corr)+\", cov:\"+str(cov), xy=(0, 1), xytext=(12, -12), va='top',xycoords='axes fraction', textcoords='offset points')\n",
    "\n",
    "\n",
    "\n",
    "        ## add row and column titles _____________________\n",
    "\n",
    "        cols = [t for t in topol_params]\n",
    "\n",
    "        for axis, col in zip(ax[:], cols):\n",
    "            axis.set_title(col, fontweight='bold')\n",
    "\n",
    "\n",
    "\n",
    "            \n",
    "    ## Visualize Latent Variables x Topological Parameters \n",
    "    \n",
    "    if z_mean.shape[-1] >= 2:  # 1 latent variable   \n",
    "\n",
    "        fig, ax = plt.subplots(nrows= z_mean.shape[-1] , ncols= len(topol_params), figsize=(30,10))\n",
    "\n",
    "        for latent_z, row in enumerate(ax):        \n",
    "            for feature, col in enumerate(row):\n",
    "\n",
    "                ## toDO: change sorting\n",
    "                y = z_mean[:,latent_z]\n",
    "                x = T[:,feature]\n",
    "                sns.regplot(x, y, color=\"steelblue\", ax=col)\n",
    "\n",
    "\n",
    "                ## compute correlation and standardized covariance\n",
    "                corr = round(pearsonr(x,y)[0],3)\n",
    "                cov = round(np.cov(x, y)[0][1]/max(x),3)\n",
    "                col.annotate(\"corr:\"+str(corr)+\", cov:\"+str(cov), xy=(0, 1), xytext=(12, -12), va='top',xycoords='axes fraction', textcoords='offset points')\n",
    "\n",
    "\n",
    "\n",
    "        ## add row and column titles _____________________\n",
    "\n",
    "        rows = ['z_{}'.format(row) for row in range(z_mean.shape[-1])]\n",
    "        cols = [t for t in topol_params]\n",
    "\n",
    "        for axis, col in zip(ax[0], cols):\n",
    "            axis.set_title(col, fontweight='bold')\n",
    "\n",
    "        for axis, row in zip(ax[:,0], rows):\n",
    "            axis.set_ylabel(row, rotation=0, size='large', fontweight='bold')\n",
    "\n",
    "        \n",
    "\n",
    "## PLOT RESULTS ________________________________________\n",
    "\n",
    "latent_space_feature_correlation(analyzeArgs, modelArgs, models, data, topol_params, batch_size=trainArgs[\"batch_size\"], model_name=\"vae_mlp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize the Latent Space in 2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "analyzeArgs = {\"save_plots\": False}\n",
    "vis2D(analyzeArgs, modelArgs, models, data, batch_size=trainArgs[\"batch_size\"], model_name=\"vae_mlp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Latent Generative Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "analyzeArgs = {\"z\": [0,1]}\n",
    "visDistr(modelArgs, analyzeArgs, models,data,trainArgs[\"batch_size\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate a Single Graph Instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "analyzeArgs = {\"activations\": [+20,+20], \"z\": [0,1]}\n",
    "generate_single(analyzeArgs, modelArgs, dataArgs, models, color_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Interpolated Manifold from Latent Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     8,
     303
    ]
   },
   "outputs": [],
   "source": [
    "from support.preprocessing import reconstruct_adjacency, unpad_matrix\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "from matplotlib import pylab as plt\n",
    "import os\n",
    "\n",
    "## apply decoder and generate data\n",
    "\n",
    "def generate_single(analyzeArgs, modelArgs, dataArgs, models, color_map):\n",
    "\n",
    "    if modelArgs[\"param_loss\"]:\n",
    "        encoder, graph_decoder, param_decoder = models  # trained models\n",
    "    else:\n",
    "        encoder, graph_decoder = models  # trained models\n",
    "\n",
    "    print(\"latent dimensions:\", modelArgs[\"latent_dim\"])\n",
    "\n",
    "    z_sample = np.zeros(modelArgs[\"latent_dim\"])\n",
    "    z_sample = np.reshape(z_sample, (1, modelArgs[\"latent_dim\"]))\n",
    "\n",
    "    for i, dim in enumerate(analyzeArgs[\"z\"]):\n",
    "        z_sample[0][dim] = analyzeArgs[\"activations\"][i]\n",
    "\n",
    "    x_decoded = graph_decoder.predict(z_sample)\n",
    "\n",
    "    ## reconstruct upper triangular adjacency matrix\n",
    "    reconstructed_a = reconstruct_adjacency(x_decoded, dataArgs[\"clip\"], dataArgs[\"diag_offset\"])\n",
    "    reconstructed_a = unpad_matrix(reconstructed_a, dataArgs[\"diag_value\"], dataArgs[\"fix_n\"])\n",
    "\n",
    "    ## reconstruct graph\n",
    "    g = nx.from_numpy_matrix(reconstructed_a)\n",
    "    # reconstructed_a = nx.adjacency_matrix(g).todense()\n",
    "\n",
    "    nx.draw(g, node_color=color_map)\n",
    "\n",
    "\n",
    "## DECODER - Latent Space Interpolation____________________________\n",
    "\n",
    "def generate_manifold(analyzeArgs, modelArgs, dataArgs, models, data, color_map, batch_size=128):\n",
    "    print(\"latent dimensions:\", modelArgs[\"latent_dim\"])\n",
    "\n",
    "    if modelArgs[\"param_loss\"]:\n",
    "        encoder, graph_decoder, param_decoder = models  # trained models\n",
    "    else:\n",
    "        encoder, graph_decoder = models  # trained models\n",
    "\n",
    "    x_test, y_test = data\n",
    "\n",
    "    # display a 2D plot of the digit classes in the latent space\n",
    "    z_mean, z_log_var, z = encoder.predict(x_test, batch_size)\n",
    "\n",
    "    ## store reconstructed adjacency matrices for later\n",
    "    if modelArgs[\"latent_dim\"] <= 1:\n",
    "        Reconstr_a = np.zeros((analyzeArgs[\"size_of_manifold\"], dataArgs[\"n_max\"], dataArgs[\"n_max\"]))\n",
    "    else:\n",
    "        Reconstr_a = np.zeros(\n",
    "            (analyzeArgs[\"size_of_manifold\"], analyzeArgs[\"size_of_manifold\"], dataArgs[\"n_max\"], dataArgs[\"n_max\"]))\n",
    "\n",
    "    ## Latent Space Dimension is 1 ______________________\n",
    "\n",
    "    if modelArgs[\"latent_dim\"] == 1:\n",
    "\n",
    "        ## 1) create adjacency plots__________________________________________\n",
    "\n",
    "        # display a 30x30 2D manifold of digits\n",
    "        n = dataArgs[\"n_max\"]  # number of nodes\n",
    "        figure = np.zeros((1 * n, analyzeArgs[\"size_of_manifold\"] * n))\n",
    "\n",
    "        # linearly spaced coordinates corresponding to the 2D plot\n",
    "        # of digit classes in the latent space\n",
    "        if analyzeArgs[\"sample\"] == \"z\":\n",
    "            grid_x = np.sort(np.random.normal(np.mean(z_mean[:, analyzeArgs[\"z\"][0]]),\n",
    "                                              np.mean(np.exp(z_log_var[:, analyzeArgs[\"z\"][0]])),\n",
    "                                              analyzeArgs[\"size_of_manifold\"]))\n",
    "        elif analyzeArgs[\"sample\"] == \"range\":\n",
    "            grid_x = np.linspace(analyzeArgs[\"act_range\"][0], analyzeArgs[\"act_range\"][1],\n",
    "                                 analyzeArgs[\"size_of_manifold\"])\n",
    "        elif analyzeArgs[\"sample\"] == \"normal\":\n",
    "            grid_x = np.sort(\n",
    "                np.random.normal(np.mean(z_mean[:, analyzeArgs[\"z\"][0]]), 1, analyzeArgs[\"size_of_manifold\"]))\n",
    "\n",
    "        z_sample = np.zeros(modelArgs[\"latent_dim\"])\n",
    "        z_sample = np.reshape(z_sample, (1, modelArgs[\"latent_dim\"]))\n",
    "\n",
    "        ## 2) create graph plots_______________________________________________\n",
    "\n",
    "        fig, axs = plt.subplots(1, analyzeArgs[\"size_of_manifold\"], figsize=(10, 10))\n",
    "        # fig.subplots_adjust(hspace = .5, wspace=.001)\n",
    "        axs = axs.ravel()\n",
    "\n",
    "        for j, xi in enumerate(grid_x):\n",
    "            z_sample[0][0] = xi ** analyzeArgs[\"act_scale\"]\n",
    "            x_decoded = graph_decoder.predict(z_sample)\n",
    "\n",
    "            ## reconstruct upper triangular adjacency matrix\n",
    "            reconstructed_a = reconstruct_adjacency(x_decoded, dataArgs[\"clip\"], dataArgs[\"diag_offset\"])\n",
    "            Reconstr_a[j] = reconstructed_a\n",
    "\n",
    "            ## 1) create adjacency plot_____________________________\n",
    "\n",
    "            figure[0:n, j * n: (j + 1) * n] = reconstructed_a\n",
    "\n",
    "            ## 2) create graph plot_____________________________\n",
    "\n",
    "            # reconstruct graph\n",
    "            reconstructed_a = unpad_matrix(reconstructed_a, dataArgs[\"diag_value\"], dataArgs[\"fix_n\"])\n",
    "            g = nx.from_numpy_matrix(reconstructed_a)\n",
    "\n",
    "            # compute index for the subplot, and set this subplot as current\n",
    "            jx = np.unravel_index(j, axs.shape)\n",
    "            plt.sca(axs[jx])\n",
    "\n",
    "            nx.draw(g, node_size=10, node_color=color_map)\n",
    "            axs[jx].set_axis_off()\n",
    "            axs[jx].set(ylabel='z_0')\n",
    "\n",
    "        start_range = n // 2\n",
    "        end_range = (analyzeArgs[\"size_of_manifold\"] - 1) * n + start_range + 1\n",
    "        pixel_range = np.arange(start_range, end_range, n)\n",
    "        sample_range_x = np.round(grid_x, 1)\n",
    "\n",
    "        # Plot_____________________________\n",
    "\n",
    "        plt.figure(figsize=(15, 300))\n",
    "        plt.xticks(pixel_range, sample_range_x)\n",
    "        plt.xlabel(\"z_0\", fontweight='bold')\n",
    "        plt.imshow(figure, cmap='Greys_r')\n",
    "        plt.show()\n",
    "\n",
    "        if analyzeArgs[\"save_plots\"] == True:\n",
    "            filename = os.path.join(model_name, \"digits_over_latent.png\")\n",
    "            plt.savefig(filename)\n",
    "\n",
    "    ## Latent Space Dimension is 2 ______________________\n",
    "\n",
    "    if modelArgs[\"latent_dim\"] == 2:\n",
    "\n",
    "        ## 1) create adjacency plots_______________________________________________\n",
    "\n",
    "        # display a 30x30 2D manifold of digits\n",
    "        n = dataArgs[\"n_max\"]  # number of nodes\n",
    "        figure = np.zeros((analyzeArgs[\"size_of_manifold\"] * n, analyzeArgs[\"size_of_manifold\"] * n))\n",
    "\n",
    "        # linearly spaced coordinates corresponding to the 2D plot\n",
    "        # of digit classes in the latent space\n",
    "        if analyzeArgs[\"sample\"] == \"z\":\n",
    "            grid_x = np.sort(np.random.normal(np.mean(z_mean[:, analyzeArgs[\"z\"][0]]),\n",
    "                                              np.mean(np.exp(z_log_var[:, analyzeArgs[\"z\"][0]])),\n",
    "                                              analyzeArgs[\"size_of_manifold\"]))\n",
    "            grid_y = np.sort(np.random.normal(np.mean(z_mean[:, analyzeArgs[\"z\"][1]]),\n",
    "                                              np.mean(np.exp(z_log_var[:, analyzeArgs[\"z\"][1]])),\n",
    "                                              analyzeArgs[\"size_of_manifold\"]))\n",
    "        elif analyzeArgs[\"sample\"] == \"range\":\n",
    "            grid_x = np.linspace(analyzeArgs[\"act_range\"][0], analyzeArgs[\"act_range\"][1],\n",
    "                                 analyzeArgs[\"size_of_manifold\"])\n",
    "            grid_y = np.linspace(analyzeArgs[\"act_range\"][0], analyzeArgs[\"act_range\"][1],\n",
    "                                 analyzeArgs[\"size_of_manifold\"])[::-1]  ## revert\n",
    "        elif analyzeArgs[\"sample\"] == \"normal\":\n",
    "            grid_x = np.sort(\n",
    "                np.random.normal(np.mean(z_mean[:, analyzeArgs[\"z\"][0]]), 1, analyzeArgs[\"size_of_manifold\"]))\n",
    "            grid_y = np.sort(\n",
    "                np.random.normal(np.mean(z_mean[:, analyzeArgs[\"z\"][1]]), 1, analyzeArgs[\"size_of_manifold\"]))\n",
    "\n",
    "        ## 2) create graph plots_______________________________________________\n",
    "\n",
    "        fig, axs = plt.subplots(analyzeArgs[\"size_of_manifold\"], analyzeArgs[\"size_of_manifold\"], figsize=(8, 8), dpi = 300)\n",
    "        # fig.subplots_adjust(hspace = .5, wspace=.001)\n",
    "\n",
    "        for i, yi in enumerate(grid_y):\n",
    "            for j, xi in enumerate(grid_x):\n",
    "                xi_value = xi ** analyzeArgs[\"act_scale\"]\n",
    "                yi_value = yi ** analyzeArgs[\"act_scale\"]\n",
    "\n",
    "                z_sample = np.array([[xi_value, yi_value]])\n",
    "                x_decoded = graph_decoder.predict(z_sample)\n",
    "\n",
    "                ## reconstruct upper triangular adjacency matrix\n",
    "                reconstructed_a = reconstruct_adjacency(x_decoded, dataArgs[\"clip\"], dataArgs[\"diag_offset\"])\n",
    "                Reconstr_a[i, j] = reconstructed_a\n",
    "\n",
    "                ## 1) create adjacency plots_____________________________________\n",
    "\n",
    "                figure[i * n: (i + 1) * n,\n",
    "                j * n: (j + 1) * n] = reconstructed_a\n",
    "\n",
    "                ## 2) create graph plot_____________________________\n",
    "\n",
    "                ## reconstruct graph\n",
    "                reconstructed_a = unpad_matrix(reconstructed_a, dataArgs[\"diag_value\"], dataArgs[\"fix_n\"])\n",
    "                g = nx.from_numpy_matrix(reconstructed_a)\n",
    "\n",
    "                # compute index for the subplot, and set this subplot as current\n",
    "                plt.sca(axs[i, j])\n",
    "                nx.draw_kamada_kawai(g, node_size=10, node_color=color_map)\n",
    "                axs[i, j].set_axis_off()\n",
    "\n",
    "        start_range = n // 2\n",
    "        end_range = (analyzeArgs[\"size_of_manifold\"] - 1) * n + start_range + 1\n",
    "        pixel_range = np.arange(start_range, end_range, n)\n",
    "        sample_range_x = np.round(grid_x, 1)\n",
    "        sample_range_y = np.round(grid_y, 1)\n",
    "\n",
    "        # Plot_____________________________\n",
    "\n",
    "        plt.figure(figsize=(10, 10), dpi = 300)\n",
    "        plt.xticks(pixel_range, sample_range_x)\n",
    "        plt.yticks(pixel_range, sample_range_y)\n",
    "        plt.xlabel(\"z_0\", fontweight='bold')\n",
    "        plt.ylabel(\"z_1\", fontweight='bold')\n",
    "        plt.imshow(figure, cmap='Greys_r')\n",
    "        plt.show()\n",
    "\n",
    "        if analyzeArgs[\"save_plots\"] == True:\n",
    "            filename = os.path.join(model_name, \"digits_over_latent.png\")\n",
    "            plt.savefig(filename)\n",
    "\n",
    "    ## Latent Space Dimension is larger than 2 ______________________\n",
    "\n",
    "    if modelArgs[\"latent_dim\"] > 2:\n",
    "\n",
    "        z_sample = np.zeros(modelArgs[\"latent_dim\"])\n",
    "        z_sample = np.reshape(z_sample, (1, modelArgs[\"latent_dim\"]))\n",
    "\n",
    "        ## fill unobserved dimensions with mean of latent variable dimension\n",
    "        for dim in range(0, len(z_sample[0])):\n",
    "            z_sample[0][dim] = np.mean(z_mean[:, dim])\n",
    "\n",
    "        ## 1) create adjacency plots_______________________________________________\n",
    "\n",
    "        # display a 30x30 2D manifold of digits\n",
    "        n = dataArgs[\"n_max\"]  # number of nodes\n",
    "        figure = np.zeros((analyzeArgs[\"size_of_manifold\"] * n, analyzeArgs[\"size_of_manifold\"] * n))\n",
    "\n",
    "        # linearly spaced coordinates corresponding to the 2D plot\n",
    "        # of digit classes in the latent space\n",
    "        if analyzeArgs[\"sample\"] == \"z\":\n",
    "            grid_x = np.sort(np.random.normal(np.mean(z_mean[:, analyzeArgs[\"z\"][0]]),\n",
    "                                              np.mean(np.square(np.exp(z_log_var[:, analyzeArgs[\"z\"][0]]))),\n",
    "                                              analyzeArgs[\"size_of_manifold\"]))\n",
    "            grid_y = np.sort(np.random.normal(np.mean(z_mean[:, analyzeArgs[\"z\"][1]]),\n",
    "                                              np.mean(np.square(np.exp(z_log_var[:, analyzeArgs[\"z\"][1]]))),\n",
    "                                              analyzeArgs[\"size_of_manifold\"]))\n",
    "        elif analyzeArgs[\"sample\"] == \"range\":\n",
    "            grid_x = np.linspace(analyzeArgs[\"act_range\"][0], analyzeArgs[\"act_range\"][1],\n",
    "                                 analyzeArgs[\"size_of_manifold\"])\n",
    "            grid_y = np.linspace(analyzeArgs[\"act_range\"][0], analyzeArgs[\"act_range\"][1],\n",
    "                                 analyzeArgs[\"size_of_manifold\"])[::-1]  ## revert\n",
    "        elif analyzeArgs[\"sample\"] == \"normal\":\n",
    "            grid_x = np.sort(\n",
    "                np.random.normal(np.mean(z_mean[:, analyzeArgs[\"z\"][0]]), 1, analyzeArgs[\"size_of_manifold\"]))\n",
    "            grid_y = np.sort(\n",
    "                np.random.normal(np.mean(z_mean[:, analyzeArgs[\"z\"][1]]), 1, analyzeArgs[\"size_of_manifold\"]))\n",
    "\n",
    "        ## 2) create graph plots_______________________________________________\n",
    "\n",
    "        fig, axs = plt.subplots(analyzeArgs[\"size_of_manifold\"], analyzeArgs[\"size_of_manifold\"], figsize=(10, 10), dpi = 300)\n",
    "        # fig.subplots_adjust(hspace = .5, wspace=.001)\n",
    "\n",
    "        for i, yi in enumerate(grid_y):\n",
    "            for j, xi in enumerate(grid_x):\n",
    "                z_sample[0][analyzeArgs[\"z\"][0]] = xi ** analyzeArgs[\"act_scale\"]\n",
    "                z_sample[0][analyzeArgs[\"z\"][1]] = yi ** analyzeArgs[\"act_scale\"]\n",
    "                x_decoded = graph_decoder.predict(z_sample)\n",
    "\n",
    "                ## reconstruct upper triangular adjacency matrix\n",
    "                reconstructed_a = reconstruct_adjacency(x_decoded, dataArgs[\"clip\"], dataArgs[\"diag_offset\"])\n",
    "                Reconstr_a[i, j] = reconstructed_a\n",
    "\n",
    "                ## 1) create adjacency plot_____________________________\n",
    "\n",
    "                figure[i * n: (i + 1) * n,\n",
    "                j * n: (j + 1) * n] = reconstructed_a\n",
    "\n",
    "                ## 2) create graph plot_____________________________\n",
    "\n",
    "                ## reconstruct graph\n",
    "                reconstructed_a = unpad_matrix(reconstructed_a, dataArgs[\"diag_value\"], dataArgs[\"fix_n\"])\n",
    "                g = nx.from_numpy_matrix(reconstructed_a)\n",
    "\n",
    "                # compute index for the subplot, and set this subplot as current\n",
    "                plt.sca(axs[i, j])\n",
    "                nx.draw_kamada_kawai(g, node_size=10, node_color=color_map)\n",
    "                axs[i, j].set_axis_off()\n",
    "\n",
    "        start_range = n // 2\n",
    "        end_range = (analyzeArgs[\"size_of_manifold\"] - 1) * n + start_range + 1\n",
    "        pixel_range = np.arange(start_range, end_range, n)\n",
    "        sample_range_x = np.round(grid_x, 1)\n",
    "        sample_range_y = np.round(grid_y, 1)\n",
    "\n",
    "        # Plot_____________________________\n",
    "\n",
    "        plt.figure(figsize=(10, 10), dpi = 300)\n",
    "        plt.xticks(pixel_range, sample_range_x)\n",
    "        plt.yticks(pixel_range, sample_range_y)\n",
    "        plt.xlabel(\"z_\" + str(analyzeArgs[\"z\"][0]), fontweight='bold')\n",
    "        plt.ylabel(\"z_\" + str(analyzeArgs[\"z\"][1]), fontweight='bold')\n",
    "        plt.imshow(figure, cmap='Greys_r')\n",
    "        plt.show()\n",
    "\n",
    "    return Reconstr_a\n",
    "\n",
    "\n",
    "def generate_topol_manifold(Reconstr_a, analyzeArgs, modelArgs, dataArgs, models, data, color_map, batch_size):\n",
    "    print(\"latent dimensions:\", modelArgs[\"latent_dim\"])\n",
    "\n",
    "    if modelArgs[\"param_loss\"]:\n",
    "        encoder, graph_decoder, param_decoder = models  # trained models\n",
    "    else:\n",
    "        encoder, graph_decoder = models  # trained models\n",
    "\n",
    "    x_test, y_test = data\n",
    "\n",
    "    # display a 2D plot of the digit classes in the latent space\n",
    "    z_mean, z_log_var, z = encoder.predict(x_test, batch_size)\n",
    "\n",
    "    ## Latent Space Dimension is 1 ______________________\n",
    "\n",
    "    if modelArgs[\"latent_dim\"] == 1:\n",
    "\n",
    "        # linearly spaced coordinates corresponding to the 2D plot\n",
    "        # of digit classes in the latent space\n",
    "        if analyzeArgs[\"sample\"] == \"z\":\n",
    "            grid_x = np.sort(np.random.normal(np.mean(z_mean[:, analyzeArgs[\"z\"][0]]),\n",
    "                                              np.mean(np.exp(z_log_var[:, analyzeArgs[\"z\"][0]])),\n",
    "                                              analyzeArgs[\"size_of_manifold\"]))\n",
    "        elif analyzeArgs[\"sample\"] == \"range\":\n",
    "            grid_x = np.linspace(analyzeArgs[\"act_range\"][0], analyzeArgs[\"act_range\"][1],\n",
    "                                 analyzeArgs[\"size_of_manifold\"])\n",
    "        elif analyzeArgs[\"sample\"] == \"normal\":\n",
    "            grid_x = np.sort(\n",
    "                np.random.normal(np.mean(z_mean[:, analyzeArgs[\"z\"][0]]), 1, analyzeArgs[\"size_of_manifold\"]))\n",
    "\n",
    "        z_sample = np.zeros(modelArgs[\"latent_dim\"])\n",
    "        z_sample = np.reshape(z_sample, (1, modelArgs[\"latent_dim\"]))\n",
    "\n",
    "        ## 1) create graph topol plots_______________________________________________\n",
    "\n",
    "        fig, axs = plt.subplots(1, analyzeArgs[\"size_of_manifold\"], figsize=(10, 10))\n",
    "        # fig.subplots_adjust(hspace = .5, wspace=.001)\n",
    "        axs = axs.ravel()\n",
    "\n",
    "        for j, xi in enumerate(grid_x):\n",
    "\n",
    "            z_sample[0][0] = xi ** analyzeArgs[\"act_scale\"]\n",
    "            x_decoded = graph_decoder.predict(z_sample)\n",
    "\n",
    "            ## reconstruct upper triangular adjacency matrix\n",
    "            reconstructed_a = reconstruct_adjacency(x_decoded, dataArgs[\"clip\"], dataArgs[\"diag_offset\"])\n",
    "\n",
    "            # reconstruct graph\n",
    "            reconstructed_a = unpad_matrix(reconstructed_a, dataArgs[\"diag_value\"], dataArgs[\"fix_n\"])\n",
    "            reconstructed_a = Reconstr_a[j]\n",
    "            g = nx.from_numpy_matrix(reconstructed_a)\n",
    "\n",
    "            ## Obtain Graph Topologies____________________________________\n",
    "\n",
    "            density = nx.density(g)\n",
    "\n",
    "            if len(g) > 0:\n",
    "                if nx.is_connected(g):\n",
    "                    diameter = nx.diameter(g)\n",
    "            else:\n",
    "                diameter = -1\n",
    "\n",
    "            if len(g) > 0:\n",
    "                cluster_coef = nx.average_clustering(g)\n",
    "            else:\n",
    "                cluster_coef = 0\n",
    "\n",
    "            if len(g) > 0:\n",
    "                if g.number_of_edges() > 0:\n",
    "                    assort = nx.degree_assortativity_coefficient(g, x='out', y='in')\n",
    "            else:\n",
    "                assort = 0\n",
    "\n",
    "            if len(g) > 0:\n",
    "                edges = g.number_of_edges()\n",
    "            else:\n",
    "                edges = 0\n",
    "\n",
    "            if len(g) > 0:\n",
    "                avg_degree = sum(i for i in nx.degree_centrality(g).values()) / len(nx.degree_centrality(g).keys())\n",
    "            else:\n",
    "                avg_degree = 0\n",
    "\n",
    "            # compute index for the subplot, and set this subplot as current\n",
    "            jx = np.unravel_index(j, axs.shape)\n",
    "            plt.sca(axs[jx])\n",
    "\n",
    "            ## create the plot_____________________________________________\n",
    "\n",
    "            if analyzeArgs[\"plot\"] == \"topol\":\n",
    "\n",
    "                topol = (\"cluster_coef\", \"assort\", \"avg_degree\")\n",
    "                colors = [\"midnightblue\", \"steelblue\", \"skyblue\"]\n",
    "\n",
    "                y_pos = np.arange(len(topol))\n",
    "                topol_values = [cluster_coef, assort, avg_degree]\n",
    "                plt.bar(y_pos, topol_values, color=colors, align='center')\n",
    "                plt.xticks(y_pos, topol)\n",
    "\n",
    "\n",
    "            elif analyzeArgs[\"plot\"] == \"nodes_edges\":\n",
    "\n",
    "                topol = (\"#nodes\", \"density\")\n",
    "                colors = [\"midnightblue\", \"steelblue\"]\n",
    "\n",
    "                y_pos = np.arange(len(topol))\n",
    "                topol_values = [len(g.nodes()) / dataArgs[\"n_max\"], min(density, 1)]\n",
    "                plt.bar(y_pos, topol_values, color=colors, align='center')\n",
    "                plt.xticks(y_pos, topol)\n",
    "\n",
    "\n",
    "            elif analyzeArgs[\"plot\"] == \"distr\":\n",
    "\n",
    "                degree_sequence = sorted([d for n, d in g.degree()], reverse=True)  # degree sequence\n",
    "                plt.plot(degree_sequence)\n",
    "\n",
    "            axs[jx].set_axis_off()\n",
    "\n",
    "        # import matplotlib.patches as mpatches\n",
    "\n",
    "        # density_patch = mpatches.Patch(color='midnightblue', label='density')\n",
    "        # cluster_patch = mpatches.Patch(color='blue', label='cluster_coef')\n",
    "        # assort_patch = mpatches.Patch(color='steelblue', label='assort')\n",
    "        # avg_degree_patch = mpatches.Patch(color='skyblue', label='avg_degree')\n",
    "        # axs[-1].legend(handles=[density_patch, cluster_patch, assort_patch, avg_degree_patch])\n",
    "\n",
    "        if analyzeArgs[\"save_plots\"] == True:\n",
    "            filename = os.path.join(model_name, \"digits_over_latent.png\")\n",
    "            plt.savefig(filename)\n",
    "\n",
    "    if modelArgs[\"latent_dim\"] == 2:\n",
    "\n",
    "        # linearly spaced coordinates corresponding to the 2D plot\n",
    "        # of digit classes in the latent space\n",
    "        if analyzeArgs[\"sample\"] == \"z\":\n",
    "            grid_x = np.sort(np.random.normal(np.mean(z_mean[:, analyzeArgs[\"z\"][0]]),\n",
    "                                              np.mean(np.exp(z_log_var[:, analyzeArgs[\"z\"][0]])),\n",
    "                                              analyzeArgs[\"size_of_manifold\"]))\n",
    "            grid_y = np.sort(np.random.normal(np.mean(z_mean[:, analyzeArgs[\"z\"][1]]),\n",
    "                                              np.mean(np.exp(z_log_var[:, analyzeArgs[\"z\"][1]])),\n",
    "                                              analyzeArgs[\"size_of_manifold\"]))\n",
    "        elif analyzeArgs[\"sample\"] == \"range\":\n",
    "            grid_x = np.linspace(analyzeArgs[\"act_range\"][0], analyzeArgs[\"act_range\"][1],\n",
    "                                 analyzeArgs[\"size_of_manifold\"])\n",
    "            grid_y = np.linspace(analyzeArgs[\"act_range\"][0], analyzeArgs[\"act_range\"][1],\n",
    "                                 analyzeArgs[\"size_of_manifold\"])[::-1]  ## revert\n",
    "        elif analyzeArgs[\"sample\"] == \"normal\":\n",
    "            grid_x = np.sort(\n",
    "                np.random.normal(np.mean(z_mean[:, analyzeArgs[\"z\"][0]]), 1, analyzeArgs[\"size_of_manifold\"]))\n",
    "            grid_y = np.sort(\n",
    "                np.random.normal(np.mean(z_mean[:, analyzeArgs[\"z\"][1]]), 1, analyzeArgs[\"size_of_manifold\"]))\n",
    "\n",
    "        ## 1) create graph topol plots_______________________________________________\n",
    "\n",
    "        fig, axs = plt.subplots(analyzeArgs[\"size_of_manifold\"], analyzeArgs[\"size_of_manifold\"], figsize=(8, 8), dpi = 300)\n",
    "        # fig.subplots_adjust(hspace = .5, wspace=.001)\n",
    "\n",
    "        for i, yi in enumerate(grid_y):\n",
    "            for j, xi in enumerate(grid_x):\n",
    "\n",
    "                xi_value = xi ** analyzeArgs[\"act_scale\"]\n",
    "                yi_value = yi ** analyzeArgs[\"act_scale\"]\n",
    "\n",
    "                z_sample = np.array([[xi_value, yi_value]])\n",
    "                x_decoded = graph_decoder.predict(z_sample)\n",
    "\n",
    "                ## reconstruct upper triangular adjacency matrix\n",
    "                reconstructed_a = reconstruct_adjacency(x_decoded, dataArgs[\"clip\"], dataArgs[\"diag_offset\"])\n",
    "\n",
    "                ## reconstruct graph\n",
    "                reconstructed_a = unpad_matrix(reconstructed_a, dataArgs[\"diag_value\"], dataArgs[\"fix_n\"])\n",
    "                reconstructed_a = Reconstr_a[i, j]\n",
    "                g = nx.from_numpy_matrix(reconstructed_a)\n",
    "\n",
    "                ## Obtain Graph Topologies____________________________________\n",
    "\n",
    "                density = nx.density(g)\n",
    "\n",
    "                if len(g) > 0:\n",
    "                    if nx.is_connected(g):\n",
    "                        diameter = nx.diameter(g)\n",
    "                else:\n",
    "                    diameter = -1\n",
    "\n",
    "                if len(g) > 0:\n",
    "                    cluster_coef = nx.average_clustering(g)\n",
    "                else:\n",
    "                    cluster_coef = 0\n",
    "\n",
    "                if len(g) > 0:\n",
    "                    if g.number_of_edges() > 0:\n",
    "                        assort = nx.degree_assortativity_coefficient(g, x='out', y='in')\n",
    "                else:\n",
    "                    assort = 0\n",
    "\n",
    "                if len(g) > 0:\n",
    "                    edges = g.number_of_edges()\n",
    "                else:\n",
    "                    edges = 0\n",
    "\n",
    "                if len(g) > 0:\n",
    "                    avg_degree = sum(i for i in nx.degree_centrality(g).values()) / len(\n",
    "                        nx.degree_centrality(g).keys())\n",
    "                else:\n",
    "                    avg_degree = 0\n",
    "\n",
    "                # compute index for the subplot, and set this subplot as current\n",
    "                plt.sca(axs[i, j])\n",
    "\n",
    "                ## create the plot_____________________________________________\n",
    "\n",
    "                if analyzeArgs[\"plot\"] == \"topol\":\n",
    "\n",
    "                    topol = (\"cluster_coef\", \"assort\", \"avg_degree\")\n",
    "                    colors = [\"midnightblue\", \"steelblue\", \"skyblue\"]\n",
    "\n",
    "                    y_pos = np.arange(len(topol))\n",
    "                    topol_values = [cluster_coef, assort, avg_degree]\n",
    "                    plt.bar(y_pos, topol_values, color=colors, align='center')\n",
    "                    plt.xticks(y_pos, topol)\n",
    "\n",
    "\n",
    "                elif analyzeArgs[\"plot\"] == \"nodes_edges\":\n",
    "\n",
    "                    topol = (\"#nodes\", \"density\")\n",
    "                    colors = [\"midnightblue\", \"steelblue\"]\n",
    "\n",
    "                    y_pos = np.arange(len(topol))\n",
    "                    topol_values = [len(g.nodes()) / dataArgs[\"n_max\"], min(density, 1)]\n",
    "                    plt.bar(y_pos, topol_values, color=colors, align='center')\n",
    "                    plt.xticks(y_pos, topol)\n",
    "\n",
    "\n",
    "                elif analyzeArgs[\"plot\"] == \"distr\":\n",
    "\n",
    "                    degree_sequence = sorted([d for n, d in g.degree()], reverse=True)  # degree sequence\n",
    "                    plt.plot(degree_sequence)\n",
    "\n",
    "                axs[i, j].set_axis_off()\n",
    "\n",
    "        if analyzeArgs[\"save_plots\"] == True:\n",
    "            filename = os.path.join(model_name, \"digits_over_latent.png\")\n",
    "            plt.savefig(filename)\n",
    "\n",
    "\n",
    "\n",
    "    if modelArgs[\"latent_dim\"] > 2:\n",
    "\n",
    "        z_sample = np.zeros(modelArgs[\"latent_dim\"])\n",
    "        z_sample = np.reshape(z_sample, (1, modelArgs[\"latent_dim\"]))\n",
    "\n",
    "        ## fill unobserved dimensions with mean of latent variable dimension\n",
    "        for dim in range(0, len(z_sample[0])):\n",
    "            z_sample[0][dim] = np.mean(z_mean[:, dim])\n",
    "\n",
    "        # linearly spaced coordinates corresponding to the 2D plot\n",
    "        # of digit classes in the latent space\n",
    "        if analyzeArgs[\"sample\"] == \"z\":\n",
    "            grid_x = np.sort(np.random.normal(np.mean(z_mean[:, analyzeArgs[\"z\"][0]]),\n",
    "                                              np.mean(np.exp(z_log_var[:, analyzeArgs[\"z\"][0]])),\n",
    "                                              analyzeArgs[\"size_of_manifold\"]))\n",
    "            grid_y = np.sort(np.random.normal(np.mean(z_mean[:, analyzeArgs[\"z\"][1]]),\n",
    "                                              np.mean(np.exp(z_log_var[:, analyzeArgs[\"z\"][1]])),\n",
    "                                              analyzeArgs[\"size_of_manifold\"]))\n",
    "        elif analyzeArgs[\"sample\"] == \"range\":\n",
    "            grid_x = np.linspace(analyzeArgs[\"act_range\"][0], analyzeArgs[\"act_range\"][1],\n",
    "                                 analyzeArgs[\"size_of_manifold\"])\n",
    "            grid_y = np.linspace(analyzeArgs[\"act_range\"][0], analyzeArgs[\"act_range\"][1],\n",
    "                                 analyzeArgs[\"size_of_manifold\"])[::-1]  ## revert\n",
    "        elif analyzeArgs[\"sample\"] == \"normal\":\n",
    "            grid_x = np.sort(\n",
    "                np.random.normal(np.mean(z_mean[:, analyzeArgs[\"z\"][0]]), 1, analyzeArgs[\"size_of_manifold\"]))\n",
    "            grid_y = np.sort(\n",
    "                np.random.normal(np.mean(z_mean[:, analyzeArgs[\"z\"][1]]), 1, analyzeArgs[\"size_of_manifold\"]))\n",
    "\n",
    "        ## 1) create graph topol plots_______________________________________________\n",
    "\n",
    "        fig, axs = plt.subplots(analyzeArgs[\"size_of_manifold\"], analyzeArgs[\"size_of_manifold\"], figsize=(10, 10), dpi = 300)\n",
    "        # fig.subplots_adjust(hspace = .5, wspace=.001)\n",
    "\n",
    "        for i, yi in enumerate(grid_y):\n",
    "            for j, xi in enumerate(grid_x):\n",
    "\n",
    "                z_sample[0][analyzeArgs[\"z\"][0]] = xi ** analyzeArgs[\"act_scale\"]\n",
    "                z_sample[0][analyzeArgs[\"z\"][1]] = xi ** analyzeArgs[\"act_scale\"]\n",
    "                x_decoded = graph_decoder.predict(z_sample)\n",
    "\n",
    "                ## reconstruct upper triangular adjacency matrix\n",
    "                reconstructed_a = reconstruct_adjacency(x_decoded, dataArgs[\"clip\"], dataArgs[\"diag_offset\"])\n",
    "\n",
    "                ## reconstruct graph\n",
    "                reconstructed_a = unpad_matrix(reconstructed_a, dataArgs[\"diag_value\"], dataArgs[\"fix_n\"])\n",
    "\n",
    "                reconstructed_a = Reconstr_a[i, j]\n",
    "                g = nx.from_numpy_matrix(reconstructed_a)\n",
    "\n",
    "                ## Obtain Graph Topologies____________________________________\n",
    "\n",
    "                density = nx.density(g)\n",
    "\n",
    "                if len(g) > 0:\n",
    "                    if nx.is_connected(g):\n",
    "                        diameter = nx.diameter(g)\n",
    "                else:\n",
    "                    diameter = -1\n",
    "\n",
    "                if len(g) > 0:\n",
    "                    cluster_coef = nx.average_clustering(g)\n",
    "                else:\n",
    "                    cluster_coef = 0\n",
    "\n",
    "                if len(g) > 0:\n",
    "                    if g.number_of_edges() > 0:\n",
    "                        assort = nx.degree_assortativity_coefficient(g, x='out', y='in')\n",
    "                else:\n",
    "                    assort = 0\n",
    "\n",
    "                if len(g) > 0:\n",
    "                    edges = g.number_of_edges()\n",
    "                else:\n",
    "                    edges = 0\n",
    "\n",
    "                if len(g) > 0:\n",
    "                    avg_degree = sum(i for i in nx.degree_centrality(g).values()) / len(\n",
    "                        nx.degree_centrality(g).keys())\n",
    "                else:\n",
    "                    avg_degree = 0\n",
    "\n",
    "                # compute index for the subplot, and set this subplot as current\n",
    "                plt.sca(axs[i, j])\n",
    "\n",
    "                ## create the plot_____________________________________________\n",
    "\n",
    "                if analyzeArgs[\"plot\"] == \"topol\":\n",
    "\n",
    "                    topol = (\"cluster_coef\", \"assort\", \"avg_degree\")\n",
    "                    colors = [\"midnightblue\", \"steelblue\", \"skyblue\"]\n",
    "\n",
    "                    y_pos = np.arange(len(topol))\n",
    "                    topol_values = [cluster_coef, assort, avg_degree]\n",
    "                    plt.bar(y_pos, topol_values, color=colors, align='center')\n",
    "                    plt.xticks(y_pos, topol)\n",
    "\n",
    "\n",
    "                elif analyzeArgs[\"plot\"] == \"nodes_edges\":\n",
    "\n",
    "                    topol = (\"#nodes\", \"density\")\n",
    "                    colors = [\"midnightblue\", \"steelblue\"]\n",
    "\n",
    "                    y_pos = np.arange(len(topol))\n",
    "                    topol_values = [len(g.nodes()) / dataArgs[\"n_max\"], min(density, 1)]\n",
    "                    plt.bar(y_pos, topol_values, color=colors, align='center')\n",
    "                    plt.xticks(y_pos, topol)\n",
    "\n",
    "\n",
    "                elif analyzeArgs[\"plot\"] == \"distr\":\n",
    "\n",
    "                    degree_sequence = sorted([d for n, d in g.degree()], reverse=True)  # degree sequence\n",
    "                    plt.plot(degree_sequence)\n",
    "\n",
    "                axs[i, j].set_axis_off()\n",
    "\n",
    "        if analyzeArgs[\"save_plots\"] == True:\n",
    "            filename = os.path.join(model_name, \"digits_over_latent.png\")\n",
    "            plt.savefig(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# range, normal, z\n",
    "analyzeArgs = {\"z\": [0,1], \"sample\": \"range\", \"act_range\": [-2, 2], \"act_scale\": 3, \"size_of_manifold\": 7, \"save_plots\": False}\n",
    "Reconstr_a = generate_manifold(analyzeArgs, modelArgs, dataArgs, models, data, color_map, batch_size=trainArgs[\"batch_size\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "## \"topol\", \"distr\", \"nodes_edges\"\n",
    "analyzeArgs[\"plot\"] = \"topol\" \n",
    "generate_topol_manifold(Reconstr_a, analyzeArgs, modelArgs, dataArgs, models, data, color_map, batch_size=trainArgs[\"batch_size\"])\n",
    "## cluster_coef\", \"assort\", \"avg_degree\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare with Original Graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Original Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     3
    ]
   },
   "outputs": [],
   "source": [
    "sampleArgs[\"random_n\"] = dataArgs[\"n_max\"]\n",
    "sampleArgs[\"source_starts\"] = dataArgs[\"n_max\"]\n",
    "\n",
    "def get_original_graph(sampleArgs, g_complete, a_complete):\n",
    "    g_subset = get_graph(sampleArgs, g_complete, a_complete)\n",
    "    g_subset, a_subset = sort_adjacency(g_subset)\n",
    "    a_subset = pad_matrix(a_subset, dataArgs[\"n_max\"], dataArgs[\"diag_value\"])\n",
    "    return g_subset, a_subset\n",
    "\n",
    "g_subset, a_subset = get_original_graph(sampleArgs, g_complete, a_complete)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adjacency, Precision and Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "analyzeArgs = {\"z\": [0,1], \"sample\": \"range\", \"act_range\": [-10, 10], \"act_scale\": 5, \"size_of_manifold\": 7}\n",
    "compare_manifold_adjacency(Reconstr_a, g_subset, a_subset, analyzeArgs, modelArgs, dataArgs, models, data, color_map, batch_size=trainArgs[\"batch_size\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     11,
     421
    ]
   },
   "outputs": [],
   "source": [
    "from support.preprocessing import reconstruct_adjacency, unpad_matrix\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "from matplotlib import pylab as plt\n",
    "import collections\n",
    "import os\n",
    "\n",
    "\n",
    "## DECODER - Latent Space Interpolation____________________________\n",
    "\n",
    "def compare_manifold_adjacency(Reconstr_a, g_original, a_original, analyzeArgs, modelArgs, dataArgs, models, data,\n",
    "                               color_map, batch_size=128):\n",
    "    print(\"latent dimensions:\", modelArgs[\"latent_dim\"])\n",
    "\n",
    "    if modelArgs[\"param_loss\"]:\n",
    "        encoder, graph_decoder, param_decoder = models  # trained models\n",
    "    else:\n",
    "        encoder, graph_decoder = models  # trained models\n",
    "\n",
    "    x_test, y_test = data\n",
    "\n",
    "    # display a 2D plot of the digit classes in the latent space\n",
    "    z_mean, z_log_var, z = encoder.predict(x_test, batch_size)\n",
    "\n",
    "    ## Latent Space Dimension is 1 ______________________\n",
    "\n",
    "    if modelArgs[\"latent_dim\"] == 1:\n",
    "\n",
    "        ## 1) create adjacency plots__________________________________________\n",
    "\n",
    "        # display a 30x30 2D manifold of digits\n",
    "        n = dataArgs[\"n_max\"]  # number of nodes\n",
    "        figure = np.zeros((1 * n, analyzeArgs[\"size_of_manifold\"] * n, 3))\n",
    "\n",
    "        # linearly spaced coordinates corresponding to the 2D plot\n",
    "        # of digit classes in the latent space\n",
    "        if analyzeArgs[\"sample\"] == \"z\":\n",
    "            grid_x = np.sort(np.random.normal(np.mean(z_mean[:, analyzeArgs[\"z\"][0]]),\n",
    "                                              np.mean(np.exp(z_log_var[:, analyzeArgs[\"z\"][0]])),\n",
    "                                              analyzeArgs[\"size_of_manifold\"]))\n",
    "        elif analyzeArgs[\"sample\"] == \"range\":\n",
    "            grid_x = np.linspace(analyzeArgs[\"act_range\"][0], analyzeArgs[\"act_range\"][1],\n",
    "                                 analyzeArgs[\"size_of_manifold\"])\n",
    "        elif analyzeArgs[\"sample\"] == \"normal\":\n",
    "            grid_x = np.sort(\n",
    "                np.random.normal(np.mean(z_mean[:, analyzeArgs[\"z\"][0]]), 1, analyzeArgs[\"size_of_manifold\"]))\n",
    "\n",
    "        z_sample = np.zeros(modelArgs[\"latent_dim\"])\n",
    "        z_sample = np.reshape(z_sample, (1, modelArgs[\"latent_dim\"]))\n",
    "\n",
    "        ## 2) create graph plots_______________________________________________\n",
    "\n",
    "        fig, axs = plt.subplots(1, analyzeArgs[\"size_of_manifold\"], figsize=(10, 10))\n",
    "        # fig.subplots_adjust(hspace = .5, wspace=.001)\n",
    "        axs = axs.ravel()\n",
    "\n",
    "        for j, xi in enumerate(grid_x):\n",
    "\n",
    "            z_sample[0][0] = xi ** analyzeArgs[\"act_scale\"]\n",
    "            x_decoded = graph_decoder.predict(z_sample)\n",
    "\n",
    "            comparison_matrix = np.zeros((n, n, 3))\n",
    "            reconstructed_a = reconstruct_adjacency(x_decoded, dataArgs[\"clip\"], dataArgs[\"diag_offset\"])\n",
    "\n",
    "            # reconstruct graph\n",
    "            g = nx.from_numpy_matrix(unpad_matrix(reconstructed_a, dataArgs[\"diag_value\"], dataArgs[\"fix_n\"]))\n",
    "            reconstructed_a = Reconstr_a[j]\n",
    "            g = nx.from_numpy_matrix(reconstructed_a)\n",
    "\n",
    "            # metrics per graph\n",
    "            metrics = {\"tp\": 0, \"fp\": 0, \"tn\": 0, \"fn\": 0}\n",
    "\n",
    "            for x in range(0, n):\n",
    "                for y in range(0, n):\n",
    "\n",
    "                    if a_original[x, y] == reconstructed_a[x, y] and a_original[x, y] == 1:  # correct\n",
    "                        comparison_matrix[x, y, :] = [0, 90, 0]  # green\n",
    "                        if x != y:\n",
    "                            metrics[\"tp\"] = metrics.get(\"tp\") + 1\n",
    "\n",
    "                    elif a_original[x, y] == reconstructed_a[x, y] and a_original[x, y] == 0:  # correct\n",
    "                        comparison_matrix[x, y, :] = [255, 255, 255]  # black\n",
    "                        if x != y:\n",
    "                            metrics[\"tn\"] = metrics.get(\"tn\") + 1\n",
    "\n",
    "                    elif a_original[x, y] != reconstructed_a[x, y] and a_original[x, y] == 1:  # underfit\n",
    "\n",
    "                        if x < len(g) and y < len(g):\n",
    "                            comparison_matrix[x, y, :] = [90, 0, 0]  # red   # missed\n",
    "                            if x != y:\n",
    "                                metrics[\"fn\"] = metrics.get(\"fn\") + 1\n",
    "                        else:\n",
    "                            comparison_matrix[x, y, :] = [150, 150, 150]  # grey   # not possible since too small\n",
    "\n",
    "                    elif a_original[x, y] != reconstructed_a[x, y] and a_original[x, y] == 0:  # overfit\n",
    "                        comparison_matrix[x, y, :] = [70, 70, 0]  # yellow\n",
    "                        if x != y:\n",
    "                            metrics[\"fp\"] = metrics.get(\"fp\") + 1\n",
    "\n",
    "            ## 1) create adjacency plots_____________________________________\n",
    "\n",
    "            figure[0:n, j * n: (j + 1) * n, :] = comparison_matrix\n",
    "\n",
    "            ## 2) create metric plots _______________________________________________\n",
    "\n",
    "            acc = (metrics[\"tp\"] + metrics[\"tn\"]) / (metrics[\"tn\"] + metrics[\"fn\"] + metrics[\"tp\"] + metrics[\"fp\"])\n",
    "\n",
    "            if (metrics[\"tp\"] + metrics[\"fp\"]) > 0:\n",
    "                prec = (metrics[\"tp\"]) / (metrics[\"tp\"] + metrics[\"fp\"])\n",
    "            else:\n",
    "                prec = 0\n",
    "\n",
    "            if (metrics[\"tp\"] + metrics[\"fn\"]) > 0:\n",
    "                recall = (metrics[\"tp\"]) / (metrics[\"tp\"] + metrics[\"fn\"])\n",
    "            else:\n",
    "                recall = 0\n",
    "\n",
    "            if (recall + prec) > 0:\n",
    "                f1 = 2 * (recall * prec) / (recall + prec)\n",
    "            else:\n",
    "                f1 = 0\n",
    "\n",
    "            y_pos = np.arange(3)\n",
    "            final_metrics = [prec, recall, f1]\n",
    "            colors = [\"skyblue\", \"skyblue\", \"midnightblue\"]\n",
    "\n",
    "            jx = np.unravel_index(j, axs.shape)\n",
    "            plt.sca(axs[jx])\n",
    "\n",
    "            plt.bar(y_pos, final_metrics, color=colors, align='center')\n",
    "            plt.plot([-1, 3], [0.25, 0.25], color='grey', linestyle='dashed')\n",
    "            plt.plot([-1, 3], [0.5, 0.5], color='grey', linestyle='dashed')\n",
    "            plt.plot([-1, 3], [0.75, 0.75], color='grey', linestyle='dashed')\n",
    "\n",
    "            axs[jx].set_axis_off()\n",
    "            axs[jx].set(ylabel='z_0')\n",
    "\n",
    "        start_range = n // 2\n",
    "        end_range = (analyzeArgs[\"size_of_manifold\"] - 1) * n + start_range + 1\n",
    "        pixel_range = np.arange(start_range, end_range, n)\n",
    "        sample_range_x = np.round(grid_x, 1)\n",
    "\n",
    "        # Plot_____________________________\n",
    "\n",
    "        plt.figure(figsize=(10, 10))\n",
    "        plt.xticks(pixel_range, sample_range_x)\n",
    "        plt.xlabel(\"z_0\", fontweight='bold')\n",
    "        plt.imshow((figure * 255).astype(np.uint8))\n",
    "        plt.show()\n",
    "\n",
    "    ## Latent Space Dimension is 2 ______________________\n",
    "\n",
    "    if modelArgs[\"latent_dim\"] == 2:\n",
    "\n",
    "        ## 1) create adjacency plots_______________________________________________\n",
    "\n",
    "        # display a 30x30 2D manifold of digits\n",
    "        n = dataArgs[\"n_max\"]  # number of nodes\n",
    "        figure = np.zeros((analyzeArgs[\"size_of_manifold\"] * n, analyzeArgs[\"size_of_manifold\"] * n, 3))\n",
    "\n",
    "        # linearly spaced coordinates corresponding to the 2D plot\n",
    "        # of digit classes in the latent space\n",
    "        if analyzeArgs[\"sample\"] == \"z\":\n",
    "            grid_x = np.sort(np.random.normal(np.mean(z_mean[:, analyzeArgs[\"z\"][0]]),\n",
    "                                              np.mean(np.exp(z_log_var[:, analyzeArgs[\"z\"][0]])),\n",
    "                                              analyzeArgs[\"size_of_manifold\"]))\n",
    "            grid_y = np.sort(np.random.normal(np.mean(z_mean[:, analyzeArgs[\"z\"][1]]),\n",
    "                                              np.mean(np.exp(z_log_var[:, analyzeArgs[\"z\"][1]])),\n",
    "                                              analyzeArgs[\"size_of_manifold\"]))\n",
    "        elif analyzeArgs[\"sample\"] == \"range\":\n",
    "            grid_x = np.linspace(analyzeArgs[\"act_range\"][0], analyzeArgs[\"act_range\"][1],\n",
    "                                 analyzeArgs[\"size_of_manifold\"])\n",
    "            grid_y = np.linspace(analyzeArgs[\"act_range\"][0], analyzeArgs[\"act_range\"][1],\n",
    "                                 analyzeArgs[\"size_of_manifold\"])[::-1]  ## revert\n",
    "        elif analyzeArgs[\"sample\"] == \"normal\":\n",
    "            grid_x = np.sort(\n",
    "                np.random.normal(np.mean(z_mean[:, analyzeArgs[\"z\"][0]]), 1, analyzeArgs[\"size_of_manifold\"]))\n",
    "            grid_y = np.sort(\n",
    "                np.random.normal(np.mean(z_mean[:, analyzeArgs[\"z\"][1]]), 1, analyzeArgs[\"size_of_manifold\"]))\n",
    "\n",
    "        ## 2) create metric plots _______________________________________________\n",
    "\n",
    "        fig, axs = plt.subplots(analyzeArgs[\"size_of_manifold\"], analyzeArgs[\"size_of_manifold\"], figsize=(10, 10))\n",
    "\n",
    "        for i, yi in enumerate(grid_y):\n",
    "            for j, xi in enumerate(grid_x):\n",
    "\n",
    "                xi_value = xi ** analyzeArgs[\"act_scale\"]\n",
    "                yi_value = yi ** analyzeArgs[\"act_scale\"]\n",
    "\n",
    "                z_sample = np.array([[xi_value, yi_value]])\n",
    "                x_decoded = graph_decoder.predict(z_sample)\n",
    "\n",
    "                comparison_matrix = np.zeros((n, n, 3))\n",
    "                reconstructed_a = reconstruct_adjacency(x_decoded, dataArgs[\"clip\"], dataArgs[\"diag_offset\"])\n",
    "\n",
    "                # reconstruct graph\n",
    "                g = nx.from_numpy_matrix(unpad_matrix(reconstructed_a, dataArgs[\"diag_value\"], dataArgs[\"fix_n\"]))\n",
    "\n",
    "                reconstructed_a = Reconstr_a[i, j]\n",
    "                g = nx.from_numpy_matrix(reconstructed_a)\n",
    "\n",
    "                # metrics per graph\n",
    "                metrics = {\"tp\": 0, \"fp\": 0, \"tn\": 0, \"fn\": 0}\n",
    "\n",
    "                for x in range(0, n):\n",
    "                    for y in range(0, n):\n",
    "\n",
    "                        if a_original[x, y] == reconstructed_a[x, y] and a_original[x, y] == 1:  # correct\n",
    "                            comparison_matrix[x, y, :] = [0, 90, 0]  # green\n",
    "                            if x != y:\n",
    "                                metrics[\"tp\"] = metrics.get(\"tp\") + 1\n",
    "\n",
    "                        elif a_original[x, y] == reconstructed_a[x, y] and a_original[x, y] == 0:  # correct\n",
    "                            comparison_matrix[x, y, :] = [255, 255, 255]  # black\n",
    "                            if x != y:\n",
    "                                metrics[\"tn\"] = metrics.get(\"tn\") + 1\n",
    "\n",
    "                        elif a_original[x, y] != reconstructed_a[x, y] and a_original[x, y] == 1:  # underfit\n",
    "\n",
    "                            if x < len(g) and y < len(g):\n",
    "                                comparison_matrix[x, y, :] = [90, 0, 0]  # red   # missed\n",
    "                                if x != y:\n",
    "                                    metrics[\"fn\"] = metrics.get(\"fn\") + 1\n",
    "                            else:\n",
    "                                comparison_matrix[x, y, :] = [150, 150, 150]  # grey   # not possible since too small\n",
    "\n",
    "                        elif a_original[x, y] != reconstructed_a[x, y] and a_original[x, y] == 0:  # overfit\n",
    "                            comparison_matrix[x, y, :] = [70, 70, 0]  # yellow\n",
    "                            if x != y:\n",
    "                                metrics[\"fp\"] = metrics.get(\"fp\") + 1\n",
    "\n",
    "                ## 1) create adjacency plots_____________________________________\n",
    "\n",
    "                figure[i * n: (i + 1) * n, j * n: (j + 1) * n, :] = comparison_matrix\n",
    "\n",
    "                ## 2) create metric plots _______________________________________________\n",
    "\n",
    "                acc = (metrics[\"tp\"] + metrics[\"tn\"]) / (metrics[\"tn\"] + metrics[\"fn\"] + metrics[\"tp\"] + metrics[\"fp\"])\n",
    "\n",
    "                if (metrics[\"tp\"] + metrics[\"fp\"]) > 0:\n",
    "                    prec = (metrics[\"tp\"]) / (metrics[\"tp\"] + metrics[\"fp\"])\n",
    "                else:\n",
    "                    prec = 0\n",
    "\n",
    "                if (metrics[\"tp\"] + metrics[\"fn\"]) > 0:\n",
    "                    recall = (metrics[\"tp\"]) / (metrics[\"tp\"] + metrics[\"fn\"])\n",
    "                else:\n",
    "                    recall = 0\n",
    "\n",
    "                if (recall + prec) > 0:\n",
    "                    f1 = 2 * (recall * prec) / (recall + prec)\n",
    "                else:\n",
    "                    f1 = 0\n",
    "\n",
    "                y_pos = np.arange(3)\n",
    "                final_metrics = [prec, recall, f1]\n",
    "                colors = [\"skyblue\", \"skyblue\", \"midnightblue\"]\n",
    "\n",
    "                plt.sca(axs[i, j])\n",
    "                plt.bar(y_pos, final_metrics, color=colors, align='center')\n",
    "                plt.plot([-1, 3], [0.25, 0.25], color='grey', linestyle='dashed')\n",
    "                plt.plot([-1, 3], [0.5, 0.5], color='grey', linestyle='dashed')\n",
    "                plt.plot([-1, 3], [0.75, 0.75], color='grey', linestyle='dashed')\n",
    "                axs[i, j].set_axis_off()\n",
    "\n",
    "        start_range = n // 2\n",
    "        end_range = (analyzeArgs[\"size_of_manifold\"] - 1) * n + start_range + 1\n",
    "        pixel_range = np.arange(start_range, end_range, n)\n",
    "        sample_range_x = np.round(grid_x, 1)\n",
    "        sample_range_y = np.round(grid_y, 1)\n",
    "\n",
    "        # Plot_____________________________\n",
    "\n",
    "        plt.figure(figsize=(10, 10))\n",
    "        plt.xticks(pixel_range, sample_range_x)\n",
    "        plt.yticks(pixel_range, sample_range_y)\n",
    "        plt.xlabel(\"z_0\", fontweight='bold')\n",
    "        plt.ylabel(\"z_1\", fontweight='bold')\n",
    "        plt.imshow((figure * 255).astype(np.uint8))\n",
    "        plt.show()\n",
    "\n",
    "    ## Latent Space Dimension is larger than 2 ______________________\n",
    "\n",
    "    if modelArgs[\"latent_dim\"] > 2:\n",
    "\n",
    "        z_sample = np.zeros(modelArgs[\"latent_dim\"])\n",
    "        z_sample = np.reshape(z_sample, (1, modelArgs[\"latent_dim\"]))\n",
    "\n",
    "        ## fill unobserved dimensions with mean of latent variable dimension\n",
    "        for dim in range(0, len(z_sample[0])):\n",
    "            z_sample[0][dim] = np.mean(z_mean[:, dim])\n",
    "\n",
    "        ## 1) create adjacency plots_______________________________________________\n",
    "\n",
    "        # display a 30x30 2D manifold of digits\n",
    "        n = dataArgs[\"n_max\"]  # number of nodes\n",
    "        figure = np.zeros((analyzeArgs[\"size_of_manifold\"] * n, analyzeArgs[\"size_of_manifold\"] * n, 3))\n",
    "\n",
    "        # linearly spaced coordinates corresponding to the 2D plot\n",
    "        # of digit classes in the latent space\n",
    "        if analyzeArgs[\"sample\"] == \"z\":\n",
    "            grid_x = np.sort(np.random.normal(np.mean(z_mean[:, analyzeArgs[\"z\"][0]]),\n",
    "                                              np.mean(np.exp(z_log_var[:, analyzeArgs[\"z\"][0]])),\n",
    "                                              analyzeArgs[\"size_of_manifold\"]))\n",
    "            grid_y = np.sort(np.random.normal(np.mean(z_mean[:, analyzeArgs[\"z\"][1]]),\n",
    "                                              np.mean(np.exp(z_log_var[:, analyzeArgs[\"z\"][1]])),\n",
    "                                              analyzeArgs[\"size_of_manifold\"]))\n",
    "        elif analyzeArgs[\"sample\"] == \"range\":\n",
    "            grid_x = np.linspace(analyzeArgs[\"act_range\"][0], analyzeArgs[\"act_range\"][1],\n",
    "                                 analyzeArgs[\"size_of_manifold\"])\n",
    "            grid_y = np.linspace(analyzeArgs[\"act_range\"][0], analyzeArgs[\"act_range\"][1],\n",
    "                                 analyzeArgs[\"size_of_manifold\"])[::-1]  ## revert\n",
    "        elif analyzeArgs[\"sample\"] == \"normal\":\n",
    "            grid_x = np.sort(\n",
    "                np.random.normal(np.mean(z_mean[:, analyzeArgs[\"z\"][0]]), 1, analyzeArgs[\"size_of_manifold\"]))\n",
    "            grid_y = np.sort(\n",
    "                np.random.normal(np.mean(z_mean[:, analyzeArgs[\"z\"][1]]), 1, analyzeArgs[\"size_of_manifold\"]))\n",
    "\n",
    "        ## 2) create metric plots _______________________________________________\n",
    "\n",
    "        fig, axs = plt.subplots(analyzeArgs[\"size_of_manifold\"], analyzeArgs[\"size_of_manifold\"], figsize=(10, 10))\n",
    "\n",
    "        for i, yi in enumerate(grid_y):\n",
    "            for j, xi in enumerate(grid_x):\n",
    "\n",
    "                z_sample[0][analyzeArgs[\"z\"][0]] = xi ** analyzeArgs[\"act_scale\"]\n",
    "                z_sample[0][analyzeArgs[\"z\"][1]] = xi ** analyzeArgs[\"act_scale\"]\n",
    "                x_decoded = graph_decoder.predict(z_sample)\n",
    "\n",
    "                comparison_matrix = np.zeros((n, n, 3))\n",
    "                reconstructed_a = reconstruct_adjacency(x_decoded, dataArgs[\"clip\"], dataArgs[\"diag_offset\"])\n",
    "\n",
    "                # reconstruct graph\n",
    "                g = nx.from_numpy_matrix(unpad_matrix(reconstructed_a, dataArgs[\"diag_value\"], dataArgs[\"fix_n\"]))\n",
    "\n",
    "                reconstructed_a = Reconstr_a[i, j]\n",
    "                g = nx.from_numpy_matrix(reconstructed_a)\n",
    "\n",
    "                # metrics per graph\n",
    "                metrics = {\"tp\": 0, \"fp\": 0, \"tn\": 0, \"fn\": 0}\n",
    "\n",
    "                for x in range(0, n):\n",
    "                    for y in range(0, n):\n",
    "\n",
    "                        if a_original[x, y] == reconstructed_a[x, y] and a_original[x, y] == 1:  # correct\n",
    "                            comparison_matrix[x, y, :] = [0, 90, 0]  # green\n",
    "                            if x != y:\n",
    "                                metrics[\"tp\"] = metrics.get(\"tp\") + 1\n",
    "\n",
    "                        elif a_original[x, y] == reconstructed_a[x, y] and a_original[x, y] == 0:  # correct\n",
    "                            comparison_matrix[x, y, :] = [255, 255, 255]  # black\n",
    "                            if x != y:\n",
    "                                metrics[\"tn\"] = metrics.get(\"tn\") + 1\n",
    "\n",
    "                        elif a_original[x, y] != reconstructed_a[x, y] and a_original[x, y] == 1:  # underfit\n",
    "\n",
    "                            if x < len(g) and y < len(g):\n",
    "                                comparison_matrix[x, y, :] = [90, 0, 0]  # red   # missed\n",
    "                                if x != y:\n",
    "                                    metrics[\"fn\"] = metrics.get(\"fn\") + 1\n",
    "                            else:\n",
    "                                comparison_matrix[x, y, :] = [150, 150, 150]  # grey   # not possible since too small\n",
    "\n",
    "                        elif a_original[x, y] != reconstructed_a[x, y] and a_original[x, y] == 0:  # overfit\n",
    "                            comparison_matrix[x, y, :] = [70, 70, 0]  # yellow\n",
    "                            if x != y:\n",
    "                                metrics[\"fp\"] = metrics.get(\"fp\") + 1\n",
    "\n",
    "                ## 1) create adjacency plots_____________________________________\n",
    "\n",
    "                figure[i * n: (i + 1) * n, j * n: (j + 1) * n, :] = comparison_matrix\n",
    "\n",
    "                ## 2) create metric plots _______________________________________________\n",
    "\n",
    "                acc = (metrics[\"tp\"] + metrics[\"tn\"]) / (metrics[\"tn\"] + metrics[\"fn\"] + metrics[\"tp\"] + metrics[\"fp\"])\n",
    "\n",
    "                if (metrics[\"tp\"] + metrics[\"fp\"]) > 0:\n",
    "                    prec = (metrics[\"tp\"]) / (metrics[\"tp\"] + metrics[\"fp\"])\n",
    "                else:\n",
    "                    prec = 0\n",
    "\n",
    "                if (metrics[\"tp\"] + metrics[\"fn\"]) > 0:\n",
    "                    recall = (metrics[\"tp\"]) / (metrics[\"tp\"] + metrics[\"fn\"])\n",
    "                else:\n",
    "                    recall = 0\n",
    "\n",
    "                if (recall + prec) > 0:\n",
    "                    f1 = 2 * (recall * prec) / (recall + prec)\n",
    "                else:\n",
    "                    f1 = 0\n",
    "\n",
    "                y_pos = np.arange(3)\n",
    "                final_metrics = [prec, recall, f1]\n",
    "                colors = [\"skyblue\", \"skyblue\", \"midnightblue\"]\n",
    "\n",
    "                plt.sca(axs[i, j])\n",
    "                plt.bar(y_pos, final_metrics, color=colors, align='center')\n",
    "                plt.plot([-1, 3], [0.25, 0.25], color='grey', linestyle='dashed')\n",
    "                plt.plot([-1, 3], [0.5, 0.5], color='grey', linestyle='dashed')\n",
    "                plt.plot([-1, 3], [0.75, 0.75], color='grey', linestyle='dashed')\n",
    "                axs[i, j].set_axis_off()\n",
    "\n",
    "        start_range = n // 2\n",
    "        end_range = (analyzeArgs[\"size_of_manifold\"] - 1) * n + start_range + 1\n",
    "        pixel_range = np.arange(start_range, end_range, n)\n",
    "        sample_range_x = np.round(grid_x, 1)\n",
    "        sample_range_y = np.round(grid_y, 1)\n",
    "\n",
    "        # Plot_____________________________\n",
    "\n",
    "        plt.figure(figsize=(10, 10))\n",
    "        plt.xticks(pixel_range, sample_range_x)\n",
    "        plt.yticks(pixel_range, sample_range_y)\n",
    "        plt.xlabel(\"z_\" + str(analyzeArgs[\"z\"][0]), fontweight='bold')\n",
    "        plt.ylabel(\"z_\" + str(analyzeArgs[\"z\"][1]), fontweight='bold')\n",
    "        plt.imshow((figure * 255).astype(np.uint8))\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "def compare_topol_manifold(Reconstr_a, g_original, a_original, analyzeArgs, modelArgs, dataArgs, models, data,\n",
    "                           color_map, batch_size=128):\n",
    "    print(\"latent dimensions:\", modelArgs[\"latent_dim\"])\n",
    "    encoder, graph_decoder = models  # trained models\n",
    "    x_test, y_test = data\n",
    "\n",
    "    # display a 2D plot of the digit classes in the latent space\n",
    "    z_mean, z_log_var, z = encoder.predict(x_test, batch_size)\n",
    "\n",
    "    ## Latent Space Dimension is 1 ______________________\n",
    "\n",
    "    if modelArgs[\"latent_dim\"] == 1:\n",
    "\n",
    "        # linearly spaced coordinates corresponding to the 2D plot\n",
    "        # of digit classes in the latent space\n",
    "        if analyzeArgs[\"sample\"] == \"z\":\n",
    "            grid_x = np.sort(np.random.normal(np.mean(z_mean[:, analyzeArgs[\"z\"][0]]),\n",
    "                                              np.mean(np.exp(z_log_var[:, analyzeArgs[\"z\"][0]])),\n",
    "                                              analyzeArgs[\"size_of_manifold\"]))\n",
    "        elif analyzeArgs[\"sample\"] == \"range\":\n",
    "            grid_x = np.linspace(analyzeArgs[\"act_range\"][0], analyzeArgs[\"act_range\"][1],\n",
    "                                 analyzeArgs[\"size_of_manifold\"])\n",
    "        elif analyzeArgs[\"sample\"] == \"normal\":\n",
    "            grid_x = np.sort(\n",
    "                np.random.normal(np.mean(z_mean[:, analyzeArgs[\"z\"][0]]), 1, analyzeArgs[\"size_of_manifold\"]))\n",
    "\n",
    "        z_sample = np.zeros(modelArgs[\"latent_dim\"])\n",
    "        z_sample = np.reshape(z_sample, (1, modelArgs[\"latent_dim\"]))\n",
    "\n",
    "        ## 1) create graph topol plots_______________________________________________\n",
    "\n",
    "        fig, axs = plt.subplots(1, analyzeArgs[\"size_of_manifold\"], figsize=(10, 10))\n",
    "        # fig.subplots_adjust(hspace = .5, wspace=.001)\n",
    "        axs = axs.ravel()\n",
    "\n",
    "        degree_sequence_original = sorted([d for n, d in g_original.degree()], reverse=True)  # degree sequence\n",
    "        degreeCount_original = collections.Counter(degree_sequence_original)\n",
    "        deg_original, cnt_original = zip(*degreeCount_original.items())\n",
    "\n",
    "        for j, xi in enumerate(grid_x):\n",
    "\n",
    "            z_sample[0][0] = xi ** analyzeArgs[\"act_scale\"]\n",
    "            x_decoded = graph_decoder.predict(z_sample)\n",
    "\n",
    "            ## reconstruct upper triangular adjacency matrix\n",
    "            reconstructed_a = reconstruct_adjacency(x_decoded, dataArgs[\"clip\"], dataArgs[\"diag_offset\"])\n",
    "\n",
    "            # reconstruct graph\n",
    "            reconstructed_a = unpad_matrix(reconstructed_a, dataArgs[\"diag_value\"], dataArgs[\"fix_n\"])\n",
    "            reconstructed_a = Reconstr_a[j]\n",
    "\n",
    "            g = nx.from_numpy_matrix(reconstructed_a)\n",
    "\n",
    "            ## Obtain Graph Topologies____________________________________\n",
    "\n",
    "            density_original = nx.density(g_original)\n",
    "            density = nx.density(g)\n",
    "\n",
    "            diameter_original = nx.diameter(g_original)\n",
    "            if len(g) > 0:\n",
    "                if nx.is_connected(g):\n",
    "                    diameter = nx.diameter(g)\n",
    "            else:\n",
    "                diameter = -1\n",
    "\n",
    "            cluster_coef_original = nx.average_clustering(g_original)\n",
    "            if len(g) > 0:\n",
    "                cluster_coef = nx.average_clustering(g)\n",
    "            else:\n",
    "                cluster_coef = 0\n",
    "\n",
    "            assort_original = nx.degree_assortativity_coefficient(g_original, x='out', y='in')\n",
    "            if len(g) > 0:\n",
    "                if g.number_of_edges() > 0:\n",
    "                    assort = nx.degree_assortativity_coefficient(g, x='out', y='in')\n",
    "            else:\n",
    "                assort = 0\n",
    "\n",
    "            edges_original = g_original.number_of_edges()\n",
    "            if len(g) > 0:\n",
    "                edges = g.number_of_edges()\n",
    "            else:\n",
    "                edges = 0\n",
    "\n",
    "            avg_degree_original = sum(i for i in nx.degree_centrality(g_original).values()) / len(\n",
    "                nx.degree_centrality(g_original).keys())\n",
    "            if len(g) > 0:\n",
    "                avg_degree = sum(i for i in nx.degree_centrality(g).values()) / len(nx.degree_centrality(g).keys())\n",
    "            else:\n",
    "                avg_degree = 0\n",
    "\n",
    "            # compute index for the subplot, and set this subplot as current\n",
    "            jx = np.unravel_index(j, axs.shape)\n",
    "            plt.sca(axs[jx])\n",
    "\n",
    "            ## create the plot_____________________________________________\n",
    "\n",
    "            if analyzeArgs[\"plot\"] == \"topol\":\n",
    "\n",
    "                topol = (\"cluster_coef\", \"assort\", \"avg_degree\")\n",
    "                colors = [\"midnightblue\", \"steelblue\", \"skyblue\"]\n",
    "\n",
    "                y_pos = np.arange(len(topol))\n",
    "                topol_values = [cluster_coef, assort, avg_degree]\n",
    "                topol_values_original = [cluster_coef_original, assort_original, avg_degree_original]\n",
    "                # plt.bar(y_pos, topol_values_original, color=colors, fill=False, align='center')\n",
    "                plt.hlines(topol_values_original[0], -0.5, 0.5)\n",
    "                plt.hlines(topol_values_original[1], 0.5, 1.5)\n",
    "                plt.hlines(topol_values_original[2], 1.5, 2.5)\n",
    "                plt.bar(y_pos, topol_values, color=colors, align='center')\n",
    "                plt.xticks(y_pos, topol)\n",
    "\n",
    "\n",
    "            elif analyzeArgs[\"plot\"] == \"topol_diff\":\n",
    "\n",
    "                topol = (\"cluster_coef\", \"assort\", \"avg_degree\")\n",
    "                colors = [\"midnightblue\", \"steelblue\", \"skyblue\"]\n",
    "\n",
    "                topol_values = [cluster_coef, assort, avg_degree]\n",
    "                topol_values_original = [cluster_coef_original, assort_original, avg_degree_original]\n",
    "\n",
    "                x_pos = np.arange(len(topol))\n",
    "                topol_differences = (np.asarray(topol_values_original) - np.asarray(topol_values))\n",
    "\n",
    "                plt.bar(x_pos, topol_differences, color=colors, align='center')\n",
    "                plt.xticks(x_pos, topol)\n",
    "\n",
    "                import matplotlib as mpl\n",
    "                import matplotlib.cm as cm\n",
    "\n",
    "                norm = mpl.colors.Normalize(vmin=0, vmax=1)\n",
    "                cmap = cm.RdYlGn_r\n",
    "                # cmap = cmap[::-1]\n",
    "\n",
    "                m = cm.ScalarMappable(norm=norm, cmap=cmap)\n",
    "\n",
    "                plt.ylim([-0.4, 0.4])\n",
    "                plt.hlines(0, -0.5, 0.5, color=m.to_rgba(np.abs(float(topol_differences[0]))), lw=3.5)\n",
    "                plt.hlines(0, 0.5, 1.5, color=m.to_rgba(np.abs(float(topol_differences[1]))), lw=3.5)\n",
    "                plt.hlines(0, 1.5, 2.5, color=m.to_rgba(np.abs(float(topol_differences[2]))), lw=3.5)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            elif analyzeArgs[\"plot\"] == \"distr\":\n",
    "\n",
    "                degree_sequence = sorted([d for n, d in g.degree()], reverse=True)  # degree sequence\n",
    "                degreeCount = collections.Counter(degree_sequence)\n",
    "                deg, cnt = zip(*degreeCount.items())\n",
    "\n",
    "                cnt = np.asarray(cnt) / sum(cnt)  # normalize degree sequence\n",
    "\n",
    "                if len(cnt) < len(cnt_original):\n",
    "                    cnt = np.repeat(cnt,\n",
    "                                    (len(cnt_original) / len(cnt)))  # stretch normalize degree sequence to match length\n",
    "                else:\n",
    "                    cnt = np.repeat(cnt,\n",
    "                                    (len(cnt) / len(cnt_original)))  # stretch normalize degree sequence to match length\n",
    "\n",
    "                cnt_original = np.asarray(cnt_original) / sum(cnt_original)  # normalize original degree sequence\n",
    "\n",
    "                plt.plot(cnt_original, color=\"midnightblue\", linestyle='dashed')\n",
    "                plt.plot(cnt, color=\"steelblue\")\n",
    "\n",
    "            axs[jx].set_axis_off()\n",
    "\n",
    "        # import matplotlib.patches as mpatches\n",
    "\n",
    "        # density_patch = mpatches.Patch(color='midnightblue', label='density')\n",
    "        # cluster_patch = mpatches.Patch(color='blue', label='cluster_coef')\n",
    "        # assort_patch = mpatches.Patch(color='steelblue', label='assort')\n",
    "        # avg_degree_patch = mpatches.Patch(color='skyblue', label='avg_degree')\n",
    "        # axs[-1].legend(handles=[density_patch, cluster_patch, assort_patch, avg_degree_patch])\n",
    "\n",
    "    if modelArgs[\"latent_dim\"] == 2:\n",
    "\n",
    "        # linearly spaced coordinates corresponding to the 2D plot\n",
    "        # of digit classes in the latent space\n",
    "        if analyzeArgs[\"sample\"] == \"z\":\n",
    "            grid_x = np.sort(np.random.normal(np.mean(z_mean[:, analyzeArgs[\"z\"][0]]),\n",
    "                                              np.mean(np.exp(z_log_var[:, analyzeArgs[\"z\"][0]])),\n",
    "                                              analyzeArgs[\"size_of_manifold\"]))\n",
    "            grid_y = np.sort(np.random.normal(np.mean(z_mean[:, analyzeArgs[\"z\"][1]]),\n",
    "                                              np.mean(np.exp(z_log_var[:, analyzeArgs[\"z\"][1]])),\n",
    "                                              analyzeArgs[\"size_of_manifold\"]))\n",
    "        elif analyzeArgs[\"sample\"] == \"range\":\n",
    "            grid_x = np.linspace(analyzeArgs[\"act_range\"][0], analyzeArgs[\"act_range\"][1],\n",
    "                                 analyzeArgs[\"size_of_manifold\"])\n",
    "            grid_y = np.linspace(analyzeArgs[\"act_range\"][0], analyzeArgs[\"act_range\"][1],\n",
    "                                 analyzeArgs[\"size_of_manifold\"])[::-1]  ## revert\n",
    "        elif analyzeArgs[\"sample\"] == \"normal\":\n",
    "            grid_x = np.sort(\n",
    "                np.random.normal(np.mean(z_mean[:, analyzeArgs[\"z\"][0]]), 1, analyzeArgs[\"size_of_manifold\"]))\n",
    "            grid_y = np.sort(\n",
    "                np.random.normal(np.mean(z_mean[:, analyzeArgs[\"z\"][1]]), 1, analyzeArgs[\"size_of_manifold\"]))\n",
    "\n",
    "        ## 1) create graph topol plots_______________________________________________\n",
    "\n",
    "        fig, axs = plt.subplots(analyzeArgs[\"size_of_manifold\"], analyzeArgs[\"size_of_manifold\"], figsize=(8, 8), dpi = 300)\n",
    "        # fig.subplots_adjust(hspace = .5, wspace=.001)\n",
    "\n",
    "        degree_sequence_original = sorted([d for n, d in g_original.degree()], reverse=True)  # degree sequence\n",
    "        degreeCount_original = collections.Counter(degree_sequence_original)\n",
    "        deg_original, cnt_original = zip(*degreeCount_original.items())\n",
    "\n",
    "        for i, yi in enumerate(grid_y):\n",
    "            for j, xi in enumerate(grid_x):\n",
    "\n",
    "                xi_value = xi ** analyzeArgs[\"act_scale\"]\n",
    "                yi_value = yi ** analyzeArgs[\"act_scale\"]\n",
    "\n",
    "                reconstructed_a = Reconstr_a[i, j]\n",
    "                g = nx.from_numpy_matrix(reconstructed_a)\n",
    "\n",
    "                ## Obtain Graph Topologies____________________________________\n",
    "\n",
    "                density_original = nx.density(g_original)\n",
    "                density = nx.density(g)\n",
    "\n",
    "                diameter_original = nx.diameter(g_original)\n",
    "                if len(g) > 0:\n",
    "                    if nx.is_connected(g):\n",
    "                        diameter = nx.diameter(g)\n",
    "                else:\n",
    "                    diameter = -1\n",
    "\n",
    "                cluster_coef_original = nx.average_clustering(g_original)\n",
    "                if len(g) > 0:\n",
    "                    cluster_coef = nx.average_clustering(g)\n",
    "                else:\n",
    "                    cluster_coef = 0\n",
    "\n",
    "                assort_original = nx.degree_assortativity_coefficient(g_original, x='out', y='in')\n",
    "                if len(g) > 0:\n",
    "                    if g.number_of_edges() > 0:\n",
    "                        assort = nx.degree_assortativity_coefficient(g, x='out', y='in')\n",
    "                else:\n",
    "                    assort = 0\n",
    "\n",
    "                edges_original = g_original.number_of_edges()\n",
    "                if len(g) > 0:\n",
    "                    edges = g.number_of_edges()\n",
    "                else:\n",
    "                    edges = 0\n",
    "\n",
    "                avg_degree_original = sum(i for i in nx.degree_centrality(g_original).values()) / len(\n",
    "                    nx.degree_centrality(g_original).keys())\n",
    "                if len(g) > 0:\n",
    "                    avg_degree = sum(i for i in nx.degree_centrality(g).values()) / len(nx.degree_centrality(g).keys())\n",
    "                else:\n",
    "                    avg_degree = 0\n",
    "\n",
    "                # compute index for the subplot, and set this subplot as current\n",
    "                plt.sca(axs[i, j])\n",
    "\n",
    "                ## create the plot_____________________________________________\n",
    "\n",
    "                if analyzeArgs[\"plot\"] == \"topol\":\n",
    "\n",
    "                    topol = (\"cluster_coef\", \"assort\", \"avg_degree\")\n",
    "                    colors = [\"midnightblue\", \"steelblue\", \"skyblue\"]\n",
    "\n",
    "                    y_pos = np.arange(len(topol))\n",
    "                    topol_values = [cluster_coef, assort, avg_degree]\n",
    "                    topol_values_original = [cluster_coef_original, assort_original, avg_degree_original]\n",
    "                    # plt.bar(y_pos, topol_values_original, color=colors, fill=False, align='center')\n",
    "                    plt.hlines(topol_values_original[0], -0.5, 0.5)\n",
    "                    plt.hlines(topol_values_original[1], 0.5, 1.5)\n",
    "                    plt.hlines(topol_values_original[2], 1.5, 2.5)\n",
    "                    plt.bar(y_pos, topol_values, color=colors, align='center')\n",
    "                    plt.xticks(y_pos, topol)\n",
    "\n",
    "\n",
    "                elif analyzeArgs[\"plot\"] == \"topol_diff\":\n",
    "\n",
    "                    topol = (\"cluster_coef\", \"assort\", \"avg_degree\")\n",
    "                    colors = [\"midnightblue\", \"steelblue\", \"skyblue\"]\n",
    "\n",
    "                    topol_values = [cluster_coef, assort, avg_degree]\n",
    "                    topol_values_original = [cluster_coef_original, assort_original, avg_degree_original]\n",
    "\n",
    "                    x_pos = np.arange(len(topol))\n",
    "                    topol_differences = (np.asarray(topol_values_original) - np.asarray(topol_values))\n",
    "\n",
    "                    plt.bar(x_pos, topol_differences, color=colors, align='center')\n",
    "                    plt.xticks(x_pos, topol)\n",
    "\n",
    "                    import matplotlib as mpl\n",
    "                    import matplotlib.cm as cm\n",
    "\n",
    "                    norm = mpl.colors.Normalize(vmin=0, vmax=1)\n",
    "                    cmap = cm.RdYlGn_r\n",
    "                    # cmap = cmap[::-1]\n",
    "\n",
    "                    m = cm.ScalarMappable(norm=norm, cmap=cmap)\n",
    "\n",
    "                    plt.ylim([-0.4, 0.4])\n",
    "                    plt.hlines(0, -0.5, 0.5, color=m.to_rgba(np.abs(float(topol_differences[0]))), lw=3.5)\n",
    "                    plt.hlines(0, 0.5, 1.5, color=m.to_rgba(np.abs(float(topol_differences[1]))), lw=3.5)\n",
    "                    plt.hlines(0, 1.5, 2.5, color=m.to_rgba(np.abs(float(topol_differences[2]))), lw=3.5)\n",
    "\n",
    "                    # color = [(x/10.0, x/20.0, 0.75) for x in 10*(np.abs(topol_differences) / np.sum(np.abs(topol_differences)))] # <-- Quick gradient example along the Red/Green dimensions.\n",
    "\n",
    "                    # from matplotlib import cm\n",
    "\n",
    "                    # colors = cm.YlOrRd(np.abs(topol_differences) / float(max(np.abs(topol_differences))))\n",
    "                    # colors = cm.Blues(np.abs(topol_differences) / float(max(np.abs(topol_differences))))\n",
    "                    # colors = cm.RdYlGn_r(np.abs(topol_differences) / float(max(np.abs(topol_differences))))\n",
    "                    # colors = colors[::-1]\n",
    "                    # plt.bar(range(len(topol_differences)), topol_differences, color=colors)\n",
    "\n",
    "\n",
    "                elif analyzeArgs[\"plot\"] == \"distr\":\n",
    "\n",
    "                    degree_sequence = sorted([d for n, d in g.degree()], reverse=True)  # degree sequence\n",
    "                    degreeCount = collections.Counter(degree_sequence)\n",
    "                    deg, cnt = zip(*degreeCount.items())\n",
    "\n",
    "                    cnt = np.asarray(cnt) / sum(cnt)  # normalize degree sequence\n",
    "\n",
    "                    if len(cnt) < len(cnt_original):\n",
    "                        cnt = np.repeat(cnt, (\n",
    "                                len(cnt_original) / len(cnt)))  # stretch normalize degree sequence to match length\n",
    "                    else:\n",
    "                        cnt = np.repeat(cnt, (\n",
    "                                len(cnt) / len(cnt_original)))  # stretch normalize degree sequence to match length\n",
    "\n",
    "                    cnt_original = np.asarray(cnt_original) / sum(cnt_original)  # normalize original degree sequence\n",
    "\n",
    "                    plt.plot(cnt_original, color=\"midnightblue\", linestyle='dashed')\n",
    "                    plt.plot(cnt, color=\"steelblue\")\n",
    "\n",
    "                axs[i, j].set_axis_off()\n",
    "\n",
    "    if modelArgs[\"latent_dim\"] > 2:\n",
    "\n",
    "        z_sample = np.zeros(modelArgs[\"latent_dim\"])\n",
    "        z_sample = np.reshape(z_sample, (1, modelArgs[\"latent_dim\"]))\n",
    "\n",
    "        ## fill unobserved dimensions with mean of latent variable dimension\n",
    "        for dim in range(0, len(z_sample[0])):\n",
    "            z_sample[0][dim] = np.mean(z_mean[:, dim])\n",
    "\n",
    "        # linearly spaced coordinates corresponding to the 2D plot\n",
    "        # of digit classes in the latent space\n",
    "        if analyzeArgs[\"sample\"] == \"z\":\n",
    "            grid_x = np.sort(np.random.normal(np.mean(z_mean[:, analyzeArgs[\"z\"][0]]),\n",
    "                                              np.mean(np.exp(z_log_var[:, analyzeArgs[\"z\"][0]])),\n",
    "                                              analyzeArgs[\"size_of_manifold\"]))\n",
    "            grid_y = np.sort(np.random.normal(np.mean(z_mean[:, analyzeArgs[\"z\"][1]]),\n",
    "                                              np.mean(np.exp(z_log_var[:, analyzeArgs[\"z\"][1]])),\n",
    "                                              analyzeArgs[\"size_of_manifold\"]))\n",
    "        elif analyzeArgs[\"sample\"] == \"range\":\n",
    "            grid_x = np.linspace(analyzeArgs[\"act_range\"][0], analyzeArgs[\"act_range\"][1],\n",
    "                                 analyzeArgs[\"size_of_manifold\"])\n",
    "            grid_y = np.linspace(analyzeArgs[\"act_range\"][0], analyzeArgs[\"act_range\"][1],\n",
    "                                 analyzeArgs[\"size_of_manifold\"])[::-1]  ## revert\n",
    "        elif analyzeArgs[\"sample\"] == \"normal\":\n",
    "            grid_x = np.sort(\n",
    "                np.random.normal(np.mean(z_mean[:, analyzeArgs[\"z\"][0]]), 1, analyzeArgs[\"size_of_manifold\"]))\n",
    "            grid_y = np.sort(\n",
    "                np.random.normal(np.mean(z_mean[:, analyzeArgs[\"z\"][1]]), 1, analyzeArgs[\"size_of_manifold\"]))\n",
    "\n",
    "        ## 1) create graph topol plots_______________________________________________\n",
    "\n",
    "        fig, axs = plt.subplots(analyzeArgs[\"size_of_manifold\"], analyzeArgs[\"size_of_manifold\"], figsize=(10, 10), dpi = 300)\n",
    "        # fig.subplots_adjust(hspace = .5, wspace=.001)\n",
    "\n",
    "        degree_sequence_original = sorted([d for n, d in g_original.degree()], reverse=True)  # degree sequence\n",
    "        degreeCount_original = collections.Counter(degree_sequence_original)\n",
    "        deg_original, cnt_original = zip(*degreeCount_original.items())\n",
    "\n",
    "        for i, yi in enumerate(grid_y):\n",
    "            for j, xi in enumerate(grid_x):\n",
    "\n",
    "                z_sample[0][analyzeArgs[\"z\"][0]] = xi ** analyzeArgs[\"act_scale\"]\n",
    "                z_sample[0][analyzeArgs[\"z\"][1]] = xi ** analyzeArgs[\"act_scale\"]\n",
    "                x_decoded = graph_decoder.predict(z_sample)\n",
    "\n",
    "                ## reconstruct upper triangular adjacency matrix\n",
    "                reconstructed_a = reconstruct_adjacency(x_decoded, dataArgs[\"clip\"], dataArgs[\"diag_offset\"])\n",
    "\n",
    "                ## reconstruct graph\n",
    "                reconstructed_a = unpad_matrix(reconstructed_a, dataArgs[\"diag_value\"], dataArgs[\"fix_n\"])\n",
    "\n",
    "                reconstructed_a = Reconstr_a[i, j]\n",
    "                g = nx.from_numpy_matrix(reconstructed_a)\n",
    "\n",
    "                ## Obtain Graph Topologies____________________________________\n",
    "\n",
    "                density_original = nx.density(g_original)\n",
    "                density = nx.density(g)\n",
    "\n",
    "                diameter_original = nx.diameter(g_original)\n",
    "                if len(g) > 0:\n",
    "                    if nx.is_connected(g):\n",
    "                        diameter = nx.diameter(g)\n",
    "                else:\n",
    "                    diameter = -1\n",
    "\n",
    "                cluster_coef_original = nx.average_clustering(g_original)\n",
    "                if len(g) > 0:\n",
    "                    cluster_coef = nx.average_clustering(g)\n",
    "                else:\n",
    "                    cluster_coef = 0\n",
    "\n",
    "                assort_original = nx.degree_assortativity_coefficient(g_original, x='out', y='in')\n",
    "                if len(g) > 0:\n",
    "                    if g.number_of_edges() > 0:\n",
    "                        assort = nx.degree_assortativity_coefficient(g, x='out', y='in')\n",
    "                else:\n",
    "                    assort = 0\n",
    "\n",
    "                edges_original = g_original.number_of_edges()\n",
    "                if len(g) > 0:\n",
    "                    edges = g.number_of_edges()\n",
    "                else:\n",
    "                    edges = 0\n",
    "\n",
    "                avg_degree_original = sum(i for i in nx.degree_centrality(g_original).values()) / len(\n",
    "                    nx.degree_centrality(g_original).keys())\n",
    "                if len(g) > 0:\n",
    "                    avg_degree = sum(i for i in nx.degree_centrality(g).values()) / len(nx.degree_centrality(g).keys())\n",
    "                else:\n",
    "                    avg_degree = 0\n",
    "\n",
    "                # compute index for the subplot, and set this subplot as current\n",
    "                plt.sca(axs[i, j])\n",
    "\n",
    "                ## create the plot_____________________________________________\n",
    "\n",
    "                ## create the plot_____________________________________________\n",
    "\n",
    "                if analyzeArgs[\"plot\"] == \"topol\":\n",
    "\n",
    "                    topol = (\"cluster_coef\", \"assort\", \"avg_degree\")\n",
    "                    colors = [\"midnightblue\", \"steelblue\", \"skyblue\"]\n",
    "\n",
    "                    y_pos = np.arange(len(topol))\n",
    "                    topol_values = [cluster_coef, assort, avg_degree]\n",
    "                    topol_values_original = [cluster_coef_original, assort_original, avg_degree_original]\n",
    "                    # plt.bar(y_pos, topol_values_original, color=colors, fill=False, align='center')\n",
    "                    plt.hlines(topol_values_original[0], -0.5, 0.5)\n",
    "                    plt.hlines(topol_values_original[1], 0.5, 1.5)\n",
    "                    plt.hlines(topol_values_original[2], 1.5, 2.5)\n",
    "                    plt.bar(y_pos, topol_values, color=colors, align='center')\n",
    "                    plt.xticks(y_pos, topol)\n",
    "\n",
    "                elif analyzeArgs[\"plot\"] == \"topol_diff\":\n",
    "\n",
    "                    topol = (\"cluster_coef\", \"assort\", \"avg_degree\")\n",
    "                    colors = [\"midnightblue\", \"steelblue\", \"skyblue\"]\n",
    "\n",
    "                    topol_values = [cluster_coef, assort, avg_degree]\n",
    "                    topol_values_original = [cluster_coef_original, assort_original, avg_degree_original]\n",
    "\n",
    "                    x_pos = np.arange(len(topol))\n",
    "                    topol_differences = (np.asarray(topol_values_original) - np.asarray(topol_values))\n",
    "\n",
    "                    plt.bar(x_pos, topol_differences, color=colors, align='center')\n",
    "                    plt.xticks(x_pos, topol)\n",
    "\n",
    "                    import matplotlib as mpl\n",
    "                    import matplotlib.cm as cm\n",
    "\n",
    "                    norm = mpl.colors.Normalize(vmin=0, vmax=1)\n",
    "                    cmap = cm.RdYlGn_r\n",
    "                    # cmap = cmap[::-1]\n",
    "\n",
    "                    m = cm.ScalarMappable(norm=norm, cmap=cmap)\n",
    "\n",
    "                    plt.ylim([-0.4, 0.4])\n",
    "                    plt.hlines(0, -0.5, 0.5, color=m.to_rgba(np.abs(float(topol_differences[0]))), lw=3.5)\n",
    "                    plt.hlines(0, 0.5, 1.5, color=m.to_rgba(np.abs(float(topol_differences[1]))), lw=3.5)\n",
    "                    plt.hlines(0, 1.5, 2.5, color=m.to_rgba(np.abs(float(topol_differences[2]))), lw=3.5)\n",
    "\n",
    "\n",
    "\n",
    "                elif analyzeArgs[\"plot\"] == \"distr\":\n",
    "\n",
    "                    degree_sequence = sorted([d for n, d in g.degree()], reverse=True)  # degree sequence\n",
    "                    degreeCount = collections.Counter(degree_sequence)\n",
    "                    deg, cnt = zip(*degreeCount.items())\n",
    "\n",
    "                    cnt = np.asarray(cnt) / sum(cnt)  # normalize degree sequence\n",
    "\n",
    "                    if len(cnt) < len(cnt_original):\n",
    "                        cnt = np.repeat(cnt, (\n",
    "                                len(cnt_original) / len(cnt)))  # stretch normalize degree sequence to match length\n",
    "                    else:\n",
    "                        cnt = np.repeat(cnt, (\n",
    "                                len(cnt) / len(cnt_original)))  # stretch normalize degree sequence to match length\n",
    "\n",
    "                    cnt_original = np.asarray(cnt_original) / sum(cnt_original)  # normalize original degree sequence\n",
    "\n",
    "                    plt.plot(cnt_original, color=\"midnightblue\", linestyle='dashed')\n",
    "                    plt.plot(cnt, color=\"steelblue\")\n",
    "\n",
    "                axs[i, j].set_axis_off()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## \"topol\", \"topol_diff\", \"distr\"\n",
    "analyzeArgs[\"plot\"] = \"topol_diff\"\n",
    "compare_topol_manifold(Reconstr_a, g_subset, a_subset, analyzeArgs, modelArgs, dataArgs, models, data, color_map, batch_size=trainArgs[\"batch_size\"])\n",
    "## cluster_coef\", \"assort\", \"avg_degree\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
