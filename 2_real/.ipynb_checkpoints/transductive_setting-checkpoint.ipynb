{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transductive Transfer Learning Setting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "## libs \n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "## Keras\n",
    "from keras.layers import Lambda, Input, Dense, Conv2D, Conv2DTranspose, Flatten, Reshape\n",
    "from keras.models import Model\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.losses import mse, binary_crossentropy\n",
    "from keras.utils import plot_model\n",
    "from keras import backend as K\n",
    "\n",
    "## Basic\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import argparse\n",
    "import os\n",
    "import random\n",
    "import itertools\n",
    "import time\n",
    "\n",
    "# Computation\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mutual_info_score\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "import scipy\n",
    "from scipy.stats.stats import pearsonr \n",
    "\n",
    "## Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "## Network Processing\n",
    "import networkx as nx\n",
    "from networkx.generators import random_graphs\n",
    "\n",
    "## node colour\n",
    "color_map = [\"steelblue\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Supporting Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "## supporting functions\n",
    "from support.preprocessing import sort_adjacency, reshape_A, calculate_A_shape, reconstruct_adjacency, pad_matrix, unpad_matrix, prepare_in_out\n",
    "from support.metrics import compute_mig, compute_mi\n",
    "from support.graph_generating import generate_single, generate_manifold, generate_topol_manifold, generate_topol_manifold\n",
    "from support.latent_space import vis2D, visDistr\n",
    "from support.comparing import compare_manifold_adjacency, compare_topol_manifold\n",
    "\n",
    "\n",
    "## graph sampling\n",
    "from sampling import ForestFire, Metropolis_Hastings, Random_Walk, Snowball, Ties, Base_Samplers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probabilistic Graph Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Graph Instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def get_graph(dataArgs, trainArgs): \n",
    "    \n",
    "    ## Generate Graph Type ______________________________________________\n",
    "    \n",
    "    graph_type = random.choice(trainArgs[\"input_networks\"])\n",
    "    params = np.zeros((trainArgs[\"n_params\"]))\n",
    "\n",
    "    if dataArgs[\"fix_n\"] == True:\n",
    "        n = dataArgs[\"n_max\"] # generate fixed number of nodes n_max\n",
    "    else:\n",
    "        n = random.randint(2, dataArgs[\"n_max\"]) # generate number of nodes n between 1 and n_max and\n",
    "            \n",
    "    if graph_type == \"ER\":\n",
    "        \n",
    "        p = np.random.rand(1)  # float in range 0 - 1 \n",
    "        g = random_graphs.erdos_renyi_graph(n, p, seed=None, directed=False)\n",
    "        \n",
    "        params[0] = 1\n",
    "        params[1] = p\n",
    "        \n",
    "        \n",
    "    if graph_type == \"BA\":\n",
    "        \n",
    "        e = np.clip(random.randint(1, max(1, n - 1)), 1, n-1) # generate number of nodes n between 1 and n_max and\n",
    "        g = random_graphs.barabasi_albert_graph(n, e, seed=None)\n",
    "        \n",
    "        params[2] = 1\n",
    "        params[3] = e\n",
    "        \n",
    "        \n",
    "    if graph_type == \"SW\":\n",
    "        \n",
    "        k = random.randint(0, n - 1)\n",
    "        p = np.random.rand(1)  # float in range 0 - 1 \n",
    "        g = random_graphs.newman_watts_strogatz_graph(n, k, p, seed=None) # no edges are removed\n",
    "        \n",
    "        params[4] = 1\n",
    "        params[5] = k\n",
    "        params[6] = p\n",
    "    \n",
    "    return g, params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "dataArgs = {\"n_max\": 24, \"n_lower\": 1, \"n_upper\": 24, \"iter_n\": True, \"fix_n\": False, \"diag_offset\": 0, \"diag_value\": 1, \"clip\": True}\n",
    "                                                                        # none, exact_n, approx_n             #\"diag_offset\" - 1, 0, 1\n",
    "trainArgs = {\"input_networks\": [\"ER\", \"BA\", \"SW\"], \"n_graphs\": 1000, \"n_params\": 7}    \n",
    "    \n",
    "    \n",
    "def generate_data(dataArgs): \n",
    "    \n",
    "    \n",
    "    ## Data ________________________________\n",
    "\n",
    "    G = np.zeros((trainArgs[\"n_graphs\"], *calculate_A_shape(dataArgs[\"n_max\"], diag_offset = dataArgs[\"diag_offset\"])))\n",
    "    Params_arr = np.zeros((trainArgs[\"n_graphs\"], trainArgs[\"n_params\"]))\n",
    "    Params = list()\n",
    "\n",
    "    ## Generate Graph Data_______________________________\n",
    "\n",
    "    for i in tqdm(range(0,trainArgs[\"n_graphs\"])):\n",
    "\n",
    "        g, params = get_graph(dataArgs, trainArgs)\n",
    "\n",
    "        g, a = sort_adjacency(g)\n",
    "        a = pad_matrix(a, dataArgs[\"n_max\"], dataArgs[\"diag_value\"])  # pad adjacency matrix to allow less nodes than n_max and fill diagonal\n",
    "        a_transformed = reshape_A(a, diag_offset = dataArgs[\"diag_offset\"])        \n",
    "\n",
    "    \n",
    "        ## Build Data Arrays___________________________________________________\n",
    "\n",
    "        G[i] = a_transformed\n",
    "        Params_arr[i] = params\n",
    "        \n",
    "        param_dict = dict()\n",
    "        param_dict[\"ER\"] = params[0]\n",
    "        param_dict[\"p1\"] = params[1]\n",
    "        param_dict[\"BA\"] = params[2]\n",
    "        param_dict[\"e\"] = params[3]\n",
    "        param_dict[\"SW\"] = params[4]\n",
    "        param_dict[\"k\"] = params[5]\n",
    "        param_dict[\"p2\"] = params[6]\n",
    "\n",
    "        Params.append(param_dict)\n",
    "    \n",
    "\n",
    "    ## Input and Output Size ___________________________________________________________\n",
    "\n",
    "    Params, input_shape, output_shape = prepare_in_out(Params, dataArgs[\"diag_offset\"], calculate_A_shape(dataArgs[\"n_max\"], dataArgs[\"diag_offset\"]))\n",
    "    print(\"input_shape:\", input_shape, \", output_shape:\", output_shape)\n",
    "    \n",
    "    ## scale parameters in T_array for smoother training\n",
    "    scaler = MinMaxScaler()\n",
    "    scaler.fit(Params_arr)\n",
    "    Params_arr = scaler.transform(Params_arr)\n",
    "    \n",
    "    return G,Params,Params_arr,input_shape,output_shape,scaler\n",
    "    \n",
    "G, Params, Params_arr, input_shape, output_shape, scaler = generate_data(dataArgs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# beta-VAE (MLP, Conv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "## libs\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "## Keras\n",
    "from keras.layers import Lambda, Input, Dense, Conv2D, Conv2DTranspose, Flatten, Reshape\n",
    "from keras.models import Model\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.losses import mse, binary_crossentropy\n",
    "from keras.utils import plot_model\n",
    "from keras import backend as K\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "class VAE():\n",
    "\n",
    "    # reparameterization trick\n",
    "    # instead of sampling from Q(z|X), sample eps = N(0,I)\n",
    "    # then z = z_mean + sqrt(var)*eps\n",
    "\n",
    "    def sampling(self, args):\n",
    "        \"\"\"Reparameterization trick by sampling fr an isotropic unit Gaussian.\n",
    "        # Arguments\n",
    "            args (tensor): mean and log of variance of Q(z|X)\n",
    "        # Returns\n",
    "            z (tensor): sampled latent vector\n",
    "        \"\"\"\n",
    "\n",
    "        z_mean, z_log_var = args\n",
    "        batch = K.shape(z_mean)[0]\n",
    "        dim = K.int_shape(z_mean)[1]\n",
    "        # by default, random_normal has mean=0 and std=1.0\n",
    "        epsilon = K.random_normal(shape=(batch, dim))\n",
    "        return z_mean + K.exp(0.5 * z_log_var) * epsilon\n",
    "\n",
    "    def __init__(self, modelArgs, trainArgs, G, T_array):\n",
    "\n",
    "        ## MODEL ______________________________________________________________\n",
    "\n",
    "        ## Multi-layer Perceptron without convolutions__________________________________\n",
    "        if modelArgs[\"nn_architecture\"] == \"mlp\":\n",
    "            ## 1) build encoder model __________________________\n",
    "\n",
    "            inputs = Input(shape=modelArgs[\"input_shape\"], name='encoder_input')\n",
    "            x = Dense(128, activation='relu')(inputs)\n",
    "            x = Dense(64, activation='relu')(x)\n",
    "            z_mean = Dense(modelArgs[\"latent_dim\"], name='z_mean')(x)\n",
    "            z_log_var = Dense(modelArgs[\"latent_dim\"], name='z_log_var')(x)\n",
    "\n",
    "            # use reparameterization trick to push the sampling out as input\n",
    "            # note that \"output_shape\" isn't necessary with the TensorFlow backend\n",
    "            z = Lambda(self.sampling, output_shape=(modelArgs[\"latent_dim\"],), name='z')([z_mean, z_log_var])\n",
    "\n",
    "            ## 2) build decoder model __________________________\n",
    "\n",
    "            latent_inputs = Input(shape=(modelArgs[\"latent_dim\"],), name='z_sampling')\n",
    "            y = Dense(64, activation='relu')(latent_inputs)\n",
    "            y = Dense(128, activation='relu')(y)\n",
    "            graph_outputs = Dense(modelArgs[\"output_shape\"], activation='sigmoid')(y)\n",
    "\n",
    "        ## Convolutional Neural Network_________________________________\n",
    "\n",
    "        if modelArgs[\"nn_architecture\"] == \"2D_conv\":\n",
    "\n",
    "            ## 1) build encoder model____________________________________\n",
    "\n",
    "            inputs = Input(shape=modelArgs[\"input_shape\"], name='encoder_input')\n",
    "            x = inputs\n",
    "\n",
    "            for i in range(2):\n",
    "                modelArgs['filters'] *= 2\n",
    "                x = Conv2D(filters=modelArgs['filters'], kernel_size=modelArgs['kernel_size'], activation='relu',\n",
    "                           strides=2, padding='same')(x)\n",
    "\n",
    "            # shape info needed to build decoder model\n",
    "            shape = K.int_shape(x)\n",
    "\n",
    "            # generate latent vector Q(z|X)\n",
    "            x = Flatten()(x)\n",
    "            x = Dense(16, activation='relu')(x)\n",
    "            z_mean = Dense(modelArgs[\"latent_dim\"], name='z_mean')(x)\n",
    "            z_log_var = Dense(modelArgs[\"latent_dim\"], name='z_log_var')(x)\n",
    "\n",
    "            # use reparameterization trick to push the sampling out as input\n",
    "            # note that \"output_shape\" isn't necessary with the TensorFlow backend\n",
    "            z = Lambda(self.sampling, output_shape=(modelArgs[\"output_shape\"],), name='z')([z_mean, z_log_var])\n",
    "\n",
    "            ## 2) build decoder model____________________________________\n",
    "\n",
    "            latent_inputs = Input(shape=(modelArgs[\"latent_dim\"],), name='z_sampling')\n",
    "            x = Dense(shape[1] * shape[2] * shape[3], activation='relu')(latent_inputs)\n",
    "            x = Reshape((shape[1], shape[2], shape[3]))(x)\n",
    "\n",
    "            for i in range(2):\n",
    "                x = Conv2DTranspose(filters=modelArgs['filters'], kernel_size=modelArgs['kernel_size'],\n",
    "                                    activation='relu', strides=2, padding='same')(x)\n",
    "                modelArgs['filters'] //= 2\n",
    "\n",
    "            graph_outputs = Conv2DTranspose(filters=1, kernel_size=modelArgs['kernel_size'], activation='sigmoid',\n",
    "                                            padding='same', name='decoder_output')(x)\n",
    "\n",
    "        ## 2.2) decode growth parameters_________________________________\n",
    "\n",
    "        # g = Dense(4, activation='relu')(latent_inputs)\n",
    "        g = Dense(4, activation='relu')(latent_inputs)\n",
    "        g = Dense(6, activation='relu')(g)\n",
    "        param_outputs = Dense(modelArgs[\"growth_param\"], activation='linear')(g)\n",
    "\n",
    "\n",
    "        ## 1) instantiate encoder model\n",
    "        encoder = Model(inputs, [z_mean, z_log_var, z], name='encoder')\n",
    "        encoder.summary()\n",
    "        # plot_model(encoder, to_file='vae_mlp_encoder.png', show_shapes=True)\n",
    "\n",
    "        ## 2) instantiate decoder model\n",
    "        graph_decoder = Model(latent_inputs, graph_outputs, name='graph_decoder')\n",
    "        graph_decoder.summary()\n",
    "        # plot_model(decoder, to_file='vae_mlp_decoder.png', show_shapes=True)\n",
    "\n",
    "        ## 2.2) decode growth parameters\n",
    "        param_decoder = Model(latent_inputs, param_outputs, name='param_decoder')\n",
    "        param_decoder.summary()\n",
    "\n",
    "        ## 3) instantiate VAE model\n",
    "        graph_outputs = graph_decoder(encoder(inputs)[2])\n",
    "        param_outputs = param_decoder(encoder(inputs)[2])\n",
    "        outputs = [graph_outputs, param_outputs]\n",
    "\n",
    "        vae = Model(inputs, outputs, name='vae_graph')\n",
    "        # vae.summary()\n",
    "\n",
    "        ## TRAINING ______________________________________\n",
    "\n",
    "        ## Train and Validation Split _______________________________________________\n",
    "\n",
    "        x_train, x_test, y_train, y_test = train_test_split(G, T_array, test_size=trainArgs[\"data_split\"],\n",
    "                                                            random_state=1, shuffle=True)\n",
    "\n",
    "        def reconstr_loss_func(y_true, y_pred):\n",
    "\n",
    "            ## RECONSTRUCTION LOSS_______________________\n",
    "\n",
    "            if trainArgs[\"loss\"] == \"mse\":\n",
    "\n",
    "                if modelArgs[\"nn_architecture\"] == \"mlp\":\n",
    "                    reconstruction_loss = mse(y_true[0], y_pred[0])\n",
    "                    reconstruction_loss *= modelArgs[\"input_shape\"]\n",
    "\n",
    "                if modelArgs[\"nn_architecture\"] == \"2D_conv\":\n",
    "                    reconstruction_loss = mse(K.flatten(y_true[0]), K.flatten(y_pred[0]))\n",
    "                    reconstruction_loss *= modelArgs[\"input_shape\"][0] * modelArgs[\"input_shape\"][1]\n",
    "\n",
    "            if trainArgs[\"loss\"] == \"binary_crossentropy\":\n",
    "\n",
    "                if modelArgs[\"nn_architecture\"] == \"mlp\":\n",
    "                    reconstruction_loss = binary_crossentropy(y_true[0], y_pred[0])\n",
    "                    reconstruction_loss *= modelArgs[\"input_shape\"]\n",
    "\n",
    "                if modelArgs[\"nn_architecture\"] == \"2D_conv\":\n",
    "                    reconstruction_loss = binary_crossentropy(K.flatten(y_true[0]), K.flatten(y_pred[0]))\n",
    "                    reconstruction_loss *= modelArgs[\"input_shape\"][0] * modelArgs[\"input_shape\"][1]\n",
    "\n",
    "            ## KL LOSS _____________________________________________\n",
    "\n",
    "            kl_loss = 1 + z_log_var - K.square(z_mean) - K.exp(z_log_var)\n",
    "            kl_loss = K.sum(kl_loss, axis=-1)\n",
    "            kl_loss *= -0.5\n",
    "\n",
    "            ## COMPLETE LOSS __________________________________________________\n",
    "\n",
    "            reconstr_loss = K.mean(reconstruction_loss + (trainArgs[\"beta\"] * kl_loss))\n",
    "\n",
    "            return reconstr_loss\n",
    "\n",
    "        # vae.add_loss(vae_loss)\n",
    "        # vae_loss = reconstr_loss_func(inputs, outputs[0])\n",
    "\n",
    "        ## PARAMETER LOSS_______________________\n",
    "\n",
    "        def param_loss_func(y_true, y_pred):\n",
    "\n",
    "            param_loss = mse(y_pred[1], y_true[1])\n",
    "\n",
    "            return param_loss\n",
    "\n",
    "        \n",
    "        vae.compile(optimizer='adam', loss=[reconstr_loss_func, param_loss_func], metrics=['accuracy'])\n",
    "        vae.summary()\n",
    "\n",
    "        ## TRAIN______________________________________________\n",
    "\n",
    "        # Set callback functions to early stop training and save the best model so far\n",
    "        callbacks = [EarlyStopping(monitor='val_loss', patience=trainArgs[\"early_stop\"]), ModelCheckpoint(\n",
    "            filepath=\"models/weights/vae_mlp_mnist_latent_dim_\" + str(modelArgs[\"latent_dim\"]) + \".h5\",\n",
    "            save_best_only=True)]\n",
    "\n",
    "        vae.fit(x_train, [x_train, y_train], epochs=trainArgs[\"epochs\"], batch_size=trainArgs[\"batch_size\"],\n",
    "                callbacks=callbacks, validation_data=(x_test, [x_test, y_test]))\n",
    "        vae.save_weights(\"models/weights/vae_mlp_mnist_latent_dim_\" + str(modelArgs[\"latent_dim\"]) + \".h5\")\n",
    "        models = (encoder, graph_decoder, param_decoder)\n",
    "\n",
    "        data = (x_test, y_test)\n",
    "        \n",
    "        self.model = (encoder, param_decoder)\n",
    "        self.data = data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     13,
     23,
     58
    ]
   },
   "outputs": [],
   "source": [
    "# network parameters\n",
    "modelArgs = {\"nn_architecture\": \"mlp\", \"latent_dim\": 2, \"growth_param\": Params_arr.shape[1], \"filters\": 16, \"kernel_size\": 3, \"input_shape\": input_shape, \"output_shape\": output_shape}\n",
    "trainArgs = {\"beta\": 1, \"loss\": \"binary_crossentropy\", \"weights\": \"train\", \"early_stop\": 3, \"batch_size\": 64, \"epochs\": 50, \"data_split\": 0.2}\n",
    "\n",
    "vae = VAE(modelArgs, trainArgs, G, Params_arr)\n",
    "\n",
    "model = vae.model \n",
    "data = vae.data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Real-World Graph Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## load graph data\n",
    "g_complete = nx.karate_club_graph()\n",
    "\n",
    "n_complete = len(g_complete)\n",
    "e_complete = len(g_complete.edges())\n",
    "a_complete = nx.adjacency_matrix(g_complete)\n",
    "max_degree = max([d for n, d in g_complete.degree()])\n",
    "\n",
    "print(\"number of nodes:\", n_complete)\n",
    "print(\"number of edges:\", e_complete)\n",
    "print(\"max_degree:\", max_degree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample Subgraph and Pre-Process\n",
    "\n",
    "**exact_n:** biased_random_walk, bfs, forestfire, random_walk_induced_graph_sampling, random_walk_sampling_with_fly_back, adjacency, select\n",
    "\n",
    "**approx_n:**  snowball, standard_bfs, walk, jump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     5
    ]
   },
   "outputs": [],
   "source": [
    "sampleArgs = {\"sample\": \"biased_random_walk\", \"jump_bias\": \"random_walk_induced_graph_sampling\", \"n\": 10, \"p\": 20.0, \"q\": 100.0, \"source_starts\": 2, \"source_returns\": 4, \"depth\": 2}\n",
    "\n",
    "##exact_n: forestfire, random_walk_induced_graph_sampling, random_walk_sampling_with_fly_back, adjacency, select\n",
    "##approx_n: snowball, bfs, walk, jump\n",
    "\n",
    "def get_graph(sampleArgs,g_complete,a_complete):\n",
    "    \n",
    "    if sampleArgs[\"sample\"] == \"biased_random_walk\":\n",
    "        sampler = Base_Samplers.Base_Samplers(g_complete,a_complete)\n",
    "        g = sampler.biased_random_walk(sampleArgs[\"n\"], sampleArgs[\"p\"], sampleArgs[\"q\"])\n",
    "\n",
    "    if sampleArgs[\"sample\"] == \"forestfire\":\n",
    "        sampler = ForestFire.ForestFire(g_complete,a_complete)\n",
    "        g = sampler.forestfire(sampleArgs[\"n\"])\n",
    "\n",
    "    if sampleArgs[\"sample\"] == \"snowball\":\n",
    "        sampler = Snowball.Snowball(g_complete,a_complete)\n",
    "        g = sampler.snowball(sampleArgs[\"source_starts\"], sampleArgs[\"source_returns\"])\n",
    "\n",
    "    if sampleArgs[\"sample\"] == \"random_walk_induced_graph_sampling\":\n",
    "        sampler = Random_Walk.Random_Walk(g_complete,a_complete)\n",
    "        g = sampler.random_walk_induced_graph_sampling(sampleArgs[\"n\"])\n",
    "\n",
    "    if sampleArgs[\"sample\"] == \"random_walk_sampling_with_fly_back\":\n",
    "        sampler = Random_Walk.Random_Walk(g_complete,a_complete)\n",
    "        g = sampler.random_walk_sampling_with_fly_back(sampleArgs[\"n\"], sampleArgs[\"p\"])\n",
    "        \n",
    "    if sampleArgs[\"sample\"] == \"standard_bfs\":\n",
    "        sampler = Base_Samplers.Base_Samplers(g_complete,a_complete)\n",
    "        g = sampler.standard_bfs(sampleArgs[\"source_starts\"], sampleArgs[\"depth\"]) \n",
    "        \n",
    "    if sampleArgs[\"sample\"] == \"bfs\":\n",
    "        sampler = Base_Samplers.Base_Samplers(g_complete,a_complete)\n",
    "        g = sampler.bfs(sampleArgs[\"n\"]) \n",
    "        \n",
    "    if sampleArgs[\"sample\"] == \"walk\":\n",
    "        sampler = Base_Samplers.Base_Samplers(g_complete,a_complete)\n",
    "        g = sampler.walk(sampleArgs[\"source_starts\"], sampleArgs[\"source_returns\"], sampleArgs[\"p\"])        \n",
    "        \n",
    "    if sampleArgs[\"sample\"] == \"jump\":\n",
    "        sampler = Base_Samplers.Base_Samplers(g_complete,a_complete)\n",
    "        g = sampler.jump(sampleArgs[\"source_starts\"], sampleArgs[\"p\"], sampleArgs[\"jump_bias\"])\n",
    "        \n",
    "    if sampleArgs[\"sample\"] == \"adjacency\":\n",
    "        sampler = Base_Samplers.Base_Samplers(g_complete,a_complete)\n",
    "        g = sampler.adjacency(sampleArgs[\"n\"]) \n",
    "        \n",
    "    if sampleArgs[\"sample\"] == \"select\":\n",
    "        sampler = Base_Samplers.Base_Samplers(g_complete,a_complete)\n",
    "        g = sampler.adjacency(sampleArgs[\"n\"]) \n",
    "    \n",
    "    return g \n",
    "\n",
    "start_time = time.time()\n",
    "g = get_graph(sampleArgs,g_complete,a_complete)\n",
    "\n",
    "print(\"-- n_max should be >=\", len(g), \"--\")\n",
    "print(\"-- function get_graph takes %s secs --\" % round((time.time() - start_time),  5))\n",
    "\n",
    "if len(g) <= 200:\n",
    "    nx.draw(g, node_color = color_map, with_labels = False)\n",
    "    \n",
    "g, a = sort_adjacency(g)\n",
    "a = pad_matrix(a, dataArgs[\"n_max\"], dataArgs[\"diag_value\"])  # pad adjacency matrix to allow less nodes than n_max and fill diagonal\n",
    "a_transformed = reshape_A(a, diag_offset = dataArgs[\"diag_offset\"]) \n",
    "a_transformed = np.reshape(a_transformed, (1,a_transformed.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict Model Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ENCODER\n",
    "encoder, param_decoder = model  ## trained model parts\n",
    "\n",
    "z_mean, z_log_var, z = encoder.predict(a_transformed, batch_size = trainArgs[\"batch_size\"])\n",
    "\n",
    "\n",
    "## DECODER\n",
    "\n",
    "decoded_param = param_decoder.predict(z, batch_size = trainArgs[\"batch_size\"])\n",
    "decoded_param = np.squeeze(decoded_param)\n",
    "print(decoded_param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "param_txt = list(Params[0].keys())\n",
    "param_txt = [\"0\"] + param_txt\n",
    "ax.set_xticklabels(param_txt)\n",
    "x = np.arange(0,7)\n",
    "rects1 = ax.bar(x, decoded_param, color='b')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
