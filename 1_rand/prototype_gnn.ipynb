{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prototype - GNN on ER Random Graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "## Basic\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import argparse\n",
    "import os\n",
    "import random\n",
    "import itertools\n",
    "\n",
    "# Computation\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mutual_info_score\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "import scipy\n",
    "from scipy.stats.stats import pearsonr \n",
    "\n",
    "## Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "## Network Processing\n",
    "import networkx as nx\n",
    "from networkx.generators import random_graphs\n",
    "\n",
    "## node colour\n",
    "color_map = [\"steelblue\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Supporting Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "## supporting functions\n",
    "from support.preprocessing import sort_adjacency, reshape_A, calculate_A_shape, reconstruct_adjacency, pad_matrix, unpad_matrix, prepare_in_out\n",
    "from support.metrics import compute_mig, compute_mi\n",
    "from support.graph_generating import generate_single, generate_manifold, generate_topol_manifold, generate_topol_manifold\n",
    "from support.param_generating import generate_param_graph_manifold, generate_param_topol_manifold\n",
    "from support.latent_space import vis2D, visDistr\n",
    "\n",
    "## import model\n",
    "from models.VAE import VAE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and Preprocessing Network Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Network Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def get_graph(n,p,draw): \n",
    "\n",
    "    g = random_graphs.erdos_renyi_graph(n, p, seed=None, directed=False)\n",
    "\n",
    "    if draw:\n",
    "        nx.draw(g, node_color = color_map, with_labels = True)\n",
    "        plt.show()\n",
    "    \n",
    "    return g\n",
    "\n",
    "g = get_graph(n = 10, p = 0.4, draw = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def generate_data(dataArgs): \n",
    "    \n",
    "    ## Data ________________________________\n",
    "\n",
    "    G = np.zeros((dataArgs[\"n_graphs\"], *calculate_A_shape(dataArgs[\"n_max\"], diag_offset = dataArgs[\"diag_offset\"])))\n",
    "    F = np.zeros((dataArgs[\"n_graphs\"], dataArgs[\"n_max\"], dataArgs[\"n_features\"]))\n",
    "    Y = np.zeros((dataArgs[\"n_graphs\"], 2))\n",
    "    \n",
    "    ## Ground Truth Labels ______________________________\n",
    "\n",
    "    T = list()\n",
    "    T_array = np.zeros((dataArgs[\"n_graphs\"],2))\n",
    "\n",
    "    ## Generate Graph Data_______________________________\n",
    "\n",
    "    for i in tqdm(range(0,dataArgs[\"n_graphs\"])):\n",
    "\n",
    "        ## Generate Graph Type ______________________________________________\n",
    "\n",
    "        if dataArgs[\"fix_n\"] == True:\n",
    "            n = dataArgs[\"n_max\"] # generate fixed number of nodes n_max\n",
    "        else:\n",
    "            n = random.randint(1, dataArgs[\"n_max\"]) # generate number of nodes n between 1 and n_max and\n",
    "\n",
    "        p = np.random.rand(1)  # float in range 0 - 1 \n",
    "        g = get_graph(n, p, draw = False)\n",
    "\n",
    "        g, a = sort_adjacency(g)\n",
    "        a = pad_matrix(a, dataArgs[\"n_max\"], dataArgs[\"diag_value\"])  # pad adjacency matrix to allow less nodes than n_max and fill diagonal\n",
    "        a_transformed = reshape_A(a, diag_offset = dataArgs[\"diag_offset\"])\n",
    "\n",
    "\n",
    "        ## Build Data Arrays___________________________________________________\n",
    "\n",
    "        G[i] = a_transformed\n",
    "        #F[i] = np.random.rand(dataArgs[\"n_max\"],dataArgs[\"n_features\"]) ## randomly initialize node features\n",
    "        F[i] = np.random.randint(2, size=(dataArgs[\"n_max\"],dataArgs[\"n_features\"]))\n",
    "        Y[i] = [0,1]\n",
    "\n",
    "        t = dict()\n",
    "        t[\"n\"] = n\n",
    "        t[\"p\"] = p\n",
    "        \n",
    "        T_array[i] = [n,p]\n",
    "        T.append(t)\n",
    "\n",
    "\n",
    "\n",
    "    ## Input and Output Size ___________________________________________________________\n",
    "\n",
    "    T, input_shape, output_shape = prepare_in_out(T, dataArgs[\"diag_offset\"], calculate_A_shape(dataArgs[\"n_max\"], dataArgs[\"diag_offset\"]))\n",
    "    print(\"input_shape:\", input_shape, \", output_shape:\", output_shape)\n",
    "    \n",
    "    ## scale parameters in T_array for smoother training\n",
    "    scaler = MinMaxScaler()\n",
    "    scaler.fit(T_array)\n",
    "    T_array = scaler.transform(T_array)\n",
    "    \n",
    "    return G,T,T_array,F,input_shape,output_shape,scaler, Y\n",
    "    \n",
    "dataArgs = {\"n_graphs\": 10000, \"n_max\": 24, \"fix_n\": True, \"diag_offset\": -1, \"diag_value\": 0, \"clip\": True, \"n_features\": 16}  #\"diag_offset\" - 1 == full adjacency\n",
    "G, T, T_array, F, input_shape, output_shape,scaler, Y = generate_data(dataArgs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# beta-VAE (MLP, Conv, GNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build and Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# network parameters\n",
    "modelArgs = {\"nn_architecture\": \"2D_conv\", \"param_loss\": True, \"latent_dim\": 2, \"growth_param\": T_array.shape[1], \"filters\": 16, \"kernel_size\": 3, \"input_shape\": input_shape, \"output_shape\": output_shape}\n",
    "trainArgs = {\"beta\": 20, \"loss\": \"binary_crossentropy\", \"weights\": \"train\", \"early_stop\": 2, \"batch_size\": 128, \"epochs\": 50, \"data_split\": 0.2}\n",
    "\n",
    "vae = VAE(modelArgs, trainArgs, G, F, T_array)\n",
    "\n",
    "models = vae.model \n",
    "data = vae.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Latent Space Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iterate through single data dimension and oberseve single latent space dimension  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     46
    ]
   },
   "outputs": [],
   "source": [
    "def latent_space_feature_correlation(analyzeArgs, modelArgs, models,batch_size=128,model_name=\"vae_graph\"):\n",
    "\n",
    "    if modelArgs[\"param_loss\"]:\n",
    "        encoder, graph_decoder, param_decoder = models  # trained models\n",
    "    else:\n",
    "        encoder, graph_decoder = models  # trained models\n",
    "    \n",
    "    if analyzeArgs[\"root_params\"] == 1 or modelArgs[\"latent_dim\"] == 1:\n",
    "        \n",
    "        ## Generate Graph Data_______________________________\n",
    "            \n",
    "        P = np.linspace(0,1,analyzeArgs[\"n_config_graphs\"])  # array 0.1, 0.2 - 1 / n_config_graphs  \n",
    "        n = dataArgs[\"n_max\"]\n",
    "        \n",
    "        ## growth and topol parameters\n",
    "        growth_topol_params = [\"p\",\"density\", \"diameter\", \"cluster_coef\", \"assort\", \"#edges\", \"avg_degree\"]\n",
    "        \n",
    "        ## store graphs and targets\n",
    "        # shape: n_config_graphs, params, upper_A_size\n",
    "        G = np.zeros((analyzeArgs[\"n_config_graphs\"], *calculate_A_shape(dataArgs[\"n_max\"], dataArgs[\"diag_offset\"])))\n",
    "        Growth_Topol = np.zeros((analyzeArgs[\"n_config_graphs\"], len(growth_topol_params)))\n",
    "    \n",
    "        for i, p in enumerate(P):\n",
    "\n",
    "            ## Generate Graph Type ______________________________________________\n",
    "\n",
    "            g = get_graph(int(n), p, draw = False)\n",
    "\n",
    "            g, a = sort_adjacency(g)\n",
    "            a = pad_matrix(a, dataArgs[\"n_max\"], dataArgs[\"diag_value\"])  # pad adjacency matrix to allow less nodes than n_max and fill diagonal\n",
    "            upper_a = reshape_A(a, dataArgs[\"diag_offset\"])\n",
    "\n",
    "\n",
    "            ## Generate Ground Truth features____________________________________\n",
    "\n",
    "            density = nx.density(g)\n",
    "\n",
    "            if nx.is_connected(g):\n",
    "                diameter = nx.diameter(g)\n",
    "            else:\n",
    "                diameter = -1\n",
    "\n",
    "            cluster_coef = nx.average_clustering(g)\n",
    "\n",
    "            if g.number_of_edges() > 0:\n",
    "                assort = nx.degree_assortativity_coefficient(g, x='out', y='in')\n",
    "            else:\n",
    "                assort = 0\n",
    "\n",
    "            edges = g.number_of_edges()\n",
    "\n",
    "            avg_degree = sum(i for i in nx.degree_centrality(g).values()) / len(nx.degree_centrality(g).keys())\n",
    "\n",
    "\n",
    "            ## toDO: add more graph topologies\n",
    "\n",
    "            ## Build Data Arrays___________________________________________________\n",
    "\n",
    "            G[i] = upper_a\n",
    "\n",
    "            Growth_Topol[i,0] = p\n",
    "            Growth_Topol[i,1] = density\n",
    "            Growth_Topol[i,2] = diameter\n",
    "            Growth_Topol[i,3] = cluster_coef\n",
    "            Growth_Topol[i,4] = assort\n",
    "            Growth_Topol[i,5] = edges\n",
    "            Growth_Topol[i,6] = avg_degree\n",
    "\n",
    "  \n",
    "    \n",
    "        ## ENCODER - 2D Digit Classes ______________________________________________\n",
    "\n",
    "        # display a 2D plot of the digit classes in the latent space\n",
    "        z_mean, _, _ = encoder.predict(G, batch_size = batch_size)\n",
    "        \n",
    "        \n",
    "        ## Measure the Mutual Information Gap ____________________________________________\n",
    "        if analyzeArgs[\"metric\"] == \"mig\":\n",
    "            mig = compute_mig(P, np.squeeze(z_mean))\n",
    "            \n",
    "        \n",
    "        ## Visualize Latent Variables x Graph Properties ____________________________\n",
    "        fig, ax = plt.subplots(nrows= z_mean.shape[1], ncols= Growth_Topol.shape[1], figsize=(20, 10))\n",
    "\n",
    "        for latent_z, row in enumerate(ax):  \n",
    "            \n",
    "            if z_mean.shape[1] == 1:   # only one latent variable\n",
    "                \n",
    "                if latent_z == 0:\n",
    "                    y = z_mean[:,0]\n",
    "                    x = Growth_Topol[:,latent_z]\n",
    "                    row.plot(x, y) \n",
    "\n",
    "                else:\n",
    "                    y = z_mean[:,0]\n",
    "                    x = Growth_Topol[:,latent_z]\n",
    "                    #row.scatter(x, y) \n",
    "                    sns.regplot(x, y, color=\"steelblue\", ax=row)\n",
    "\n",
    "                    ## plot trend line\n",
    "                    #x = np.nan_to_num(x)\n",
    "                    #y = np.nan_to_num(y)\n",
    "\n",
    "                    #z = np.polyfit(x, y, 1)\n",
    "                    #p = np.poly1d(z)\n",
    "                    #row.plot(x,p(x),\"steelblue\")\n",
    "                    \n",
    "                ## compute correlation and standardized covariance\n",
    "                corr = round(pearsonr(x,y)[0],3)\n",
    "                cov = round(np.cov(x, y)[0][1]/max(x),3)\n",
    "                row.annotate(\"corr:\"+str(corr)+\", cov:\"+str(cov), xy=(0, 1), xytext=(12, -12), va='top',xycoords='axes fraction', textcoords='offset points')\n",
    "\n",
    "                    \n",
    "            else:                     # multiple latent variables\n",
    "                \n",
    "                for feature, col in enumerate(row):\n",
    "\n",
    "                    if feature == 0:\n",
    "                        y = z_mean[:,latent_z]\n",
    "                        x = Growth_Topol[:,feature]\n",
    "                        col.plot(x, y) \n",
    "\n",
    "                    else:\n",
    "                        y = z_mean[:,latent_z]\n",
    "                        x = Growth_Topol[:,feature]\n",
    "                        #col.scatter(x, y) \n",
    "                        sns.regplot(x, y, color=\"steelblue\", ax=col)\n",
    "\n",
    "                        ## plot trend line\n",
    "                        #x = np.nan_to_num(x)\n",
    "                        #y = np.nan_to_num(y)\n",
    "\n",
    "                        #z = np.polyfit(x, y, 1)\n",
    "                        #p = np.poly1d(z)\n",
    "                        #col.plot(x,p(x),\"steelblue\")\n",
    "            \n",
    "                \n",
    "                    ## compute correlation and standardized covariance\n",
    "                    corr = round(pearsonr(x,y)[0],3)\n",
    "                    cov = round(np.cov(x, y)[0][1]/max(x),3)\n",
    "                    col.annotate(\"corr:\"+str(corr)+\", cov:\"+str(cov), xy=(0, 1), xytext=(12, -12), va='top',xycoords='axes fraction', textcoords='offset points')\n",
    "\n",
    "\n",
    "\n",
    "        ## add row and column titles _____________________\n",
    "        \n",
    "        if z_mean.shape[1] == 1:   # only one latent variable\n",
    "                \n",
    "            cols = [t for t in growth_topol_params]\n",
    "            \n",
    "            for axis, col in zip(ax[:,], cols):\n",
    "                axis.set_title(col, fontweight='bold')\n",
    "                \n",
    "        \n",
    "        if z_mean.shape[1] != 1:   # more than one latent variable\n",
    "            \n",
    "            rows = ['z_{}'.format(row) for row in range(z_mean.shape[-1])]\n",
    "            cols = [t for t in growth_topol_params]\n",
    "\n",
    "            for axis, row in zip(ax[:,0], rows):\n",
    "                axis.set_ylabel(row, rotation=0, size='large', fontweight='bold')\n",
    "            \n",
    "            for axis, col in zip(ax[0], cols):\n",
    "                axis.set_title(col, fontweight='bold')\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "\n",
    "    if analyzeArgs[\"root_params\"] == 2 and modelArgs[\"latent_dim\"] != 1:\n",
    "        \n",
    "        ## Generate Graph Data_______________________________\n",
    "    \n",
    "        N = np.linspace(1,dataArgs[\"n_max\"],analyzeArgs[\"n_config_graphs\"], dtype=int)  # array 1,2,3,4,5 - n_max / n_config_graphs\n",
    "        P = np.linspace(0,1,analyzeArgs[\"n_config_graphs\"])  # array 0.1, 0.2 - 1 / n_config_graphs \n",
    "        \n",
    "        ## growth and topol parameters\n",
    "        growth_params = [\"p\", \"n\"]\n",
    "        topol_params = [\"density\", \"diameter\", \"cluster_coef\", \"assort\", \"#edges\", \"avg_degree\"]\n",
    "\n",
    "        ## store graphs and targets\n",
    "        # shape: n_config_graphs, params, upper_A_size\n",
    "        G = np.zeros((analyzeArgs[\"n_config_graphs\"]**len(growth_params), *calculate_A_shape(dataArgs[\"n_max\"], dataArgs[\"diag_offset\"])))\n",
    "        Growth = np.zeros((analyzeArgs[\"n_config_graphs\"]**len(growth_params), len(growth_params)))\n",
    "        Topol = np.zeros((analyzeArgs[\"n_config_graphs\"]**len(growth_params), len(topol_params)))\n",
    "\n",
    "        ## iterate through topological features\n",
    "        graph_configs = np.asarray(list(itertools.product(P,N)))\n",
    "        \n",
    "\n",
    "        for i, (p,n) in enumerate(graph_configs):\n",
    "\n",
    "            ## Generate Graph Type ______________________________________________\n",
    "\n",
    "            g = get_graph(int(n), p, draw = False)\n",
    "\n",
    "            g, a = sort_adjacency(g)\n",
    "            a = pad_matrix(a, dataArgs[\"n_max\"], dataArgs[\"diag_value\"])  # pad adjacency matrix to allow less nodes than n_max and fill diagonal\n",
    "            upper_a = reshape_A(a, dataArgs[\"diag_offset\"])\n",
    "\n",
    "\n",
    "            ## Generate Ground Truth features____________________________________\n",
    "\n",
    "            density = nx.density(g)\n",
    "\n",
    "            if nx.is_connected(g):\n",
    "                diameter = nx.diameter(g)\n",
    "            else:\n",
    "                diameter = -1\n",
    "\n",
    "            cluster_coef = nx.average_clustering(g)\n",
    "\n",
    "            if g.number_of_edges() > 0:\n",
    "                assort = nx.degree_assortativity_coefficient(g, x='out', y='in')\n",
    "            else:\n",
    "                assort = 0\n",
    "\n",
    "            edges = g.number_of_edges()\n",
    "\n",
    "            avg_degree = sum(i for i in nx.degree_centrality(g).values()) / len(nx.degree_centrality(g).keys())\n",
    "\n",
    "\n",
    "            ## toDO: add more graph topologies\n",
    "\n",
    "            ## Build Data Arrays___________________________________________________\n",
    "\n",
    "            G[i] = upper_a\n",
    "\n",
    "            Growth[i,0] = p\n",
    "            Growth[i,1] = int(n)\n",
    "\n",
    "            Topol[i,0] = density\n",
    "            Topol[i,1] = diameter\n",
    "            Topol[i,2] = cluster_coef\n",
    "            Topol[i,3] = assort\n",
    "            Topol[i,4] = edges\n",
    "            Topol[i,5] = avg_degree\n",
    "  \n",
    "    \n",
    "        ## ENCODER - 2D Digit Classes ______________________________________________\n",
    "\n",
    "        # display a 2D plot of the digit classes in the latent space\n",
    "        z_mean, _, _ = encoder.predict(G, batch_size = batch_size)\n",
    "        \n",
    "                \n",
    "        ## Measure the Mutual Information Gap ____________________________________________\n",
    "        if analyzeArgs[\"metric\"] == \"mig\":\n",
    "            #mi = compute_mi(P, np.squeeze(z_mean))\n",
    "            mig = compute_mig(Growth, z_mean)\n",
    "        \n",
    "        \n",
    "        ##  Reshape Array according to Parameters  \n",
    "        z_mean_growth = np.reshape(z_mean, (analyzeArgs[\"n_config_graphs\"], analyzeArgs[\"n_config_graphs\"], -1))\n",
    "        Growth = np.reshape(Growth,(analyzeArgs[\"n_config_graphs\"], analyzeArgs[\"n_config_graphs\"], -1))\n",
    "            \n",
    "        ## 1.) Growth Parameters________________________________________________________\n",
    "\n",
    "        ## Visualize Latent Variables x Growth Parameters ____________________________\n",
    "\n",
    "        fig, ax = plt.subplots(nrows= z_mean_growth.shape[-1] , ncols= len(growth_params))\n",
    "\n",
    "        for latent_z, row in enumerate(ax):        \n",
    "            for feature, col in enumerate(row):\n",
    "\n",
    "                if feature == 0:\n",
    "                    feature_1 = 1\n",
    "                if feature == 1:\n",
    "                    feature_1 = 0\n",
    "\n",
    "                y = np.mean(z_mean_growth[:,:,latent_z], axis= feature_1)\n",
    "                x = np.mean(Growth[:,:,feature], axis= feature_1)\n",
    "                col.plot(x, y)  \n",
    "\n",
    "                ## compute correlation and standardized covariance\n",
    "                corr = round(pearsonr(x,y)[0],3)\n",
    "                cov = round(np.cov(x, y)[0][1]/max(x),3)\n",
    "                col.annotate(\"corr:\"+str(corr)+\", cov:\"+str(cov), xy=(0, 1), xytext=(12, -12), va='top',xycoords='axes fraction', textcoords='offset points')\n",
    "\n",
    "\n",
    "        ## add row and column titles _____________________\n",
    "\n",
    "        rows = ['z_{}'.format(row) for row in range(z_mean_growth.shape[-1])]\n",
    "        cols = [t for t in growth_params]\n",
    "\n",
    "        for axis, col in zip(ax[0], cols):\n",
    "            axis.set_title(col, fontweight='bold')\n",
    "\n",
    "        for axis, row in zip(ax[:,0], rows):\n",
    "            axis.set_ylabel(row, rotation=0, size='large', fontweight='bold')\n",
    "\n",
    "\n",
    "\n",
    "        ## 2.) Graph Topologies________________________________________________________\n",
    "\n",
    "        ## Visualize Latent Variables x Growth Parameters ____________________________\n",
    "\n",
    "        ##  Reshape Array according to Parameters  \n",
    "        #Topol = np.reshape(Topol,(n_config_graphs, n_config_graphs, -1))\n",
    "\n",
    "        fig, ax = plt.subplots(nrows= z_mean.shape[-1] , ncols= len(topol_params), figsize=(30,10))\n",
    "\n",
    "        for latent_z, row in enumerate(ax):        \n",
    "            for feature, col in enumerate(row):\n",
    "\n",
    "                ## toDO: change sorting\n",
    "                y = z_mean[:,latent_z]\n",
    "                x = Topol[:,feature]\n",
    "                sns.regplot(x, y, color=\"steelblue\", ax=col)\n",
    "                #col.scatter(x, y) \n",
    "\n",
    "                # set axes range\n",
    "                #plt.xlim(-4, 4)\n",
    "                #plt.ylim(-4, 4)\n",
    "\n",
    "               # try:\n",
    "               #     ## plot trend line\n",
    "               #     x = np.nan_to_num(x)\n",
    "               #     y = np.nan_to_num(y)\n",
    "\n",
    "               #     z = np.polyfit(x, y, 1)\n",
    "               #     p = np.poly1d(z)\n",
    "               #     col.plot(x,p(x),\"steelblue\")\n",
    "               # except:\n",
    "               #     pass\n",
    "\n",
    "\n",
    "                ## compute correlation and standardized covariance\n",
    "                corr = round(pearsonr(x,y)[0],3)\n",
    "                cov = round(np.cov(x, y)[0][1]/max(x),3)\n",
    "                col.annotate(\"corr:\"+str(corr)+\", cov:\"+str(cov), xy=(0, 1), xytext=(12, -12), va='top',xycoords='axes fraction', textcoords='offset points')\n",
    "\n",
    "\n",
    "\n",
    "        ## add row and column titles _____________________\n",
    "\n",
    "        rows = ['z_{}'.format(row) for row in range(z_mean.shape[-1])]\n",
    "        cols = [t for t in topol_params]\n",
    "\n",
    "        for axis, col in zip(ax[0], cols):\n",
    "            axis.set_title(col, fontweight='bold')\n",
    "\n",
    "        for axis, row in zip(ax[:,0], rows):\n",
    "            axis.set_ylabel(row, rotation=0, size='large', fontweight='bold')\n",
    "\n",
    "\n",
    "## PLOT RESULTS ________________________________________\n",
    "\n",
    "analyzeArgs = {\"root_params\": 2, \"n_config_graphs\": 10, \"metric\": \"mig\"}\n",
    "latent_space_feature_correlation(analyzeArgs, modelArgs, models, batch_size=trainArgs[\"batch_size\"], model_name=\"vae_mlp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize the Latent Space in 2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "analyzeArgs = {\"save_plots\": False}\n",
    "vis2D(analyzeArgs, modelArgs, models, data, batch_size=trainArgs[\"batch_size\"], model_name=\"vae_mlp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Latent Generative Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "analyzeArgs = {\"z\": [0,1]}\n",
    "visDistr(modelArgs, analyzeArgs, models,data,trainArgs[\"batch_size\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate a Single Graph Instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "analyzeArgs = {\"activations\": [0], \"z\": [0]}\n",
    "generate_single(analyzeArgs, modelArgs, dataArgs, models, color_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Interpolated Manifold from Latent Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# range, normal, z\n",
    "analyzeArgs = {\"z\": [0,1], \"sample\": \"range\", \"act_range\": [-4, 4], \"act_scale\": 1, \"size_of_manifold\": 5, \"save_plots\": False}\n",
    "generate_manifold(analyzeArgs, modelArgs, dataArgs, models, data, color_map, batch_size=trainArgs[\"batch_size\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## \"topol\", \"distr\", \"nodes_edges\"\n",
    "analyzeArgs[\"plot\"] = \"topol\"\n",
    "generate_topol_manifold(analyzeArgs, modelArgs, dataArgs, models, data, color_map, batch_size=trainArgs[\"batch_size\"])\n",
    "\n",
    "## \"cluster_coef\", \"assort\", \"avg_degree\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze Parameter Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "## range, normal, z\n",
    "analyzeArgs = {\"z\": [0,1], \"graph_type\": \"ER\", \"sample\": \"normal\", \"act_range\": [-4, 4], \"act_scale\": 1, \"size_of_manifold\": 5, \"save_plots\": False}\n",
    "generate_param_graph_manifold(analyzeArgs, modelArgs, dataArgs, models, data, color_map, trainArgs[\"batch_size\"], scaler)\n",
    "\n",
    "## \"cluster_coef\", \"assort\", \"avg_degree\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_param_topol_manifold(analyzeArgs, modelArgs, dataArgs, models, data, color_map, trainArgs[\"batch_size\"], scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
